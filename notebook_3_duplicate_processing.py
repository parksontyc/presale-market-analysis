# é å”®å±‹å¸‚å ´åˆ†æç³»çµ± - 03_é‡è¤‡äº¤æ˜“è­˜åˆ¥èˆ‡è™•ç†
# åŸºæ–¼ PRD v2.3 è¦æ ¼é€²è¡Œé‡è¤‡äº¤æ˜“å»é‡èˆ‡æœ‰æ•ˆäº¤æ˜“åˆ¤æ–·
# ================================================================================

# %% [markdown]
# # é å”®å±‹å¸‚å ´åˆ†æç³»çµ± - é‡è¤‡äº¤æ˜“è­˜åˆ¥èˆ‡è™•ç†
# 
# ## ğŸ“‹ ç›®æ¨™
# - âœ… å¯¦ä½œPRDä¸­çš„é‡è¤‡äº¤æ˜“è­˜åˆ¥é‚è¼¯
# - âœ… å»ºç«‹æœ‰æ•ˆäº¤æ˜“åˆ¤æ–·æ©Ÿåˆ¶
# - âœ… é©—è­‰å»é‡è™•ç†çµæœ
# - âœ… åˆ†æé‡è¤‡äº¤æ˜“æ¨¡å¼èˆ‡å½±éŸ¿
# 
# ## ğŸ¯ å…§å®¹å¤§ç¶±
# 1. ç‰©ä»¶å”¯ä¸€IDå»ºç«‹é‚è¼¯
# 2. é‡è¤‡äº¤æ˜“è­˜åˆ¥èˆ‡åˆ†çµ„
# 3. æœ‰æ•ˆäº¤æ˜“åˆ¤æ–·è¦å‰‡å¯¦ä½œ
# 4. å»é‡è™•ç†çµæœé©—è­‰
# 5. é‡è¤‡äº¤æ˜“æ¨¡å¼åˆ†æ
# 6. è³‡æ–™å“è³ªå½±éŸ¿è©•ä¼°
# 
# ## ğŸ“Š å»¶çºŒ Notebook 1-2 çš„åˆ†æçµæœ
# - ç¸½äº¤æ˜“ç­†æ•¸: 43,007 ç­†
# - è§£ç´„äº¤æ˜“ç­†æ•¸: 293 ç­† (0.68%)
# - éœ€è¦é€²è¡Œå»é‡è™•ç†ä»¥æå‡è³‡æ–™å“è³ª

# %% [markdown]
# ## 1. ç’°å¢ƒè¨­å®šèˆ‡è³‡æ–™è¼‰å…¥

# %%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import re
import warnings
from collections import Counter
warnings.filterwarnings('ignore')

# è¨­å®šé¡¯ç¤ºé¸é …
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)
pd.set_option('display.width', None)
pd.set_option('display.max_colwidth', 80)

# è¨­å®šä¸­æ–‡å­—å‹
plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'Arial Unicode MS', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False

# è¨­å®šåœ–è¡¨æ¨£å¼
sns.set_style("whitegrid")
plt.style.use('default')

print("âœ… ç’°å¢ƒè¨­å®šå®Œæˆ")
print(f"ğŸ“… åˆ†ææ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

# %%
# è¼‰å…¥è³‡æ–™æª”æ¡ˆ (å»¶çºŒ Notebook 1-2)
print("ğŸ”„ è¼‰å…¥è³‡æ–™æª”æ¡ˆ...")

try:
    # è¼‰å…¥é€ç­†äº¤æ˜“è³‡æ–™ (ä¸»è¦åˆ†æå°è±¡)
    transaction_df = pd.read_csv('../data/raw/lvr_presale_test.csv', encoding='utf-8')
    print(f"âœ… é€ç­†äº¤æ˜“è³‡æ–™è¼‰å…¥æˆåŠŸ: {transaction_df.shape}")
    
    # è¼‰å…¥é å”®ç¤¾å€è³‡æ–™ (è¼”åŠ©åˆ†æ)
    community_df = pd.read_csv('../data/raw/lvr_community_data_test.csv', encoding='utf-8')
    print(f"âœ… é å”®ç¤¾å€è³‡æ–™è¼‰å…¥æˆåŠŸ: {community_df.shape}")
    
    # è¼‰å…¥è§£ç´„åˆ†æçµæœ
    try:
        cancellation_df = pd.read_csv('../data/processed/02_cancellation_analysis.csv', encoding='utf-8')
        print(f"âœ… è§£ç´„åˆ†æçµæœè¼‰å…¥æˆåŠŸ: {cancellation_df.shape}")
    except FileNotFoundError:
        print("âš ï¸ æœªæ‰¾åˆ°è§£ç´„åˆ†æçµæœï¼Œå°‡é‡æ–°è¨ˆç®—è§£ç´„è³‡è¨Š")
        cancellation_df = None
        
except FileNotFoundError as e:
    print(f"âŒ æª”æ¡ˆè¼‰å…¥å¤±æ•—: {e}")
    print("ğŸ“ è«‹ç¢ºèªæª”æ¡ˆæ˜¯å¦æ”¾ç½®åœ¨ ../data/raw/ è³‡æ–™å¤¾ä¸­")
except Exception as e:
    print(f"âŒ è¼‰å…¥éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")

# %% [markdown]
# ## 2. ç‰©ä»¶å”¯ä¸€IDå»ºç«‹é‚è¼¯

# %%
# æ ¹æ“šPRDå®šç¾©å»ºç«‹ç‰©ä»¶å”¯ä¸€ID
print("ğŸ”‘ å»ºç«‹ç‰©ä»¶å”¯ä¸€ID")
print("=" * 80)

def create_property_id(row):
    """
    æ ¹æ“šPRDè¦æ ¼å»ºç«‹ç‰©ä»¶å”¯ä¸€è­˜åˆ¥ç¢¼
    ç‰©ä»¶å”¯ä¸€è­˜åˆ¥ = å‚™æŸ¥ç·¨è™Ÿ + åè½è¡—é“ + æ¨“å±¤
    
    Args:
        row (pd.Series): äº¤æ˜“è¨˜éŒ„
        
    Returns:
        str: ç‰©ä»¶å”¯ä¸€ID
    """
    try:
        # å‚™æŸ¥ç·¨è™Ÿ
        property_code = str(row.get('å‚™æŸ¥ç·¨è™Ÿ', '')).strip()
        
        # åè½è¡—é“
        street = str(row.get('åè½è¡—é“', '')).strip()
        
        # æ¨“å±¤è³‡è¨Š
        floor_info = str(row.get('æ¨“å±¤', '')).strip()
        
        # çµ„åˆå”¯ä¸€ID
        property_id = f"{property_code}_{street}_{floor_info}"
        
        # æ¸…ç†ç‰¹æ®Šå­—å…ƒ
        property_id = re.sub(r'[^\w\-_]', '_', property_id)
        
        return property_id
        
    except Exception as e:
        return f"ERROR_{hash(str(row))}"

# æ‡‰ç”¨ç‰©ä»¶IDå»ºç«‹é‚è¼¯
print("ğŸ”„ å»ºç«‹æ‰€æœ‰äº¤æ˜“è¨˜éŒ„çš„ç‰©ä»¶å”¯ä¸€ID...")

transaction_df['ç‰©ä»¶å”¯ä¸€ID'] = transaction_df.apply(create_property_id, axis=1)

print(f"âœ… æˆåŠŸå»ºç«‹ {len(transaction_df)} ç­†äº¤æ˜“è¨˜éŒ„çš„ç‰©ä»¶ID")

# æª¢è¦–ç‰©ä»¶IDæ¨£æœ¬
print(f"\nç‰©ä»¶IDæ¨£æœ¬ (å‰10ç­†):")
sample_ids = transaction_df[['å‚™æŸ¥ç·¨è™Ÿ', 'åè½è¡—é“', 'æ¨“å±¤', 'ç‰©ä»¶å”¯ä¸€ID']].head(10)
for i, (idx, row) in enumerate(sample_ids.iterrows()):
    print(f"{i+1:2d}. {row['ç‰©ä»¶å”¯ä¸€ID']}")
    print(f"    å‚™æŸ¥ç·¨è™Ÿ: {row['å‚™æŸ¥ç·¨è™Ÿ']}")
    print(f"    åè½è¡—é“: {row['åè½è¡—é“']}")
    print(f"    æ¨“å±¤: {row['æ¨“å±¤']}")

# %%
# ç‰©ä»¶IDçµ±è¨ˆåˆ†æ
print(f"\nğŸ“Š ç‰©ä»¶IDçµ±è¨ˆåˆ†æ")
print("-" * 50)

# è¨ˆç®—å”¯ä¸€ç‰©ä»¶æ•¸é‡
unique_properties = transaction_df['ç‰©ä»¶å”¯ä¸€ID'].nunique()
total_transactions = len(transaction_df)

print(f"ç¸½äº¤æ˜“ç­†æ•¸: {total_transactions:,}")
print(f"å”¯ä¸€ç‰©ä»¶æ•¸é‡: {unique_properties:,}")
print(f"å¹³å‡æ¯ç‰©ä»¶äº¤æ˜“æ¬¡æ•¸: {total_transactions/unique_properties:.2f}")

# è¨ˆç®—é‡è¤‡äº¤æ˜“çµ±è¨ˆ
property_counts = transaction_df['ç‰©ä»¶å”¯ä¸€ID'].value_counts()
duplicate_properties = property_counts[property_counts > 1]

print(f"\né‡è¤‡äº¤æ˜“çµ±è¨ˆ:")
print(f"å–®æ¬¡äº¤æ˜“ç‰©ä»¶: {len(property_counts[property_counts == 1]):,} å€‹ ({len(property_counts[property_counts == 1])/len(property_counts)*100:.1f}%)")
print(f"é‡è¤‡äº¤æ˜“ç‰©ä»¶: {len(duplicate_properties):,} å€‹ ({len(duplicate_properties)/len(property_counts)*100:.1f}%)")
print(f"é‡è¤‡äº¤æ˜“ç­†æ•¸: {(property_counts - 1).sum():,} ç­†")

# é‡è¤‡äº¤æ˜“æ¬¡æ•¸åˆ†å¸ƒ
print(f"\né‡è¤‡äº¤æ˜“æ¬¡æ•¸åˆ†å¸ƒ:")
repeat_distribution = duplicate_properties.value_counts().sort_index()
for count, frequency in repeat_distribution.items():
    print(f"   {count}æ¬¡äº¤æ˜“: {frequency} å€‹ç‰©ä»¶")

# %% [markdown]
# ## 3. é‡è¤‡äº¤æ˜“è­˜åˆ¥èˆ‡åˆ†çµ„

# %%
# è­˜åˆ¥ä¸¦åˆ†æé‡è¤‡äº¤æ˜“æ¡ˆä¾‹
print("ğŸ” é‡è¤‡äº¤æ˜“æ¡ˆä¾‹åˆ†æ")
print("=" * 60)

if len(duplicate_properties) > 0:
    # å–å¾—é‡è¤‡äº¤æ˜“çš„è©³ç´°è³‡è¨Š
    duplicate_transaction_details = []
    
    for property_id, count in duplicate_properties.head(20).items():  # åˆ†æå‰20å€‹é‡è¤‡æ¡ˆä¾‹
        property_transactions = transaction_df[transaction_df['ç‰©ä»¶å”¯ä¸€ID'] == property_id].copy()
        
        # æŒ‰äº¤æ˜“æ—¥æœŸæ’åº
        property_transactions = property_transactions.sort_values('äº¤æ˜“æ—¥æœŸ')
        
        duplicate_info = {
            'property_id': property_id,
            'transaction_count': count,
            'transactions': property_transactions[['äº¤æ˜“æ—¥æœŸ', 'äº¤æ˜“å¹´å­£', 'äº¤æ˜“ç¸½åƒ¹', 'å»ºç‰©å–®åƒ¹', 'è§£ç´„æƒ…å½¢']].to_dict('records'),
            'price_range': property_transactions['äº¤æ˜“ç¸½åƒ¹'].max() - property_transactions['äº¤æ˜“ç¸½åƒ¹'].min(),
            'has_cancellation': property_transactions['è§£ç´„æƒ…å½¢'].notna().any(),
            'date_range': (property_transactions['äº¤æ˜“æ—¥æœŸ'].max(), property_transactions['äº¤æ˜“æ—¥æœŸ'].min())
        }
        
        duplicate_transaction_details.append(duplicate_info)
    
    print(f"é‡è¤‡äº¤æ˜“æ¡ˆä¾‹æ¨£æœ¬ (å‰10å€‹):")
    print("-" * 80)
    
    for i, detail in enumerate(duplicate_transaction_details[:10]):
        print(f"\næ¡ˆä¾‹ {i+1}: {detail['property_id']}")
        print(f"   äº¤æ˜“æ¬¡æ•¸: {detail['transaction_count']}")
        print(f"   åƒ¹æ ¼è®Šå‹•ç¯„åœ: {detail['price_range']:,.0f} è¬å…ƒ")
        print(f"   æ˜¯å¦æœ‰è§£ç´„: {'æ˜¯' if detail['has_cancellation'] else 'å¦'}")
        
        # é¡¯ç¤ºå„æ¬¡äº¤æ˜“è©³æƒ…
        for j, transaction in enumerate(detail['transactions']):
            cancellation_status = "è§£ç´„" if pd.notna(transaction['è§£ç´„æƒ…å½¢']) else "æ­£å¸¸"
            print(f"   äº¤æ˜“ {j+1}: {transaction['äº¤æ˜“æ—¥æœŸ']} | {transaction['äº¤æ˜“ç¸½åƒ¹']:.0f}è¬ | {transaction['å»ºç‰©å–®åƒ¹']:.1f}è¬/åª | {cancellation_status}")

else:
    print("âŒ ç„¡é‡è¤‡äº¤æ˜“æ¡ˆä¾‹")
    duplicate_transaction_details = []

# %%
# é‡è¤‡äº¤æ˜“æ¨¡å¼åˆ†æ
print(f"\nğŸ“Š é‡è¤‡äº¤æ˜“æ¨¡å¼åˆ†æ")
print("-" * 50)

if len(duplicate_properties) > 0:
    # åˆ†æé‡è¤‡äº¤æ˜“çš„ç‰¹å¾µ
    duplicate_df = transaction_df[transaction_df['ç‰©ä»¶å”¯ä¸€ID'].isin(duplicate_properties.index)].copy()
    
    print(f"é‡è¤‡äº¤æ˜“ç‰©ä»¶æ¶‰åŠäº¤æ˜“: {len(duplicate_df)} ç­†")
    
    # 1. é‡è¤‡äº¤æ˜“çš„ç¸£å¸‚åˆ†å¸ƒ
    duplicate_city_dist = duplicate_df['ç¸£å¸‚'].value_counts()
    print(f"\né‡è¤‡äº¤æ˜“ç¸£å¸‚åˆ†å¸ƒ:")
    for city, count in duplicate_city_dist.head(10).items():
        total_city_transactions = transaction_df[transaction_df['ç¸£å¸‚'] == city].shape[0]
        percentage = count / total_city_transactions * 100
        print(f"   {city}: {count} ç­† ({percentage:.1f}%)")
    
    # 2. é‡è¤‡äº¤æ˜“çš„åƒ¹æ ¼ç‰¹å¾µ
    print(f"\né‡è¤‡äº¤æ˜“åƒ¹æ ¼ç‰¹å¾µ:")
    print(f"   å¹³å‡äº¤æ˜“ç¸½åƒ¹: {duplicate_df['äº¤æ˜“ç¸½åƒ¹'].mean():,.0f} è¬å…ƒ")
    print(f"   å¹³å‡å»ºç‰©å–®åƒ¹: {duplicate_df['å»ºç‰©å–®åƒ¹'].mean():.1f} è¬/åª")
    
    # èˆ‡æ•´é«”å¸‚å ´æ¯”è¼ƒ
    overall_avg_price = transaction_df['äº¤æ˜“ç¸½åƒ¹'].mean()
    overall_avg_unit_price = transaction_df['å»ºç‰©å–®åƒ¹'].mean()
    
    print(f"   vs æ•´é«”å¸‚å ´ç¸½åƒ¹: {overall_avg_price:,.0f} è¬å…ƒ (å·®ç•°: {(duplicate_df['äº¤æ˜“ç¸½åƒ¹'].mean() - overall_avg_price):+,.0f})")
    print(f"   vs æ•´é«”å¸‚å ´å–®åƒ¹: {overall_avg_unit_price:.1f} è¬/åª (å·®ç•°: {(duplicate_df['å»ºç‰©å–®åƒ¹'].mean() - overall_avg_unit_price):+.1f})")
    
    # 3. é‡è¤‡äº¤æ˜“çš„è§£ç´„æƒ…æ³
    duplicate_cancellation_rate = duplicate_df['è§£ç´„æƒ…å½¢'].notna().sum() / len(duplicate_df) * 100
    overall_cancellation_rate = transaction_df['è§£ç´„æƒ…å½¢'].notna().sum() / len(transaction_df) * 100
    
    print(f"\né‡è¤‡äº¤æ˜“è§£ç´„æƒ…æ³:")
    print(f"   é‡è¤‡äº¤æ˜“è§£ç´„ç‡: {duplicate_cancellation_rate:.2f}%")
    print(f"   æ•´é«”å¸‚å ´è§£ç´„ç‡: {overall_cancellation_rate:.2f}%")
    print(f"   å·®ç•°: {duplicate_cancellation_rate - overall_cancellation_rate:+.2f}%")

# %% [markdown]
# ## 4. æœ‰æ•ˆäº¤æ˜“åˆ¤æ–·è¦å‰‡å¯¦ä½œ

# %%
# å¯¦ä½œæœ‰æ•ˆäº¤æ˜“åˆ¤æ–·é‚è¼¯
print("âš–ï¸ æœ‰æ•ˆäº¤æ˜“åˆ¤æ–·è¦å‰‡å¯¦ä½œ")
print("=" * 60)

def determine_valid_transaction(property_transactions):
    """
    æ ¹æ“šPRDè¦æ ¼åˆ¤æ–·æœ‰æ•ˆäº¤æ˜“
    
    åˆ¤æ–·é‚è¼¯ï¼š
    1. å„ªå…ˆé¸æ“‡æ­£å¸¸äº¤æ˜“ï¼ˆéè§£ç´„ï¼‰
    2. å¦‚æœ‰å¤šç­†æ­£å¸¸äº¤æ˜“ï¼Œé¸æ“‡æœ€æ—©çš„äº¤æ˜“
    3. å¦‚å…¨éƒ¨è§£ç´„ï¼Œé¸æ“‡æœ€æ—©çš„è§£ç´„äº¤æ˜“ä½†æ¨™è¨˜ç‚ºç„¡æ•ˆ
    
    Args:
        property_transactions (pd.DataFrame): åŒä¸€ç‰©ä»¶çš„æ‰€æœ‰äº¤æ˜“è¨˜éŒ„
        
    Returns:
        dict: åŒ…å«æœ‰æ•ˆäº¤æ˜“è³‡è¨Šå’Œåˆ¤æ–·çµæœ
    """
    # æŒ‰äº¤æ˜“æ—¥æœŸæ’åº
    sorted_transactions = property_transactions.sort_values('äº¤æ˜“æ—¥æœŸ').copy()
    
    # å€åˆ†æ­£å¸¸äº¤æ˜“å’Œè§£ç´„äº¤æ˜“
    normal_transactions = sorted_transactions[sorted_transactions['è§£ç´„æƒ…å½¢'].isna()]
    cancelled_transactions = sorted_transactions[sorted_transactions['è§£ç´„æƒ…å½¢'].notna()]
    
    result = {
        'total_transactions': len(sorted_transactions),
        'normal_count': len(normal_transactions),
        'cancelled_count': len(cancelled_transactions),
        'valid_transaction': None,
        'is_valid': False,
        'selection_reason': '',
        'duplicate_count': len(sorted_transactions) - 1  # é‡è¤‡æ¬¡æ•¸
    }
    
    if len(normal_transactions) > 0:
        # é¸æ“‡æœ€æ—©çš„æ­£å¸¸äº¤æ˜“
        valid_transaction = normal_transactions.iloc[0]
        result['valid_transaction'] = valid_transaction
        result['is_valid'] = True
        result['selection_reason'] = f'æœ€æ—©æ­£å¸¸äº¤æ˜“ (å…±{len(normal_transactions)}ç­†æ­£å¸¸äº¤æ˜“)'
        
    elif len(cancelled_transactions) > 0:
        # å…¨éƒ¨è§£ç´„ï¼Œé¸æ“‡æœ€æ—©çš„è§£ç´„äº¤æ˜“ä½†æ¨™è¨˜ç‚ºç„¡æ•ˆ
        valid_transaction = cancelled_transactions.iloc[0]
        result['valid_transaction'] = valid_transaction
        result['is_valid'] = False
        result['selection_reason'] = f'å…¨éƒ¨è§£ç´„ï¼Œé¸æ“‡æœ€æ—©è§£ç´„ (å…±{len(cancelled_transactions)}ç­†è§£ç´„)'
        
    else:
        # ç†è«–ä¸Šä¸æ‡‰è©²ç™¼ç”Ÿ
        result['selection_reason'] = 'ç„¡æœ‰æ•ˆäº¤æ˜“è¨˜éŒ„'
    
    return result

# %%
# æ‡‰ç”¨æœ‰æ•ˆäº¤æ˜“åˆ¤æ–·é‚è¼¯
print("ğŸ”„ å°æ‰€æœ‰é‡è¤‡äº¤æ˜“ç‰©ä»¶é€²è¡Œæœ‰æ•ˆäº¤æ˜“åˆ¤æ–·...")

if len(duplicate_properties) > 0:
    # å°æ¯å€‹é‡è¤‡äº¤æ˜“ç‰©ä»¶é€²è¡Œæœ‰æ•ˆäº¤æ˜“åˆ¤æ–·
    valid_transaction_results = []
    
    for property_id in duplicate_properties.index:
        property_transactions = transaction_df[transaction_df['ç‰©ä»¶å”¯ä¸€ID'] == property_id]
        result = determine_valid_transaction(property_transactions)
        result['property_id'] = property_id
        valid_transaction_results.append(result)
    
    # è½‰æ›ç‚ºDataFrameä»¥ä¾¿åˆ†æ
    valid_results_df = pd.DataFrame(valid_transaction_results)
    
    print(f"âœ… å®Œæˆ {len(valid_transaction_results)} å€‹é‡è¤‡äº¤æ˜“ç‰©ä»¶çš„æœ‰æ•ˆäº¤æ˜“åˆ¤æ–·")
    
    # æœ‰æ•ˆäº¤æ˜“åˆ¤æ–·çµæœçµ±è¨ˆ
    print(f"\næœ‰æ•ˆäº¤æ˜“åˆ¤æ–·çµæœçµ±è¨ˆ:")
    valid_count = valid_results_df['is_valid'].sum()
    invalid_count = len(valid_results_df) - valid_count
    
    print(f"   æœ‰æ•ˆäº¤æ˜“ç‰©ä»¶: {valid_count} å€‹ ({valid_count/len(valid_results_df)*100:.1f}%)")
    print(f"   ç„¡æ•ˆäº¤æ˜“ç‰©ä»¶: {invalid_count} å€‹ ({invalid_count/len(valid_results_df)*100:.1f}%)")
    
    # é¸æ“‡åŸå› çµ±è¨ˆ
    reason_counts = valid_results_df['selection_reason'].value_counts()
    print(f"\né¸æ“‡åŸå› åˆ†å¸ƒ:")
    for reason, count in reason_counts.items():
        print(f"   {reason}: {count} å€‹ç‰©ä»¶")
    
    # é‡è¤‡äº¤æ˜“æ•¸é‡çµ±è¨ˆ
    duplicate_count_stats = valid_results_df['duplicate_count'].describe()
    print(f"\né‡è¤‡äº¤æ˜“æ•¸é‡çµ±è¨ˆ:")
    print(f"   å¹³å‡é‡è¤‡æ¬¡æ•¸: {duplicate_count_stats['mean']:.1f}")
    print(f"   æœ€å¤šé‡è¤‡æ¬¡æ•¸: {duplicate_count_stats['max']:.0f}")
    print(f"   ç¸½é‡è¤‡äº¤æ˜“ç­†æ•¸: {valid_results_df['duplicate_count'].sum()}")

else:
    print("âŒ ç„¡é‡è¤‡äº¤æ˜“ç‰©ä»¶éœ€è¦è™•ç†")
    valid_results_df = pd.DataFrame()

# %%
# è©³ç´°å±•ç¤ºæœ‰æ•ˆäº¤æ˜“åˆ¤æ–·æ¡ˆä¾‹
print(f"\nğŸ” æœ‰æ•ˆäº¤æ˜“åˆ¤æ–·æ¡ˆä¾‹å±•ç¤º (å‰5å€‹)")
print("-" * 80)

if not valid_results_df.empty:
    for i, (idx, result) in enumerate(valid_results_df.head().iterrows()):
        print(f"\næ¡ˆä¾‹ {i+1}: {result['property_id']}")
        print(f"   ç¸½äº¤æ˜“æ•¸: {result['total_transactions']}")
        print(f"   æ­£å¸¸äº¤æ˜“: {result['normal_count']} ç­†")
        print(f"   è§£ç´„äº¤æ˜“: {result['cancelled_count']} ç­†")
        print(f"   åˆ¤æ–·çµæœ: {'âœ… æœ‰æ•ˆ' if result['is_valid'] else 'âŒ ç„¡æ•ˆ'}")
        print(f"   é¸æ“‡åŸå› : {result['selection_reason']}")
        print(f"   é‡è¤‡æ¬¡æ•¸: {result['duplicate_count']}")
        
        if result['valid_transaction'] is not None:
            valid_tx = result['valid_transaction']
            print(f"   é¸ä¸­äº¤æ˜“: {valid_tx['äº¤æ˜“æ—¥æœŸ']} | {valid_tx['äº¤æ˜“ç¸½åƒ¹']:.0f}è¬ | {valid_tx['å»ºç‰©å–®åƒ¹']:.1f}è¬/åª")

# %% [markdown]
# ## 5. å»é‡è™•ç†çµæœç”Ÿæˆ

# %%
# ç”Ÿæˆå»é‡è™•ç†å¾Œçš„ä¹¾æ·¨è³‡æ–™é›†
print("ğŸ§¹ ç”Ÿæˆå»é‡è™•ç†å¾Œçš„è³‡æ–™é›†")
print("=" * 60)

# å‰µå»ºå»é‡æ¨™è¨˜
transaction_df['æ˜¯å¦é‡è¤‡äº¤æ˜“'] = transaction_df['ç‰©ä»¶å”¯ä¸€ID'].isin(duplicate_properties.index)
transaction_df['æ˜¯å¦æœ‰æ•ˆäº¤æ˜“'] = True  # é è¨­ç‚ºæœ‰æ•ˆ
transaction_df['ç„¡æ•ˆåŸå› '] = ''
transaction_df['é‡è¤‡äº¤æ˜“æ¬¡æ•¸'] = transaction_df['ç‰©ä»¶å”¯ä¸€ID'].map(property_counts)

# æ¨™è¨˜ç„¡æ•ˆçš„é‡è¤‡äº¤æ˜“
if not valid_results_df.empty:
    # å»ºç«‹æœ‰æ•ˆäº¤æ˜“ç´¢å¼•æ˜ å°„
    valid_transaction_map = {}
    
    for _, result in valid_results_df.iterrows():
        property_id = result['property_id']
        
        if result['valid_transaction'] is not None:
            # æ‰¾åˆ°å°æ‡‰çš„äº¤æ˜“è¨˜éŒ„ç´¢å¼•
            property_transactions = transaction_df[transaction_df['ç‰©ä»¶å”¯ä¸€ID'] == property_id]
            valid_tx = result['valid_transaction']
            
            # æ‰¾åˆ°æœ€åŒ¹é…çš„äº¤æ˜“è¨˜éŒ„ï¼ˆé€šéå¤šå€‹æ¬„ä½æ¯”å°ï¼‰
            matching_transactions = property_transactions[
                (property_transactions['äº¤æ˜“æ—¥æœŸ'] == valid_tx['äº¤æ˜“æ—¥æœŸ']) &
                (property_transactions['äº¤æ˜“ç¸½åƒ¹'] == valid_tx['äº¤æ˜“ç¸½åƒ¹']) &
                (property_transactions['å»ºç‰©å–®åƒ¹'] == valid_tx['å»ºç‰©å–®åƒ¹'])
            ]
            
            if len(matching_transactions) > 0:
                valid_transaction_map[property_id] = matching_transactions.index[0]
                
                # æ¨™è¨˜è©²ç‰©ä»¶çš„å…¶ä»–äº¤æ˜“ç‚ºç„¡æ•ˆ
                other_transactions = property_transactions[property_transactions.index != matching_transactions.index[0]]
                transaction_df.loc[other_transactions.index, 'æ˜¯å¦æœ‰æ•ˆäº¤æ˜“'] = False
                transaction_df.loc[other_transactions.index, 'ç„¡æ•ˆåŸå› '] = 'é‡è¤‡äº¤æ˜“-éæœ€æ—©æœ‰æ•ˆäº¤æ˜“'
                
                # å¦‚æœé¸ä¸­çš„äº¤æ˜“æœ¬èº«ç„¡æ•ˆï¼ˆå…¨éƒ¨è§£ç´„æƒ…æ³ï¼‰
                if not result['is_valid']:
                    transaction_df.loc[matching_transactions.index[0], 'æ˜¯å¦æœ‰æ•ˆäº¤æ˜“'] = False
                    transaction_df.loc[matching_transactions.index[0], 'ç„¡æ•ˆåŸå› '] = 'å…¨éƒ¨è§£ç´„'

# çµ±è¨ˆå»é‡è™•ç†çµæœ
total_before = len(transaction_df)
valid_after = transaction_df['æ˜¯å¦æœ‰æ•ˆäº¤æ˜“'].sum()
removed_count = total_before - valid_after

print(f"å»é‡è™•ç†çµæœçµ±è¨ˆ:")
print(f"   è™•ç†å‰ç¸½ç­†æ•¸: {total_before:,}")
print(f"   è™•ç†å¾Œæœ‰æ•ˆç­†æ•¸: {valid_after:,}")
print(f"   ç§»é™¤é‡è¤‡ç­†æ•¸: {removed_count:,}")
print(f"   è³‡æ–™ä¿ç•™ç‡: {valid_after/total_before*100:.2f}%")

# ç„¡æ•ˆåŸå› çµ±è¨ˆ
invalid_reasons = transaction_df[~transaction_df['æ˜¯å¦æœ‰æ•ˆäº¤æ˜“']]['ç„¡æ•ˆåŸå› '].value_counts()
print(f"\nç„¡æ•ˆäº¤æ˜“åŸå› åˆ†å¸ƒ:")
for reason, count in invalid_reasons.items():
    print(f"   {reason}: {count:,} ç­†")

# %%
# å‰µå»ºä¹¾æ·¨çš„è³‡æ–™é›†
clean_transaction_df = transaction_df[transaction_df['æ˜¯å¦æœ‰æ•ˆäº¤æ˜“']].copy()

print(f"\nğŸ“Š ä¹¾æ·¨è³‡æ–™é›†ç‰¹æ€§:")
print(f"   æœ‰æ•ˆäº¤æ˜“ç­†æ•¸: {len(clean_transaction_df):,}")
print(f"   å”¯ä¸€ç‰©ä»¶æ•¸é‡: {clean_transaction_df['ç‰©ä»¶å”¯ä¸€ID'].nunique():,}")
print(f"   å¹³å‡æ¯ç‰©ä»¶äº¤æ˜“æ¬¡æ•¸: {len(clean_transaction_df)/clean_transaction_df['ç‰©ä»¶å”¯ä¸€ID'].nunique():.3f}")

# æ¯”è¼ƒå»é‡å‰å¾Œçš„åŸºæœ¬çµ±è¨ˆ
print(f"\nğŸ“ˆ å»é‡å‰å¾Œçµ±è¨ˆæ¯”è¼ƒ:")

comparison_stats = pd.DataFrame({
    'å»é‡å‰': [
        transaction_df['äº¤æ˜“ç¸½åƒ¹'].mean(),
        transaction_df['å»ºç‰©å–®åƒ¹'].mean(),
        transaction_df['ç¸½é¢ç©_æ•¸å€¼'].mean(),
        transaction_df['è§£ç´„æƒ…å½¢'].notna().sum(),
        transaction_df['è§£ç´„æƒ…å½¢'].notna().sum() / len(transaction_df) * 100
    ],
    'å»é‡å¾Œ': [
        clean_transaction_df['äº¤æ˜“ç¸½åƒ¹'].mean(),
        clean_transaction_df['å»ºç‰©å–®åƒ¹'].mean(),
        clean_transaction_df['ç¸½é¢ç©_æ•¸å€¼'].mean(),
        clean_transaction_df['è§£ç´„æƒ…å½¢'].notna().sum(),
        clean_transaction_df['è§£ç´„æƒ…å½¢'].notna().sum() / len(clean_transaction_df) * 100
    ]
}, index=['å¹³å‡äº¤æ˜“ç¸½åƒ¹(è¬)', 'å¹³å‡å»ºç‰©å–®åƒ¹(è¬/åª)', 'å¹³å‡ç¸½é¢ç©(åª)', 'è§£ç´„ç­†æ•¸', 'è§£ç´„ç‡(%)'])

comparison_stats['å·®ç•°'] = comparison_stats['å»é‡å¾Œ'] - comparison_stats['å»é‡å‰']
comparison_stats['å·®ç•°ç‡(%)'] = (comparison_stats['å·®ç•°'] / comparison_stats['å»é‡å‰'] * 100).round(2)

print(comparison_stats.round(2))

# %% [markdown]
# ## 6. é‡è¤‡äº¤æ˜“å½±éŸ¿åˆ†æ

# %%
# åˆ†æé‡è¤‡äº¤æ˜“å°å¸‚å ´æŒ‡æ¨™çš„å½±éŸ¿
print("ğŸ“Š é‡è¤‡äº¤æ˜“å½±éŸ¿åˆ†æ")
print("=" * 60)

# 1. å°åƒ¹æ ¼çµ±è¨ˆçš„å½±éŸ¿
print("1ï¸âƒ£ å°åƒ¹æ ¼çµ±è¨ˆçš„å½±éŸ¿:")

price_impact = {
    'ç¸½äº¤æ˜“æ•¸é‡è®ŠåŒ–': len(clean_transaction_df) - len(transaction_df),
    'å¹³å‡ç¸½åƒ¹è®ŠåŒ–': clean_transaction_df['äº¤æ˜“ç¸½åƒ¹'].mean() - transaction_df['äº¤æ˜“ç¸½åƒ¹'].mean(),
    'å¹³å‡å–®åƒ¹è®ŠåŒ–': clean_transaction_df['å»ºç‰©å–®åƒ¹'].mean() - transaction_df['å»ºç‰©å–®åƒ¹'].mean(),
    'ç¸½åƒ¹æ¨™æº–å·®è®ŠåŒ–': clean_transaction_df['äº¤æ˜“ç¸½åƒ¹'].std() - transaction_df['äº¤æ˜“ç¸½åƒ¹'].std(),
    'å–®åƒ¹æ¨™æº–å·®è®ŠåŒ–': clean_transaction_df['å»ºç‰©å–®åƒ¹'].std() - transaction_df['å»ºç‰©å–®åƒ¹'].std(),
}

for indicator, change in price_impact.items():
    print(f"   {indicator}: {change:+.2f}")

# %%
# 2. å°ç¸£å¸‚çµ±è¨ˆçš„å½±éŸ¿
print("\n2ï¸âƒ£ å°ä¸»è¦ç¸£å¸‚çµ±è¨ˆçš„å½±éŸ¿:")

# è¨ˆç®—å„ç¸£å¸‚å»é‡å‰å¾Œçš„äº¤æ˜“é‡è®ŠåŒ–
city_impact = {}
for city in transaction_df['ç¸£å¸‚'].value_counts().head(5).index:
    before_count = len(transaction_df[transaction_df['ç¸£å¸‚'] == city])
    after_count = len(clean_transaction_df[clean_transaction_df['ç¸£å¸‚'] == city])
    
    city_impact[city] = {
        'before': before_count,
        'after': after_count,
        'removed': before_count - after_count,
        'removal_rate': (before_count - after_count) / before_count * 100
    }

for city, stats in city_impact.items():
    print(f"   {city}: {stats['before']} â†’ {stats['after']} (-{stats['removed']}, -{stats['removal_rate']:.2f}%)")

# %%
# 3. å°è§£ç´„çµ±è¨ˆçš„å½±éŸ¿
print("\n3ï¸âƒ£ å°è§£ç´„çµ±è¨ˆçš„å½±éŸ¿:")

cancellation_impact = {
    'å»é‡å‰è§£ç´„ç­†æ•¸': transaction_df['è§£ç´„æƒ…å½¢'].notna().sum(),
    'å»é‡å¾Œè§£ç´„ç­†æ•¸': clean_transaction_df['è§£ç´„æƒ…å½¢'].notna().sum(),
    'å»é‡å‰è§£ç´„ç‡': transaction_df['è§£ç´„æƒ…å½¢'].notna().sum() / len(transaction_df) * 100,
    'å»é‡å¾Œè§£ç´„ç‡': clean_transaction_df['è§£ç´„æƒ…å½¢'].notna().sum() / len(clean_transaction_df) * 100,
}

cancellation_impact['è§£ç´„ç­†æ•¸è®ŠåŒ–'] = cancellation_impact['å»é‡å¾Œè§£ç´„ç­†æ•¸'] - cancellation_impact['å»é‡å‰è§£ç´„ç­†æ•¸']
cancellation_impact['è§£ç´„ç‡è®ŠåŒ–'] = cancellation_impact['å»é‡å¾Œè§£ç´„ç‡'] - cancellation_impact['å»é‡å‰è§£ç´„ç‡']

for indicator, value in cancellation_impact.items():
    if 'ç‡' in indicator:
        print(f"   {indicator}: {value:.3f}%")
    else:
        print(f"   {indicator}: {value}")

# %% [markdown]
# ## 7. è³‡æ–™å“è³ªè©•ä¼°

# %%
# è³‡æ–™å“è³ªè©•ä¼°
print("ğŸ” è³‡æ–™å“è³ªè©•ä¼°")
print("=" * 50)

# 1. é‡è¤‡äº¤æ˜“ç‰¹å¾µåˆ†æ
print("1ï¸âƒ£ é‡è¤‡äº¤æ˜“ç‰¹å¾µåˆ†æ:")

if len(duplicate_properties) > 0:
    # é‡è¤‡äº¤æ˜“çš„ç‰©ä»¶ç‰¹å¾µ
    duplicate_transactions = transaction_df[transaction_df['æ˜¯å¦é‡è¤‡äº¤æ˜“']]
    
    print(f"   é‡è¤‡äº¤æ˜“ç‰©ä»¶æ•¸é‡: {duplicate_properties.nunique()} å€‹")
    print(f"   é‡è¤‡äº¤æ˜“ç¸½ç­†æ•¸: {len(duplicate_transactions)} ç­†")
    print(f"   å¹³å‡æ¯å€‹é‡è¤‡ç‰©ä»¶äº¤æ˜“æ¬¡æ•¸: {len(duplicate_transactions) / duplicate_properties.nunique():.2f}")
    
    # é‡è¤‡äº¤æ˜“çš„æ™‚é–“åˆ†å¸ƒ
    repeat_by_season = duplicate_transactions['äº¤æ˜“å¹´å­£'].value_counts().sort_index()
    print(f"\n   é‡è¤‡äº¤æ˜“å¹´å­£åˆ†å¸ƒ (å‰5å):")
    for season, count in repeat_by_season.head().items():
        total_season = transaction_df[transaction_df['äº¤æ˜“å¹´å­£'] == season].shape[0]
        percentage = count / total_season * 100 if total_season > 0 else 0
        print(f"      {season}: {count} ç­† ({percentage:.1f}%)")

# %%
# 2. è³‡æ–™å®Œæ•´æ€§æª¢æŸ¥
print("\n2ï¸âƒ£ è³‡æ–™å®Œæ•´æ€§æª¢æŸ¥:")

# æª¢æŸ¥é—œéµæ¬„ä½çš„å®Œæ•´æ€§
key_fields = ['å‚™æŸ¥ç·¨è™Ÿ', 'åè½è¡—é“', 'æ¨“å±¤', 'äº¤æ˜“æ—¥æœŸ', 'äº¤æ˜“ç¸½åƒ¹', 'å»ºç‰©å–®åƒ¹']

print("   é—œéµæ¬„ä½å®Œæ•´æ€§ (å»é‡å¾Œ):")
for field in key_fields:
    if field in clean_transaction_df.columns:
        missing_count = clean_transaction_df[field].isna().sum()
        missing_rate = missing_count / len(clean_transaction_df) * 100
        print(f"      {field}: {len(clean_transaction_df) - missing_count}/{len(clean_transaction_df)} ({100-missing_rate:.1f}% å®Œæ•´)")
    else:
        print(f"      {field}: æ¬„ä½ä¸å­˜åœ¨")

# %%
# 3. ç•°å¸¸å€¼æª¢æ¸¬
print("\n3ï¸âƒ£ ç•°å¸¸å€¼æª¢æ¸¬:")

# æª¢æ¸¬åƒ¹æ ¼ç•°å¸¸å€¼
def detect_price_outliers(df, column, method='iqr'):
    """æª¢æ¸¬åƒ¹æ ¼ç•°å¸¸å€¼"""
    if method == 'iqr':
        Q1 = df[column].quantile(0.25)
        Q3 = df[column].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
        return outliers, lower_bound, upper_bound

# æª¢æ¸¬ç¸½åƒ¹ç•°å¸¸å€¼
total_price_outliers, tp_lower, tp_upper = detect_price_outliers(clean_transaction_df, 'äº¤æ˜“ç¸½åƒ¹')
print(f"   äº¤æ˜“ç¸½åƒ¹ç•°å¸¸å€¼: {len(total_price_outliers)} ç­† ({len(total_price_outliers)/len(clean_transaction_df)*100:.2f}%)")
print(f"      æ­£å¸¸ç¯„åœ: {tp_lower:.0f} - {tp_upper:.0f} è¬å…ƒ")

# æª¢æ¸¬å–®åƒ¹ç•°å¸¸å€¼
unit_price_outliers, up_lower, up_upper = detect_price_outliers(clean_transaction_df, 'å»ºç‰©å–®åƒ¹')
print(f"   å»ºç‰©å–®åƒ¹ç•°å¸¸å€¼: {len(unit_price_outliers)} ç­† ({len(unit_price_outliers)/len(clean_transaction_df)*100:.2f}%)")
print(f"      æ­£å¸¸ç¯„åœ: {up_lower:.1f} - {up_upper:.1f} è¬/åª")

# %%
# 4. é‚è¼¯ä¸€è‡´æ€§æª¢æŸ¥
print("\n4ï¸âƒ£ é‚è¼¯ä¸€è‡´æ€§æª¢æŸ¥:")

consistency_issues = {}

# æª¢æŸ¥ç¸½åƒ¹èˆ‡å–®åƒ¹ã€é¢ç©çš„ä¸€è‡´æ€§
clean_transaction_df['è¨ˆç®—ç¸½åƒ¹'] = clean_transaction_df['å»ºç‰©å–®åƒ¹'] * clean_transaction_df['ç¸½é¢ç©_æ•¸å€¼']
clean_transaction_df['åƒ¹æ ¼å·®ç•°'] = abs(clean_transaction_df['äº¤æ˜“ç¸½åƒ¹'] - clean_transaction_df['è¨ˆç®—ç¸½åƒ¹'])
clean_transaction_df['åƒ¹æ ¼å·®ç•°ç‡'] = clean_transaction_df['åƒ¹æ ¼å·®ç•°'] / clean_transaction_df['äº¤æ˜“ç¸½åƒ¹'] * 100

# è¨­å®šå®¹å¿èª¤å·®ç‚º5%
price_inconsistent = clean_transaction_df[clean_transaction_df['åƒ¹æ ¼å·®ç•°ç‡'] > 5]
consistency_issues['åƒ¹æ ¼è¨ˆç®—ä¸ä¸€è‡´'] = len(price_inconsistent)

# æª¢æŸ¥é¢ç©åˆç†æ€§
area_unreasonable = clean_transaction_df[
    (clean_transaction_df['ç¸½é¢ç©_æ•¸å€¼'] < 5) | 
    (clean_transaction_df['ç¸½é¢ç©_æ•¸å€¼'] > 200)
]
consistency_issues['é¢ç©ä¸åˆç†'] = len(area_unreasonable)

# æª¢æŸ¥å–®åƒ¹åˆç†æ€§
unit_price_unreasonable = clean_transaction_df[
    (clean_transaction_df['å»ºç‰©å–®åƒ¹'] < 5) | 
    (clean_transaction_df['å»ºç‰©å–®åƒ¹'] > 300)
]
consistency_issues['å–®åƒ¹ä¸åˆç†'] = len(unit_price_unreasonable)

print("   é‚è¼¯ä¸€è‡´æ€§å•é¡Œçµ±è¨ˆ:")
for issue, count in consistency_issues.items():
    percentage = count / len(clean_transaction_df) * 100
    print(f"      {issue}: {count} ç­† ({percentage:.2f}%)")

# %% [markdown]
# ## 8. è¦–è¦ºåŒ–åˆ†æ

# %%
# å‰µå»ºè¦–è¦ºåŒ–åœ–è¡¨
print("ğŸ“Š é‡è¤‡äº¤æ˜“è™•ç†è¦–è¦ºåŒ–åˆ†æ")
print("=" * 50)

# å‰µå»ºåœ–è¡¨
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# 1. é‡è¤‡äº¤æ˜“æ¬¡æ•¸åˆ†å¸ƒ
if len(duplicate_properties) > 0:
    repeat_counts = duplicate_properties.value_counts().value_counts().sort_index()
    
    bars = axes[0, 0].bar(repeat_counts.index, repeat_counts.values, color='skyblue')
    axes[0, 0].set_title('é‡è¤‡äº¤æ˜“æ¬¡æ•¸åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    axes[0, 0].set_xlabel('äº¤æ˜“æ¬¡æ•¸')
    axes[0, 0].set_ylabel('ç‰©ä»¶æ•¸é‡')
    
    # æ·»åŠ æ•¸å€¼æ¨™ç±¤
    for bar in bars:
        height = bar.get_height()
        axes[0, 0].text(bar.get_x() + bar.get_width()/2., height,
                       f'{int(height)}', ha='center', va='bottom')
else:
    axes[0, 0].text(0.5, 0.5, 'ç„¡é‡è¤‡äº¤æ˜“', ha='center', va='center', 
                   transform=axes[0, 0].transAxes, fontsize=12)
    axes[0, 0].set_title('é‡è¤‡äº¤æ˜“æ¬¡æ•¸åˆ†å¸ƒ (ç„¡è³‡æ–™)', fontsize=14)

# 2. å»é‡å‰å¾Œç¸£å¸‚äº¤æ˜“é‡æ¯”è¼ƒ
top_cities = list(city_impact.keys())
before_counts = [city_impact[city]['before'] for city in top_cities]
after_counts = [city_impact[city]['after'] for city in top_cities]

x = np.arange(len(top_cities))
width = 0.35

bars1 = axes[0, 1].bar(x - width/2, before_counts, width, label='å»é‡å‰', color='lightcoral')
bars2 = axes[0, 1].bar(x + width/2, after_counts, width, label='å»é‡å¾Œ', color='lightgreen')

axes[0, 1].set_title('ä¸»è¦ç¸£å¸‚å»é‡å‰å¾Œäº¤æ˜“é‡æ¯”è¼ƒ', fontsize=14, fontweight='bold')
axes[0, 1].set_xlabel('ç¸£å¸‚')
axes[0, 1].set_ylabel('äº¤æ˜“ç­†æ•¸')
axes[0, 1].set_xticks(x)
axes[0, 1].set_xticklabels(top_cities, rotation=45, ha='right')
axes[0, 1].legend()

# 3. åƒ¹æ ¼å½±éŸ¿åˆ†æ
price_metrics = ['å¹³å‡ç¸½åƒ¹', 'å¹³å‡å–®åƒ¹']
before_prices = [transaction_df['äº¤æ˜“ç¸½åƒ¹'].mean(), transaction_df['å»ºç‰©å–®åƒ¹'].mean()]
after_prices = [clean_transaction_df['äº¤æ˜“ç¸½åƒ¹'].mean(), clean_transaction_df['å»ºç‰©å–®åƒ¹'].mean()]

x = np.arange(len(price_metrics))
bars1 = axes[1, 0].bar(x - width/2, before_prices, width, label='å»é‡å‰', color='orange')
bars2 = axes[1, 0].bar(x + width/2, after_prices, width, label='å»é‡å¾Œ', color='blue')

axes[1, 0].set_title('å»é‡å‰å¾Œåƒ¹æ ¼çµ±è¨ˆæ¯”è¼ƒ', fontsize=14, fontweight='bold')
axes[1, 0].set_xlabel('åƒ¹æ ¼æŒ‡æ¨™')
axes[1, 0].set_ylabel('åƒ¹æ ¼')
axes[1, 0].set_xticks(x)
axes[1, 0].set_xticklabels(price_metrics)
axes[1, 0].legend()

# æ·»åŠ æ•¸å€¼æ¨™ç±¤
for i, (bar1, bar2) in enumerate(zip(bars1, bars2)):
    axes[1, 0].text(bar1.get_x() + bar1.get_width()/2., bar1.get_height(),
                   f'{before_prices[i]:.0f}', ha='center', va='bottom')
    axes[1, 0].text(bar2.get_x() + bar2.get_width()/2., bar2.get_height(),
                   f'{after_prices[i]:.0f}', ha='center', va='bottom')

# 4. è³‡æ–™å“è³ªå•é¡Œåˆ†å¸ƒ
if consistency_issues:
    issue_names = list(consistency_issues.keys())
    issue_counts = list(consistency_issues.values())
    
    bars = axes[1, 1].bar(range(len(issue_names)), issue_counts, color='red', alpha=0.7)
    axes[1, 1].set_title('è³‡æ–™å“è³ªå•é¡Œåˆ†å¸ƒ', fontsize=14, fontweight='bold')
    axes[1, 1].set_xlabel('å•é¡Œé¡å‹')
    axes[1, 1].set_ylabel('å•é¡Œç­†æ•¸')
    axes[1, 1].set_xticks(range(len(issue_names)))
    axes[1, 1].set_xticklabels(issue_names, rotation=45, ha='right')
    
    # æ·»åŠ æ•¸å€¼æ¨™ç±¤
    for bar in bars:
        height = bar.get_height()
        axes[1, 1].text(bar.get_x() + bar.get_width()/2., height,
                       f'{int(height)}', ha='center', va='bottom')
else:
    axes[1, 1].text(0.5, 0.5, 'ç„¡å“è³ªå•é¡Œ', ha='center', va='center', 
                   transform=axes[1, 1].transAxes, fontsize=12)
    axes[1, 1].set_title('è³‡æ–™å“è³ªå•é¡Œåˆ†å¸ƒ (ç„¡å•é¡Œ)', fontsize=14)

plt.tight_layout()
plt.show()

# %% [markdown]
# ## 9. çµæœå„²å­˜èˆ‡é©—è­‰

# %%
# å„²å­˜è™•ç†çµæœ
print("ğŸ’¾ å„²å­˜é‡è¤‡äº¤æ˜“è™•ç†çµæœ...")

# 1. å„²å­˜å®Œæ•´çš„äº¤æ˜“è³‡æ–™ï¼ˆåŒ…å«å»é‡æ¨™è¨˜ï¼‰
enhanced_transaction_df = transaction_df[[
    'å‚™æŸ¥ç·¨è™Ÿ', 'ç¸£å¸‚', 'è¡Œæ”¿å€', 'åè½è¡—é“', 'æ¨“å±¤', 'äº¤æ˜“æ—¥æœŸ', 'äº¤æ˜“å¹´å­£',
    'äº¤æ˜“ç¸½åƒ¹', 'å»ºç‰©å–®åƒ¹', 'ç¸½é¢ç©_æ•¸å€¼', 'è§£ç´„æƒ…å½¢', 'ç‰©ä»¶å”¯ä¸€ID',
    'æ˜¯å¦é‡è¤‡äº¤æ˜“', 'æ˜¯å¦æœ‰æ•ˆäº¤æ˜“', 'ç„¡æ•ˆåŸå› ', 'é‡è¤‡äº¤æ˜“æ¬¡æ•¸'
]].copy()

enhanced_transaction_df.to_csv('../data/processed/03_enhanced_transactions.csv', 
                              index=False, encoding='utf-8-sig')
print("âœ… å®Œæ•´äº¤æ˜“è³‡æ–™å·²å„²å­˜è‡³: ../data/processed/03_enhanced_transactions.csv")

# 2. å„²å­˜ä¹¾æ·¨çš„è³‡æ–™é›†ï¼ˆåƒ…æœ‰æ•ˆäº¤æ˜“ï¼‰
clean_transaction_df.to_csv('../data/processed/03_clean_transactions.csv', 
                           index=False, encoding='utf-8-sig')
print("âœ… ä¹¾æ·¨äº¤æ˜“è³‡æ–™å·²å„²å­˜è‡³: ../data/processed/03_clean_transactions.csv")

# 3. å„²å­˜é‡è¤‡äº¤æ˜“åˆ†æçµæœ
if not valid_results_df.empty:
    duplicate_analysis_summary = valid_results_df[[
        'property_id', 'total_transactions', 'normal_count', 'cancelled_count',
        'is_valid', 'selection_reason', 'duplicate_count'
    ]].copy()
    
    duplicate_analysis_summary.to_csv('../data/processed/03_duplicate_analysis.csv', 
                                     index=False, encoding='utf-8-sig')
    print("âœ… é‡è¤‡äº¤æ˜“åˆ†æçµæœå·²å„²å­˜è‡³: ../data/processed/03_duplicate_analysis.csv")

# %%
# ç”Ÿæˆè™•ç†ç¸½çµå ±å‘Š
processing_summary = {
    'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
    'original_transactions': len(transaction_df),
    'unique_properties': transaction_df['ç‰©ä»¶å”¯ä¸€ID'].nunique(),
    'duplicate_properties': len(duplicate_properties) if len(duplicate_properties) > 0 else 0,
    'duplicate_transactions': len(transaction_df[transaction_df['æ˜¯å¦é‡è¤‡äº¤æ˜“']]),
    'valid_transactions': transaction_df['æ˜¯å¦æœ‰æ•ˆäº¤æ˜“'].sum(),
    'invalid_transactions': (~transaction_df['æ˜¯å¦æœ‰æ•ˆäº¤æ˜“']).sum(),
    'data_retention_rate': transaction_df['æ˜¯å¦æœ‰æ•ˆäº¤æ˜“'].sum() / len(transaction_df) * 100,
    'avg_price_change': clean_transaction_df['äº¤æ˜“ç¸½åƒ¹'].mean() - transaction_df['äº¤æ˜“ç¸½åƒ¹'].mean(),
    'avg_unit_price_change': clean_transaction_df['å»ºç‰©å–®åƒ¹'].mean() - transaction_df['å»ºç‰©å–®åƒ¹'].mean(),
    'cancellation_rate_before': transaction_df['è§£ç´„æƒ…å½¢'].notna().sum() / len(transaction_df) * 100,
    'cancellation_rate_after': clean_transaction_df['è§£ç´„æƒ…å½¢'].notna().sum() / len(clean_transaction_df) * 100
}

# è½‰æ›ç‚ºDataFrameä¸¦å„²å­˜
summary_df = pd.DataFrame([processing_summary])
summary_df.to_csv('../data/processed/03_processing_summary.csv', 
                 index=False, encoding='utf-8-sig')
print("âœ… è™•ç†ç¸½çµå·²å„²å­˜è‡³: ../data/processed/03_processing_summary.csv")

# %%
# é©—è­‰è™•ç†çµæœ
print("\nğŸ” è™•ç†çµæœé©—è­‰")
print("=" * 50)

# 1. åŸºæœ¬æ•¸é‡é©—è­‰
print("1ï¸âƒ£ åŸºæœ¬æ•¸é‡é©—è­‰:")
print(f"   åŸå§‹äº¤æ˜“ç­†æ•¸: {len(transaction_df):,}")
print(f"   æœ‰æ•ˆäº¤æ˜“ç­†æ•¸: {len(clean_transaction_df):,}")
print(f"   ç§»é™¤ç­†æ•¸: {len(transaction_df) - len(clean_transaction_df):,}")
print(f"   è³‡æ–™ä¿ç•™ç‡: {len(clean_transaction_df)/len(transaction_df)*100:.2f}%")

# 2. å”¯ä¸€æ€§é©—è­‰
print(f"\n2ï¸âƒ£ å”¯ä¸€æ€§é©—è­‰:")
remaining_duplicates = clean_transaction_df['ç‰©ä»¶å”¯ä¸€ID'].value_counts()
remaining_duplicate_count = (remaining_duplicates > 1).sum()
print(f"   è™•ç†å¾Œå‰©é¤˜é‡è¤‡ç‰©ä»¶: {remaining_duplicate_count} å€‹")

if remaining_duplicate_count > 0:
    print("   âš ï¸ ä»å­˜åœ¨é‡è¤‡äº¤æ˜“ï¼Œéœ€è¦é€²ä¸€æ­¥æª¢æŸ¥")
    print("   å‰©é¤˜é‡è¤‡æ¡ˆä¾‹:")
    for property_id, count in remaining_duplicates[remaining_duplicates > 1].head().items():
        print(f"      {property_id}: {count} ç­†")
else:
    print("   âœ… æ‰€æœ‰é‡è¤‡äº¤æ˜“å·²æ­£ç¢ºè™•ç†")

# 3. è§£ç´„ç‡å½±éŸ¿é©—è­‰
print(f"\n3ï¸âƒ£ è§£ç´„ç‡å½±éŸ¿é©—è­‰:")
print(f"   è™•ç†å‰è§£ç´„ç‡: {transaction_df['è§£ç´„æƒ…å½¢'].notna().sum() / len(transaction_df) * 100:.3f}%")
print(f"   è™•ç†å¾Œè§£ç´„ç‡: {clean_transaction_df['è§£ç´„æƒ…å½¢'].notna().sum() / len(clean_transaction_df) * 100:.3f}%")
print(f"   è§£ç´„ç‡è®ŠåŒ–: {(clean_transaction_df['è§£ç´„æƒ…å½¢'].notna().sum() / len(clean_transaction_df) - transaction_df['è§£ç´„æƒ…å½¢'].notna().sum() / len(transaction_df)) * 100:+.3f}%")

# %% [markdown]
# ## 10. åˆ†æç¸½çµèˆ‡å»ºè­°

# %%
# é‡è¤‡äº¤æ˜“è™•ç†åˆ†æç¸½çµ
print("ğŸ“‹ é‡è¤‡äº¤æ˜“è™•ç†åˆ†æç¸½çµ")
print("=" * 80)

print("1ï¸âƒ£ è™•ç†æˆæœ:")
print(f"   âœ… æˆåŠŸè­˜åˆ¥ {len(duplicate_properties) if len(duplicate_properties) > 0 else 0:,} å€‹é‡è¤‡äº¤æ˜“ç‰©ä»¶")
print(f"   âœ… è™•ç† {len(transaction_df) - len(clean_transaction_df):,} ç­†é‡è¤‡äº¤æ˜“")
print(f"   âœ… è³‡æ–™ä¿ç•™ç‡é” {len(clean_transaction_df)/len(transaction_df)*100:.2f}%")
print(f"   âœ… å»é‡å¾Œå”¯ä¸€ç‰©ä»¶æ¯”ä¾‹: {clean_transaction_df['ç‰©ä»¶å”¯ä¸€ID'].nunique()/len(clean_transaction_df)*100:.2f}%")

print(f"\n2ï¸âƒ£ å“è³ªæå‡:")
if len(duplicate_properties) > 0:
    avg_duplicates_per_property = len(transaction_df[transaction_df['æ˜¯å¦é‡è¤‡äº¤æ˜“']]) / len(duplicate_properties)
    print(f"   ğŸ“Š å¹³å‡æ¯å€‹é‡è¤‡ç‰©ä»¶æ¸›å°‘ {avg_duplicates_per_property-1:.1f} ç­†å†—é¤˜äº¤æ˜“")

price_change_pct = (clean_transaction_df['äº¤æ˜“ç¸½åƒ¹'].mean() - transaction_df['äº¤æ˜“ç¸½åƒ¹'].mean()) / transaction_df['äº¤æ˜“ç¸½åƒ¹'].mean() * 100
unit_price_change_pct = (clean_transaction_df['å»ºç‰©å–®åƒ¹'].mean() - transaction_df['å»ºç‰©å–®åƒ¹'].mean()) / transaction_df['å»ºç‰©å–®åƒ¹'].mean() * 100

print(f"   ğŸ’° å¹³å‡äº¤æ˜“ç¸½åƒ¹è®ŠåŒ–: {price_change_pct:+.2f}%")
print(f"   ğŸ  å¹³å‡å»ºç‰©å–®åƒ¹è®ŠåŒ–: {unit_price_change_pct:+.2f}%")

print(f"\n3ï¸âƒ£ ä¸»è¦ç™¼ç¾:")
if 'city_impact' in locals() and city_impact:
    highest_removal_city = max(city_impact.items(), key=lambda x: x[1]['removal_rate'])
    print(f"   ğŸ—ºï¸ é‡è¤‡äº¤æ˜“æ¯”ä¾‹æœ€é«˜ç¸£å¸‚: {highest_removal_city[0]} ({highest_removal_city[1]['removal_rate']:.2f}%)")

if not valid_results_df.empty:
    all_cancelled_count = len(valid_results_df[~valid_results_df['is_valid']])
    print(f"   âš ï¸ å…¨éƒ¨è§£ç´„ç‰©ä»¶: {all_cancelled_count} å€‹")

print(f"\n4ï¸âƒ£ å¾ŒçºŒå»ºè­°:")
print("   ğŸ“ å»ºè­°å®šæœŸåŸ·è¡Œé‡è¤‡äº¤æ˜“æª¢æŸ¥æ©Ÿåˆ¶")
print("   ğŸ” å»ºç«‹è‡ªå‹•åŒ–ç•°å¸¸å€¼æª¢æ¸¬æµç¨‹")
print("   ğŸ“Š æŒçºŒç›£æ§è³‡æ–™å“è³ªæŒ‡æ¨™")

if remaining_duplicate_count > 0:
    print("   âš ï¸ å»ºè­°é€²ä¸€æ­¥èª¿æŸ¥å‰©é¤˜é‡è¤‡æ¡ˆä¾‹çš„åŸå› ")

# å“è³ªå•é¡Œå»ºè­°
if consistency_issues:
    total_issues = sum(consistency_issues.values())
    issue_rate = total_issues / len(clean_transaction_df) * 100
    if issue_rate > 5:
        print(f"   ğŸš¨ è³‡æ–™å“è³ªå•é¡Œæ¯”ä¾‹è¼ƒé«˜ ({issue_rate:.1f}%)ï¼Œå»ºè­°åŠ å¼·è³‡æ–™é©—è­‰")

print(f"\n5ï¸âƒ£ ä¸‹ä¸€æ­¥å·¥ä½œ:")
print("   ğŸ¯ é€²è¡Œç¤¾å€ç´šå»åŒ–ç‡è¨ˆç®— (Notebook 4)")
print("   ğŸ“ˆ å»ºç«‹ä¸‰å±¤ç´šå¸‚å ´åˆ†ææ¶æ§‹")
print("   ğŸš¨ å¯¦ä½œé¢¨éšªè©•ä¼°èˆ‡é è­¦æ©Ÿåˆ¶")

# %% [markdown]
# ## 11. ä¸‹ä¸€æ­¥å·¥ä½œé‡é»
# 
# ### âœ… å·²å®Œæˆé …ç›®:
# 1. ç‰©ä»¶å”¯ä¸€IDå»ºç«‹é‚è¼¯å¯¦ä½œ
# 2. é‡è¤‡äº¤æ˜“è­˜åˆ¥èˆ‡åˆ†çµ„æ©Ÿåˆ¶
# 3. æœ‰æ•ˆäº¤æ˜“åˆ¤æ–·è¦å‰‡å¯¦ä½œ
# 4. å»é‡è™•ç†çµæœé©—è­‰
# 5. é‡è¤‡äº¤æ˜“æ¨¡å¼èˆ‡å½±éŸ¿åˆ†æ
# 6. è³‡æ–™å“è³ªè©•ä¼°èˆ‡æ”¹å–„
# 
# ### ğŸ”„ å¾…é€²è¡Œé …ç›®:
# 1. **Notebook 4**: ç¤¾å€ç´šå»åŒ–ç‡åˆ†æ
#    - å»ºæ¡ˆåŸºæœ¬è³‡è¨ŠåŒ¹é…
#    - å»åŒ–ç‡è¨ˆç®—ï¼ˆæ¯›/æ·¨/èª¿æ•´ï¼‰
#    - å»åŒ–å‹•æ…‹åˆ†æï¼ˆé€Ÿåº¦/åŠ é€Ÿåº¦ï¼‰
# 
# 2. **Notebook 5**: è¡Œæ”¿å€ç´šèšåˆåˆ†æ
#    - å€åŸŸè§£ç´„é¢¨éšªèšåˆ
#    - å€åŸŸå»åŒ–æ•ˆç‡æ’å
#    - å€åŸŸé¢¨éšªç­‰ç´šè©•ä¼°
# 
# ### ğŸ¯ é—œéµæˆæœ:
# 1. è³‡æ–™ä¿ç•™ç‡ {len(clean_transaction_df)/len(transaction_df)*100:.2f}% ç¬¦åˆå“è³ªè¦æ±‚
# 2. é‡è¤‡äº¤æ˜“è™•ç†é‚è¼¯é‹ä½œæ­£å¸¸
# 3. ç‚ºå¾ŒçºŒä¸‰å±¤ç´šåˆ†æå»ºç«‹äº†å¯é çš„è³‡æ–™åŸºç¤
# 4. è³‡æ–™å“è³ªå•é¡Œå·²è­˜åˆ¥ä¸¦æ¨™è¨˜

print("\n" + "="*80)
print("ğŸ‰ Notebook 3 - é‡è¤‡äº¤æ˜“è­˜åˆ¥èˆ‡è™•ç†å®Œæˆï¼")
print("ğŸ“ è«‹ç¹¼çºŒåŸ·è¡Œ Notebook 4 é€²è¡Œç¤¾å€ç´šå»åŒ–ç‡åˆ†æ")
print("="*80)