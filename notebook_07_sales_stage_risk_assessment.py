# é å”®å±‹å¸‚å ´åˆ†æç³»çµ± - 07_éŠ·å”®éšæ®µåˆ¤æ–·èˆ‡é¢¨éšªè©•ä¼°
# åŸºæ–¼ PRD v2.3 è¦æ ¼é€²è¡ŒéŠ·å”®éšæ®µåˆ¤æ–·ã€è§£ç´„é¢¨éšªè©•ä¼°èˆ‡ç¶œåˆé¢¨éšªåˆ†æ
# ================================================================================

# %% [markdown]
# # é å”®å±‹å¸‚å ´åˆ†æç³»çµ± - éŠ·å”®éšæ®µåˆ¤æ–·èˆ‡é¢¨éšªè©•ä¼°
# 
# ## ğŸ“‹ ç›®æ¨™
# - âœ… å¯¦ä½œéŠ·å”®éšæ®µåˆ¤æ–·é‚è¼¯
# - âœ… å»ºç«‹è§£ç´„é¢¨éšªè©•ä¼°ç³»çµ±
# - âœ… æ•´åˆå¤šç¶­åº¦é¢¨éšªæŒ‡æ¨™
# - âœ… å¯¦ä½œé•·æœŸæ»¯éŠ·é¢¨éšªè©•ä¼°
# - âœ… å»ºç«‹ç¶œåˆé¢¨éšªè©•åˆ†æ©Ÿåˆ¶
# - âœ… è¨­å®šé¢¨éšªé è­¦é–¾å€¼
# - âœ… æº–å‚™å®Œæ•´ç¤¾å€ç´šå ±å‘Šè³‡æ–™
# 
# ## ğŸ¯ å…§å®¹å¤§ç¶±
# 1. éŠ·å”®éšæ®µåˆ¤æ–·é‚è¼¯å¯¦ä½œ
# 2. éšæ®µè¡¨ç¾è©•ç´šç³»çµ±
# 3. è§£ç´„é¢¨éšªåˆ†ç´šå¯¦ä½œ
# 4. é•·æœŸæ»¯éŠ·é¢¨éšªè©•ä¼°
# 5. ç¶œåˆé¢¨éšªè©•åˆ†æ©Ÿåˆ¶
# 6. é¢¨éšªé è­¦é–¾å€¼è¨­å®š
# 7. å¤šç¶­åº¦é¢¨éšªæ•´åˆåˆ†æ
# 8. ç¤¾å€ç´šå®Œæ•´å ±å‘Šæº–å‚™
# 
# ## ğŸ“Š å»¶çºŒ Notebook 1-6 çš„åˆ†æçµæœ
# - ä¹¾æ·¨äº¤æ˜“è³‡æ–™: å»é‡å¾Œçš„æœ‰æ•ˆäº¤æ˜“è¨˜éŒ„
# - è§£ç´„åˆ†æçµæœ: è§£ç´„è³‡æ–™è§£æèˆ‡çµ±è¨ˆ
# - å»ºæ¡ˆæ•´åˆçµæœ: æ´»èºå»ºæ¡ˆè­˜åˆ¥èˆ‡åŸºæœ¬è³‡è¨Š
# - å»åŒ–ç‡è¨ˆç®—çµæœ: æ¯›/æ·¨/èª¿æ•´å»åŒ–ç‡å®Œæ•´è¨ˆç®—
# - å»åŒ–å‹•æ…‹çµæœ: é€Ÿåº¦ã€åŠ é€Ÿåº¦ã€æ•ˆç‡è©•ç´šã€å®Œå”®é æ¸¬

# %% [markdown]
# ## 1. ç’°å¢ƒè¨­å®šèˆ‡è³‡æ–™è¼‰å…¥

# %%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import re
import warnings
from collections import Counter, defaultdict
import math
from scipy import stats
warnings.filterwarnings('ignore')

# è¨­å®šé¡¯ç¤ºé¸é …
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)
pd.set_option('display.width', None)
pd.set_option('display.max_colwidth', 80)

# è¨­å®šä¸­æ–‡å­—å‹
plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'Arial Unicode MS', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False

# è¨­å®šåœ–è¡¨æ¨£å¼
sns.set_style("whitegrid")
plt.style.use('default')

print("âœ… ç’°å¢ƒè¨­å®šå®Œæˆ")
print(f"ğŸ“… åˆ†ææ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

# %%
# è¼‰å…¥å‰éšæ®µè™•ç†çµæœ
print("ğŸ”„ è¼‰å…¥å‰éšæ®µè™•ç†çµæœ...")

try:
    # è¼‰å…¥å»åŒ–ç‡åˆ†æçµæœ
    absorption_analysis = pd.read_csv('../data/processed/05_absorption_rate_analysis.csv', encoding='utf-8')
    print(f"âœ… å»åŒ–ç‡åˆ†æçµæœ: {absorption_analysis.shape}")
    
    # è¼‰å…¥å»åŒ–å‹•æ…‹åˆ†æçµæœ
    quarterly_speed = pd.read_csv('../data/processed/06_quarterly_absorption_speed.csv', encoding='utf-8')
    print(f"âœ… å­£åº¦å»åŒ–é€Ÿåº¦: {quarterly_speed.shape}")
    
    absorption_acceleration = pd.read_csv('../data/processed/06_absorption_acceleration.csv', encoding='utf-8')
    print(f"âœ… å»åŒ–åŠ é€Ÿåº¦: {absorption_acceleration.shape}")
    
    completion_prediction = pd.read_csv('../data/processed/06_completion_prediction.csv', encoding='utf-8')
    print(f"âœ… å®Œå”®é æ¸¬: {completion_prediction.shape}")
    
    absorption_efficiency = pd.read_csv('../data/processed/06_absorption_efficiency.csv', encoding='utf-8')
    print(f"âœ… å»åŒ–æ•ˆç‡è©•ç´š: {absorption_efficiency.shape}")
    
    # è¼‰å…¥è§£ç´„åˆ†æçµæœ
    cancellation_analysis = pd.read_csv('../data/processed/02_cancellation_analysis.csv', encoding='utf-8')
    print(f"âœ… è§£ç´„åˆ†æçµæœ: {cancellation_analysis.shape}")
    
    # è¼‰å…¥æ´»èºå»ºæ¡ˆåˆ†æ
    active_projects = pd.read_csv('../data/processed/04_active_projects_analysis.csv', encoding='utf-8')
    print(f"âœ… æ´»èºå»ºæ¡ˆåˆ†æ: {active_projects.shape}")
    
    # è¼‰å…¥ä¹¾æ·¨äº¤æ˜“è³‡æ–™ï¼ˆç”¨æ–¼è¨ˆç®—éŠ·å”®å­£æ•¸ï¼‰
    clean_transactions = pd.read_csv('../data/processed/03_clean_transactions.csv', encoding='utf-8')
    print(f"âœ… ä¹¾æ·¨äº¤æ˜“è³‡æ–™: {clean_transactions.shape}")
    
except FileNotFoundError as e:
    print(f"âŒ æª”æ¡ˆè¼‰å…¥å¤±æ•—: {e}")
    print("ğŸ“ è«‹ç¢ºèªæ˜¯å¦å·²åŸ·è¡Œ Notebook 1-6")
except Exception as e:
    print(f"âŒ è¼‰å…¥éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")

# %%
# å¹´å­£è™•ç†å·¥å…·å‡½æ•¸
def season_to_number(season_str):
    """å°‡å¹´å­£å­—ä¸²è½‰æ›ç‚ºå¯æ¯”è¼ƒçš„æ•¸å­—"""
    try:
        if not season_str or pd.isna(season_str):
            return 0
        season_str = str(season_str).strip()
        if 'Y' not in season_str or 'S' not in season_str:
            return 0
        year_part = season_str.split('Y')[0]
        season_part = season_str.split('Y')[1].replace('S', '')
        return int(year_part) * 10 + int(season_part)
    except:
        return 0

def number_to_season(season_num):
    """å°‡æ•¸å­—è½‰æ›å›å¹´å­£å­—ä¸²"""
    try:
        if season_num <= 0:
            return ""
        year = season_num // 10
        season = season_num % 10
        return f"{year:03d}Y{season}S"
    except:
        return ""

def get_season_sequence(start_season, end_season):
    """ç²å–å¾é–‹å§‹åˆ°çµæŸçš„æ‰€æœ‰å¹´å­£åºåˆ—"""
    seasons = []
    current = start_season
    while season_to_number(current) <= season_to_number(end_season):
        seasons.append(current)
        current_num = season_to_number(current)
        year = current_num // 10
        season = current_num % 10
        if season == 4:
            next_num = (year + 1) * 10 + 1
        else:
            next_num = year * 10 + (season + 1)
        current = number_to_season(next_num)
        if current == "" or len(seasons) > 100:
            break
    return seasons

print("âœ… å¹´å­£è™•ç†å‡½æ•¸æº–å‚™å®Œæˆ")

# %%
# è³‡æ–™æ¦‚æ³æª¢è¦–
print("ğŸ“Š éŠ·å”®éšæ®µèˆ‡é¢¨éšªè©•ä¼°åŸºç¤è³‡æ–™æ¦‚æ³")
print("=" * 70)

print("å»åŒ–ç‡åˆ†æè³‡æ–™:")
print(f"   ç¸½è¨˜éŒ„æ•¸: {len(absorption_analysis):,}")
valid_absorption = absorption_analysis[absorption_analysis['calculation_status'] == 'success']
print(f"   æœ‰æ•ˆè¨˜éŒ„æ•¸: {len(valid_absorption):,}")
print(f"   å»ºæ¡ˆæ•¸: {valid_absorption['project_code'].nunique():,}")
print(f"   å¹´å­£ç¯„åœ: {valid_absorption['target_season'].nunique()} å€‹å¹´å­£")

print(f"\nå»åŒ–å‹•æ…‹è³‡æ–™:")
valid_speed = quarterly_speed[quarterly_speed['calculation_status'] == 'success']
print(f"   æœ‰æ•ˆé€Ÿåº¦è¨˜éŒ„: {len(valid_speed):,}")
valid_efficiency = absorption_efficiency[absorption_efficiency['calculation_status'] == 'success']
print(f"   æœ‰æ•ˆæ•ˆç‡è¨˜éŒ„: {len(valid_efficiency):,}")

print(f"\nè§£ç´„åˆ†æè³‡æ–™:")
print(f"   è§£ç´„è¨˜éŒ„æ•¸: {len(cancellation_analysis):,}")
if 'æ˜¯å¦è§£ç´„' in cancellation_analysis.columns:
    cancellation_count = len(cancellation_analysis[cancellation_analysis['æ˜¯å¦è§£ç´„'] == True])
    print(f"   å¯¦éš›è§£ç´„æ•¸: {cancellation_count:,}")
    print(f"   è§£ç´„ç‡: {cancellation_count/len(cancellation_analysis)*100:.2f}%")

print(f"\næ´»èºå»ºæ¡ˆè³‡æ–™:")
print(f"   å»ºæ¡ˆç¸½æ•¸: {len(active_projects):,}")
if 'is_active' in active_projects.columns:
    active_count = len(active_projects[active_projects['is_active'] == True])
    print(f"   æ´»èºå»ºæ¡ˆæ•¸: {active_count:,}")

# %% [markdown]
# ## 2. éŠ·å”®éšæ®µåˆ¤æ–·é‚è¼¯å¯¦ä½œ

# %%
# éŠ·å”®éšæ®µåˆ¤æ–·é‚è¼¯
print("ğŸ—ï¸ éŠ·å”®éšæ®µåˆ¤æ–·é‚è¼¯å¯¦ä½œ")
print("=" * 60)

def determine_sales_stage(project_code, target_season, absorption_data, speed_data, sales_seasons=None):
    """
    åˆ¤æ–·å»ºæ¡ˆéŠ·å”®éšæ®µ
    
    éŠ·å”®éšæ®µå®šç¾©ï¼š
    1. é–‹ç›¤åˆæœŸï¼šéŠ·å”®å­£æ•¸ â‰¤ 2
    2. ç©©å®šéŠ·å”®æœŸï¼šéŠ·å”®å­£æ•¸ 3-6 ä¸”å»åŒ–ç‡ < 80%
    3. ä¸­å¾ŒæœŸèª¿æ•´ï¼šéŠ·å”®å­£æ•¸ > 6 ä¸”å»åŒ–ç‡ < 90%
    4. å°¾ç›¤æ¸…å”®ï¼šå»åŒ–ç‡ 90-99%
    5. å®Œå”®ï¼šå»åŒ–ç‡ â‰¥ 100%
    
    Args:
        project_code: å»ºæ¡ˆç·¨è™Ÿ
        target_season: ç›®æ¨™å¹´å­£
        absorption_data: å»åŒ–ç‡è³‡æ–™
        speed_data: å»åŒ–é€Ÿåº¦è³‡æ–™
        sales_seasons: éŠ·å”®å­£æ•¸ï¼ˆå¯é¸ï¼‰
        
    Returns:
        dict: éŠ·å”®éšæ®µåˆ¤æ–·çµæœ
    """
    
    result = {
        'project_code': project_code,
        'target_season': target_season,
        'sales_stage': 'unknown',
        'sales_seasons': 0,
        'current_absorption_rate': 0.0,
        'quarterly_speed': 0.0,
        'stage_logic': '',
        'calculation_status': 'success',
        'error_message': ''
    }
    
    try:
        # ç²å–å»åŒ–ç‡è³‡æ–™
        absorption_row = absorption_data[
            (absorption_data['project_code'] == project_code) & 
            (absorption_data['target_season'] == target_season) &
            (absorption_data['calculation_status'] == 'success')
        ]
        
        if absorption_row.empty:
            result['calculation_status'] = 'error'
            result['error_message'] = 'æ‰¾ä¸åˆ°å»åŒ–ç‡è³‡æ–™'
            return result
        
        absorption_info = absorption_row.iloc[0]
        current_absorption_rate = absorption_info['net_absorption_rate']
        result['current_absorption_rate'] = current_absorption_rate
        
        # ç²å–å»åŒ–é€Ÿåº¦è³‡æ–™
        speed_row = speed_data[
            (speed_data['project_code'] == project_code) & 
            (speed_data['target_season'] == target_season) &
            (speed_data['calculation_status'] == 'success')
        ]
        
        quarterly_speed = 0.0
        if not speed_row.empty:
            quarterly_speed = speed_row.iloc[0]['quarterly_absorption_speed']
        result['quarterly_speed'] = quarterly_speed
        
        # è¨ˆç®—éŠ·å”®å­£æ•¸
        if sales_seasons is not None:
            calculated_seasons = sales_seasons
        elif 'sales_seasons' in absorption_info.index:
            calculated_seasons = absorption_info['sales_seasons']
        else:
            # å¾éŠ·å”®èµ·å§‹å¹´å­£æ¨ç®—
            start_season = absorption_info.get('start_season', '')
            if start_season:
                season_list = get_season_sequence(start_season, target_season)
                calculated_seasons = len(season_list)
            else:
                calculated_seasons = 1  # é è¨­å€¼
        
        result['sales_seasons'] = max(1, calculated_seasons)
        
        # éŠ·å”®éšæ®µåˆ¤æ–·é‚è¼¯
        if current_absorption_rate >= 100:
            result['sales_stage'] = 'å®Œå”®'
            result['stage_logic'] = f'å»åŒ–ç‡{current_absorption_rate:.1f}% â‰¥ 100%'
            
        elif current_absorption_rate >= 90:
            result['sales_stage'] = 'å°¾ç›¤æ¸…å”®'
            result['stage_logic'] = f'å»åŒ–ç‡{current_absorption_rate:.1f}% â‰¥ 90%'
            
        elif calculated_seasons <= 2:
            result['sales_stage'] = 'é–‹ç›¤åˆæœŸ'
            result['stage_logic'] = f'éŠ·å”®{calculated_seasons}å­£ â‰¤ 2å­£'
            
        elif calculated_seasons <= 6 and current_absorption_rate < 80:
            result['sales_stage'] = 'ç©©å®šéŠ·å”®æœŸ'
            result['stage_logic'] = f'éŠ·å”®{calculated_seasons}å­£(3-6å­£) ä¸” å»åŒ–ç‡{current_absorption_rate:.1f}% < 80%'
            
        elif calculated_seasons > 6 and current_absorption_rate < 90:
            result['sales_stage'] = 'ä¸­å¾ŒæœŸèª¿æ•´'
            result['stage_logic'] = f'éŠ·å”®{calculated_seasons}å­£ > 6å­£ ä¸” å»åŒ–ç‡{current_absorption_rate:.1f}% < 90%'
            
        else:
            # é‚Šç•Œæƒ…æ³è™•ç†
            if current_absorption_rate >= 80:
                result['sales_stage'] = 'å°¾ç›¤æ¸…å”®'
                result['stage_logic'] = f'å»åŒ–ç‡{current_absorption_rate:.1f}% â‰¥ 80% (é‚Šç•Œåˆ¤æ–·)'
            else:
                result['sales_stage'] = 'ä¸­å¾ŒæœŸèª¿æ•´'
                result['stage_logic'] = f'é‚Šç•Œæƒ…æ³ï¼šéŠ·å”®{calculated_seasons}å­£ï¼Œå»åŒ–ç‡{current_absorption_rate:.1f}%'
    
    except Exception as e:
        result['calculation_status'] = 'error'
        result['error_message'] = str(e)
    
    return result

# %%
# æ‰¹é‡è¨ˆç®—éŠ·å”®éšæ®µ
print("ğŸ”„ æ‰¹é‡è¨ˆç®—éŠ·å”®éšæ®µ...")

sales_stage_results = []

# å°æ‰€æœ‰æœ‰æ•ˆçš„å»åŒ–ç‡è¨˜éŒ„é€²è¡ŒéŠ·å”®éšæ®µåˆ¤æ–·
for _, row in valid_absorption.iterrows():
    result = determine_sales_stage(
        row['project_code'],
        row['target_season'],
        absorption_analysis,
        quarterly_speed
    )
    
    # æ·»åŠ åŸºæœ¬è³‡è¨Š
    result.update({
        'county': row.get('county', ''),
        'district': row.get('district', ''),
        'project_name': row.get('project_name', ''),
        'total_units': row.get('total_units', 0),
        'has_complete_info': row.get('has_complete_info', False)
    })
    
    sales_stage_results.append(result)

# è½‰æ›ç‚ºDataFrame
sales_stage_df = pd.DataFrame(sales_stage_results)

print(f"âœ… å®Œæˆ {len(sales_stage_df)} ç­†éŠ·å”®éšæ®µåˆ¤æ–·")

# %%
# éŠ·å”®éšæ®µçµ±è¨ˆåˆ†æ
print(f"\nğŸ“Š éŠ·å”®éšæ®µçµ±è¨ˆåˆ†æ:")

if not sales_stage_df.empty:
    successful_stage_calcs = sales_stage_df[sales_stage_df['calculation_status'] == 'success']
    
    print(f"è¨ˆç®—çµæœçµ±è¨ˆ:")
    print(f"   æˆåŠŸè¨ˆç®—: {len(successful_stage_calcs):,} ç­†")
    print(f"   è¨ˆç®—æˆåŠŸç‡: {len(successful_stage_calcs)/len(sales_stage_df)*100:.1f}%")
    
    if not successful_stage_calcs.empty:
        # éŠ·å”®éšæ®µåˆ†å¸ƒ
        stage_distribution = successful_stage_calcs['sales_stage'].value_counts()
        print(f"\néŠ·å”®éšæ®µåˆ†å¸ƒ:")
        total_valid = len(successful_stage_calcs)
        
        stage_order = ['é–‹ç›¤åˆæœŸ', 'ç©©å®šéŠ·å”®æœŸ', 'ä¸­å¾ŒæœŸèª¿æ•´', 'å°¾ç›¤æ¸…å”®', 'å®Œå”®']
        for stage in stage_order:
            if stage in stage_distribution.index:
                count = stage_distribution[stage]
                percentage = count / total_valid * 100
                print(f"   {stage}: {count:,} å€‹ ({percentage:.1f}%)")
        
        # å…¶ä»–éšæ®µ
        other_stages = [stage for stage in stage_distribution.index if stage not in stage_order]
        for stage in other_stages:
            count = stage_distribution[stage]
            percentage = count / total_valid * 100
            print(f"   {stage}: {count:,} å€‹ ({percentage:.1f}%)")
        
        # å„éšæ®µå¹³å‡å»åŒ–ç‡
        print(f"\nå„éšæ®µå¹³å‡å»åŒ–ç‡:")
        for stage in stage_order:
            stage_data = successful_stage_calcs[successful_stage_calcs['sales_stage'] == stage]
            if not stage_data.empty:
                avg_absorption = stage_data['current_absorption_rate'].mean()
                avg_speed = stage_data['quarterly_speed'].mean()
                avg_seasons = stage_data['sales_seasons'].mean()
                print(f"   {stage}: å»åŒ–ç‡{avg_absorption:.1f}%, é€Ÿåº¦{avg_speed:.2f}æˆ¶/å­£, å¹³å‡{avg_seasons:.1f}å­£")
        
        # ç¸£å¸‚åˆ¥éšæ®µåˆ†å¸ƒ
        if 'county' in successful_stage_calcs.columns:
            print(f"\nä¸»è¦ç¸£å¸‚éšæ®µåˆ†å¸ƒ:")
            city_stage = successful_stage_calcs.groupby(['county', 'sales_stage']).size().unstack(fill_value=0)
            city_totals = city_stage.sum(axis=1).sort_values(ascending=False)
            
            for county in city_totals.head(5).index:  # å‰5å¤§ç¸£å¸‚
                total_projects = city_totals[county]
                print(f"   {county} ({total_projects}å€‹):")
                for stage in stage_order:
                    if stage in city_stage.columns and city_stage.loc[county, stage] > 0:
                        count = city_stage.loc[county, stage]
                        percentage = count / total_projects * 100
                        print(f"     {stage}: {count}å€‹ ({percentage:.1f}%)")

# %% [markdown]
# ## 3. éšæ®µè¡¨ç¾è©•ç´šç³»çµ±

# %%
# éšæ®µè¡¨ç¾è©•ç´šé‚è¼¯
print("â­ éšæ®µè¡¨ç¾è©•ç´šç³»çµ±")
print("=" * 60)

def evaluate_stage_performance(sales_stage, sales_seasons, absorption_rate, quarterly_speed, target_season):
    """
    è©•ä¼°éŠ·å”®éšæ®µè¡¨ç¾
    
    Args:
        sales_stage: éŠ·å”®éšæ®µ
        sales_seasons: éŠ·å”®å­£æ•¸
        absorption_rate: ç•¶å‰å»åŒ–ç‡
        quarterly_speed: å­£åº¦å»åŒ–é€Ÿåº¦
        target_season: ç›®æ¨™å¹´å­£
        
    Returns:
        dict: éšæ®µè¡¨ç¾è©•ç´šçµæœ
    """
    
    result = {
        'sales_stage': sales_stage,
        'stage_performance': 'unknown',
        'performance_emoji': 'â“',
        'performance_score': 0.0,
        'performance_logic': '',
        'benchmark_comparison': ''
    }
    
    try:
        # ä¸åŒéšæ®µçš„è©•ç´šæ¨™æº–
        if sales_stage == 'é–‹ç›¤åˆæœŸ':
            # é–‹ç›¤åˆæœŸè©•ç´šæ¨™æº–ï¼ˆ1-2å­£ï¼‰
            if absorption_rate >= 30 and quarterly_speed >= 3:
                result.update({
                    'stage_performance': 'è‰¯å¥½',
                    'performance_emoji': 'ğŸŸ¢',
                    'performance_score': 85.0,
                    'performance_logic': f'é–‹ç›¤{sales_seasons}å­£é”{absorption_rate:.1f}%å»åŒ–ï¼Œé€Ÿåº¦{quarterly_speed:.1f}æˆ¶/å­£è¡¨ç¾å„ªç•°'
                })
            elif absorption_rate >= 20 and quarterly_speed >= 2:
                result.update({
                    'stage_performance': 'æ™®é€š',
                    'performance_emoji': 'ğŸŸ¡',
                    'performance_score': 65.0,
                    'performance_logic': f'é–‹ç›¤{sales_seasons}å­£é”{absorption_rate:.1f}%å»åŒ–ï¼Œé€Ÿåº¦{quarterly_speed:.1f}æˆ¶/å­£å°šå¯'
                })
            else:
                result.update({
                    'stage_performance': 'ä¸ä½³',
                    'performance_emoji': 'ğŸ”´',
                    'performance_score': 35.0,
                    'performance_logic': f'é–‹ç›¤{sales_seasons}å­£åƒ…{absorption_rate:.1f}%å»åŒ–ï¼Œé€Ÿåº¦{quarterly_speed:.1f}æˆ¶/å­£åæ…¢'
                })
        
        elif sales_stage == 'ç©©å®šéŠ·å”®æœŸ':
            # ç©©å®šéŠ·å”®æœŸè©•ç´šæ¨™æº–ï¼ˆ3-6å­£ï¼‰
            expected_absorption = sales_seasons * 12  # æœŸæœ›æ¯å­£12%
            if absorption_rate >= expected_absorption and quarterly_speed >= 2:
                result.update({
                    'stage_performance': 'è‰¯å¥½',
                    'performance_emoji': 'ğŸŸ¢',
                    'performance_score': 80.0,
                    'performance_logic': f'éŠ·å”®{sales_seasons}å­£é”{absorption_rate:.1f}%ï¼Œç¬¦åˆ{expected_absorption:.0f}%æœŸæœ›'
                })
            elif absorption_rate >= expected_absorption * 0.8 and quarterly_speed >= 1:
                result.update({
                    'stage_performance': 'æ™®é€š',
                    'performance_emoji': 'ğŸŸ¡',
                    'performance_score': 60.0,
                    'performance_logic': f'éŠ·å”®{sales_seasons}å­£é”{absorption_rate:.1f}%ï¼Œç•¥ä½æ–¼{expected_absorption:.0f}%æœŸæœ›'
                })
            else:
                result.update({
                    'stage_performance': 'ä¸ä½³',
                    'performance_emoji': 'ğŸ”´',
                    'performance_score': 40.0,
                    'performance_logic': f'éŠ·å”®{sales_seasons}å­£åƒ…{absorption_rate:.1f}%ï¼Œé ä½æ–¼{expected_absorption:.0f}%æœŸæœ›'
                })
        
        elif sales_stage == 'ä¸­å¾ŒæœŸèª¿æ•´':
            # ä¸­å¾ŒæœŸèª¿æ•´è©•ç´šæ¨™æº–ï¼ˆ>6å­£ä¸”<90%ï¼‰
            if absorption_rate >= 70 and quarterly_speed >= 1.5:
                result.update({
                    'stage_performance': 'è‰¯å¥½',
                    'performance_emoji': 'ğŸŸ¢',
                    'performance_score': 75.0,
                    'performance_logic': f'ä¸­å¾ŒæœŸ{sales_seasons}å­£é”{absorption_rate:.1f}%ï¼Œä»æœ‰è‰¯å¥½å»åŒ–å‹•èƒ½'
                })
            elif absorption_rate >= 50 and quarterly_speed >= 1:
                result.update({
                    'stage_performance': 'æ™®é€š',
                    'performance_emoji': 'ğŸŸ¡',
                    'performance_score': 55.0,
                    'performance_logic': f'ä¸­å¾ŒæœŸ{sales_seasons}å­£é”{absorption_rate:.1f}%ï¼Œéœ€åŠ å¼·å»åŒ–åŠ›é“'
                })
            else:
                result.update({
                    'stage_performance': 'ä¸ä½³',
                    'performance_emoji': 'ğŸ”´',
                    'performance_score': 30.0,
                    'performance_logic': f'ä¸­å¾ŒæœŸ{sales_seasons}å­£åƒ…{absorption_rate:.1f}%ï¼Œå»åŒ–åš´é‡é²ç·©'
                })
        
        elif sales_stage == 'å°¾ç›¤æ¸…å”®':
            # å°¾ç›¤æ¸…å”®è©•ç´šæ¨™æº–ï¼ˆ90-99%ï¼‰
            if quarterly_speed >= 2:
                result.update({
                    'stage_performance': 'è‰¯å¥½',
                    'performance_emoji': 'ğŸŸ¢',
                    'performance_score': 90.0,
                    'performance_logic': f'å°¾ç›¤éšæ®µ{absorption_rate:.1f}%ï¼Œæ¸…å”®é€Ÿåº¦{quarterly_speed:.1f}æˆ¶/å­£è‰¯å¥½'
                })
            elif quarterly_speed >= 1:
                result.update({
                    'stage_performance': 'æ™®é€š',
                    'performance_emoji': 'ğŸŸ¡',
                    'performance_score': 70.0,
                    'performance_logic': f'å°¾ç›¤éšæ®µ{absorption_rate:.1f}%ï¼Œæ¸…å”®é€Ÿåº¦{quarterly_speed:.1f}æˆ¶/å­£å°šå¯'
                })
            else:
                result.update({
                    'stage_performance': 'ä¸ä½³',
                    'performance_emoji': 'ğŸ”´',
                    'performance_score': 50.0,
                    'performance_logic': f'å°¾ç›¤éšæ®µ{absorption_rate:.1f}%ï¼Œæ¸…å”®é€Ÿåº¦{quarterly_speed:.1f}æˆ¶/å­£éæ…¢'
                })
        
        elif sales_stage == 'å®Œå”®':
            # å®Œå”®è©•ç´šæ¨™æº–ï¼ˆâ‰¥100%ï¼‰
            if sales_seasons <= 8:
                result.update({
                    'stage_performance': 'è‰¯å¥½',
                    'performance_emoji': 'ğŸŸ¢',
                    'performance_score': 95.0,
                    'performance_logic': f'{sales_seasons}å­£å®Œå”®ï¼ŒéŠ·å”®è¡¨ç¾å„ªç•°'
                })
            elif sales_seasons <= 12:
                result.update({
                    'stage_performance': 'æ™®é€š',
                    'performance_emoji': 'ğŸŸ¡',
                    'performance_score': 80.0,
                    'performance_logic': f'{sales_seasons}å­£å®Œå”®ï¼ŒéŠ·å”®é€Ÿåº¦æ­£å¸¸'
                })
            else:
                result.update({
                    'stage_performance': 'ä¸ä½³',
                    'performance_emoji': 'ğŸ”´',
                    'performance_score': 60.0,
                    'performance_logic': f'{sales_seasons}å­£æ‰å®Œå”®ï¼ŒéŠ·å”®è¼ƒç‚ºç·©æ…¢'
                })
        
        else:
            # æœªçŸ¥éšæ®µ
            result.update({
                'stage_performance': 'æœªçŸ¥',
                'performance_emoji': 'â“',
                'performance_score': 50.0,
                'performance_logic': f'ç„¡æ³•åˆ¤æ–·éšæ®µè¡¨ç¾'
            })
        
        # åŸºæº–æ¯”è¼ƒ
        if result['performance_score'] >= 80:
            result['benchmark_comparison'] = 'å„ªæ–¼å¸‚å ´å¹³å‡'
        elif result['performance_score'] >= 60:
            result['benchmark_comparison'] = 'ç¬¦åˆå¸‚å ´é æœŸ'
        else:
            result['benchmark_comparison'] = 'ä½æ–¼å¸‚å ´æ¨™æº–'
    
    except Exception as e:
        result['performance_logic'] = f'è©•ç´šéŒ¯èª¤: {str(e)}'
    
    return result

# %%
# æ‰¹é‡è¨ˆç®—éšæ®µè¡¨ç¾è©•ç´š
print("ğŸ”„ æ‰¹é‡è¨ˆç®—éšæ®µè¡¨ç¾è©•ç´š...")

stage_performance_results = []

# å°æ‰€æœ‰æˆåŠŸçš„éŠ·å”®éšæ®µåˆ¤æ–·é€²è¡Œè¡¨ç¾è©•ç´š
successful_stages = sales_stage_df[sales_stage_df['calculation_status'] == 'success']

for _, row in successful_stages.iterrows():
    performance_result = evaluate_stage_performance(
        row['sales_stage'],
        row['sales_seasons'],
        row['current_absorption_rate'],
        row['quarterly_speed'],
        row['target_season']
    )
    
    # åˆä½µåŸºæœ¬è³‡è¨Š
    combined_result = {
        'project_code': row['project_code'],
        'target_season': row['target_season'],
        'county': row.get('county', ''),
        'district': row.get('district', ''),
        'project_name': row.get('project_name', ''),
        'total_units': row.get('total_units', 0),
        'sales_seasons': row['sales_seasons'],
        'current_absorption_rate': row['current_absorption_rate'],
        'quarterly_speed': row['quarterly_speed'],
        'has_complete_info': row.get('has_complete_info', False),
        **performance_result
    }
    
    stage_performance_results.append(combined_result)

# è½‰æ›ç‚ºDataFrame
stage_performance_df = pd.DataFrame(stage_performance_results)

print(f"âœ… å®Œæˆ {len(stage_performance_df)} ç­†éšæ®µè¡¨ç¾è©•ç´š")

# %%
# éšæ®µè¡¨ç¾è©•ç´šçµ±è¨ˆåˆ†æ
print(f"\nğŸ“Š éšæ®µè¡¨ç¾è©•ç´šçµ±è¨ˆåˆ†æ:")

if not stage_performance_df.empty:
    print(f"éšæ®µè¡¨ç¾åˆ†å¸ƒ:")
    performance_distribution = stage_performance_df['stage_performance'].value_counts()
    total_records = len(stage_performance_df)
    
    for performance, count in performance_distribution.items():
        percentage = count / total_records * 100
        # å–å¾—å°æ‡‰çš„emoji
        sample_record = stage_performance_df[stage_performance_df['stage_performance'] == performance].iloc[0]
        emoji = sample_record['performance_emoji']
        print(f"   {emoji} {performance}: {count:,} å€‹ ({percentage:.1f}%)")
    
    # å„éšæ®µçš„è¡¨ç¾åˆ†å¸ƒ
    print(f"\nå„éŠ·å”®éšæ®µè¡¨ç¾åˆ†å¸ƒ:")
    stage_performance_cross = pd.crosstab(
        stage_performance_df['sales_stage'], 
        stage_performance_df['stage_performance'],
        normalize='index'
    ) * 100
    
    for stage in ['é–‹ç›¤åˆæœŸ', 'ç©©å®šéŠ·å”®æœŸ', 'ä¸­å¾ŒæœŸèª¿æ•´', 'å°¾ç›¤æ¸…å”®', 'å®Œå”®']:
        if stage in stage_performance_cross.index:
            print(f"   {stage}:")
            for performance in ['è‰¯å¥½', 'æ™®é€š', 'ä¸ä½³']:
                if performance in stage_performance_cross.columns:
                    percentage = stage_performance_cross.loc[stage, performance]
                    if percentage > 0:
                        print(f"     {performance}: {percentage:.1f}%")
    
    # å¹³å‡è¡¨ç¾åˆ†æ•¸
    print(f"\nå„éšæ®µå¹³å‡è¡¨ç¾åˆ†æ•¸:")
    avg_scores = stage_performance_df.groupby('sales_stage')['performance_score'].mean().sort_values(ascending=False)
    for stage, score in avg_scores.items():
        print(f"   {stage}: {score:.1f}åˆ†")
    
    # ç¸£å¸‚è¡¨ç¾æ¯”è¼ƒ
    if 'county' in stage_performance_df.columns:
        print(f"\nä¸»è¦ç¸£å¸‚è¡¨ç¾æ¯”è¼ƒ:")
        city_performance = stage_performance_df.groupby('county').agg({
            'performance_score': 'mean',
            'stage_performance': lambda x: (x == 'è‰¯å¥½').sum() / len(x) * 100
        }).round(1)
        city_performance.columns = ['å¹³å‡åˆ†æ•¸', 'è‰¯å¥½æ¯”ä¾‹%']
        city_performance = city_performance.sort_values('å¹³å‡åˆ†æ•¸', ascending=False)
        
        # åªé¡¯ç¤ºå»ºæ¡ˆæ•¸â‰¥5çš„ç¸£å¸‚
        city_counts = stage_performance_df['county'].value_counts()
        for county in city_performance.head(8).index:
            if city_counts[county] >= 5:
                score = city_performance.loc[county, 'å¹³å‡åˆ†æ•¸']
                good_ratio = city_performance.loc[county, 'è‰¯å¥½æ¯”ä¾‹%']
                count = city_counts[county]
                print(f"   {county}: {score:.1f}åˆ†, è‰¯å¥½ç‡{good_ratio:.1f}% ({count}å€‹å»ºæ¡ˆ)")

# %% [markdown]
# ## 4. è§£ç´„é¢¨éšªåˆ†ç´šå¯¦ä½œ

# %%
# è§£ç´„é¢¨éšªåˆ†ç´šé‚è¼¯
print("âš ï¸ è§£ç´„é¢¨éšªåˆ†ç´šå¯¦ä½œ")
print("=" * 60)

def assess_cancellation_risk(project_code, target_season, cancellation_data, absorption_data):
    """
    è©•ä¼°è§£ç´„é¢¨éšªç­‰ç´š
    
    Args:
        project_code: å»ºæ¡ˆç·¨è™Ÿ
        target_season: ç›®æ¨™å¹´å­£
        cancellation_data: è§£ç´„åˆ†æè³‡æ–™
        absorption_data: å»åŒ–ç‡è³‡æ–™
        
    Returns:
        dict: è§£ç´„é¢¨éšªè©•ä¼°çµæœ
    """
    
    result = {
        'project_code': project_code,
        'target_season': target_season,
        'cancellation_risk_level': 'unknown',
        'risk_emoji': 'â“',
        'cumulative_cancellation_count': 0,
        'cumulative_cancellation_rate': 0.0,
        'quarterly_cancellation_count': 0,
        'quarterly_cancellation_rate': 0.0,
        'consecutive_no_cancellation_seasons': 0,
        'latest_cancellation_season': '',
        'risk_score': 0.0,
        'risk_factors': [],
        'calculation_status': 'success',
        'error_message': ''
    }
    
    try:
        # ç²å–è©²å»ºæ¡ˆçš„æ‰€æœ‰è§£ç´„è¨˜éŒ„
        project_cancellations = cancellation_data[
            cancellation_data['å‚™æŸ¥ç·¨è™Ÿ'] == project_code
        ]
        
        if project_cancellations.empty:
            # æ²’æœ‰è©²å»ºæ¡ˆçš„è¨˜éŒ„ï¼Œè¦–ç‚ºä½é¢¨éšª
            result.update({
                'cancellation_risk_level': 'ä½é¢¨éšª',
                'risk_emoji': 'ğŸŸ¢',
                'risk_score': 10.0,
                'risk_factors': ['ç„¡è§£ç´„è¨˜éŒ„']
            })
            return result
        
        # è¨ˆç®—ç´¯ç©è§£ç´„çµ±è¨ˆ
        total_transactions = len(project_cancellations)
        if 'æ˜¯å¦è§£ç´„' in project_cancellations.columns:
            cancellation_records = project_cancellations[project_cancellations['æ˜¯å¦è§£ç´„'] == True]
            cumulative_cancellation_count = len(cancellation_records)
        else:
            # æ ¹æ“šè§£ç´„æƒ…å½¢æ¬„ä½åˆ¤æ–·
            cancellation_records = project_cancellations[
                project_cancellations['è§£ç´„æƒ…å½¢'].notna() & 
                project_cancellations['è§£ç´„æƒ…å½¢'].str.contains('è§£ç´„', na=False)
            ]
            cumulative_cancellation_count = len(cancellation_records)
        
        cumulative_cancellation_rate = (cumulative_cancellation_count / total_transactions * 100) if total_transactions > 0 else 0
        
        result.update({
            'cumulative_cancellation_count': cumulative_cancellation_count,
            'cumulative_cancellation_rate': cumulative_cancellation_rate
        })
        
        # è¨ˆç®—æœ¬å­£è§£ç´„çµ±è¨ˆ
        if 'äº¤æ˜“å¹´å­£' in project_cancellations.columns:
            current_season_cancellations = cancellation_records[
                cancellation_records['äº¤æ˜“å¹´å­£'] == target_season
            ]
            quarterly_cancellation_count = len(current_season_cancellations)
            
            current_season_total = len(project_cancellations[
                project_cancellations['äº¤æ˜“å¹´å­£'] == target_season
            ])
            quarterly_cancellation_rate = (quarterly_cancellation_count / current_season_total * 100) if current_season_total > 0 else 0
            
            result.update({
                'quarterly_cancellation_count': quarterly_cancellation_count,
                'quarterly_cancellation_rate': quarterly_cancellation_rate
            })
        
        # è¨ˆç®—æœ€è¿‘è§£ç´„æ™‚é–“èˆ‡é€£çºŒç„¡è§£ç´„å­£æ•¸
        if not cancellation_records.empty and 'è§£ç´„å¹´å­£' in cancellation_records.columns:
            latest_cancellation_seasons = cancellation_records['è§£ç´„å¹´å­£'].dropna()
            if not latest_cancellation_seasons.empty:
                latest_cancellation_season = max(latest_cancellation_seasons, key=season_to_number)
                result['latest_cancellation_season'] = latest_cancellation_season
                
                # è¨ˆç®—é€£çºŒç„¡è§£ç´„å­£æ•¸
                latest_season_num = season_to_number(latest_cancellation_season)
                current_season_num = season_to_number(target_season)
                if current_season_num > latest_season_num:
                    consecutive_seasons = 0
                    temp_season_num = latest_season_num
                    while temp_season_num < current_season_num:
                        temp_season_num += 1 if (temp_season_num % 10) < 4 else 7  # å­£åº¦åŠ 1æˆ–å¹´åº¦åŠ 1
                        consecutive_seasons += 1
                    result['consecutive_no_cancellation_seasons'] = consecutive_seasons - 1
        
        # é¢¨éšªè©•åˆ†è¨ˆç®—ï¼ˆ100åˆ†åˆ¶ï¼‰
        risk_score = 0
        risk_factors = []
        
        # 1. ç´¯ç©è§£ç´„ç‡è©•åˆ†ï¼ˆ0-40åˆ†ï¼‰
        if cumulative_cancellation_rate > 10:
            risk_score += 40
            risk_factors.append(f'ç´¯ç©è§£ç´„ç‡{cumulative_cancellation_rate:.1f}%éé«˜')
        elif cumulative_cancellation_rate > 5:
            risk_score += 25
            risk_factors.append(f'ç´¯ç©è§£ç´„ç‡{cumulative_cancellation_rate:.1f}%åé«˜')
        elif cumulative_cancellation_rate > 2:
            risk_score += 10
            risk_factors.append(f'ç´¯ç©è§£ç´„ç‡{cumulative_cancellation_rate:.1f}%ç•¥é«˜')
        
        # 2. å­£åº¦è§£ç´„ç‡è©•åˆ†ï¼ˆ0-30åˆ†ï¼‰
        if quarterly_cancellation_rate > 20:
            risk_score += 30
            risk_factors.append(f'æœ¬å­£è§£ç´„ç‡{quarterly_cancellation_rate:.1f}%åš´é‡')
        elif quarterly_cancellation_rate > 10:
            risk_score += 20
            risk_factors.append(f'æœ¬å­£è§£ç´„ç‡{quarterly_cancellation_rate:.1f}%åé«˜')
        elif quarterly_cancellation_rate > 5:
            risk_score += 10
            risk_factors.append(f'æœ¬å­£è§£ç´„ç‡{quarterly_cancellation_rate:.1f}%éœ€é—œæ³¨')
        
        # 3. è§£ç´„é »ç‡è©•åˆ†ï¼ˆ0-20åˆ†ï¼‰
        if cumulative_cancellation_count >= 10:
            risk_score += 20
            risk_factors.append(f'ç´¯ç©è§£ç´„{cumulative_cancellation_count}ç­†é »ç¹')
        elif cumulative_cancellation_count >= 5:
            risk_score += 10
            risk_factors.append(f'ç´¯ç©è§£ç´„{cumulative_cancellation_count}ç­†éœ€é—œæ³¨')
        
        # 4. é€£çºŒè§£ç´„è¶¨å‹¢è©•åˆ†ï¼ˆ0-10åˆ†ï¼‰
        if result['consecutive_no_cancellation_seasons'] == 0 and quarterly_cancellation_count > 0:
            risk_score += 10
            risk_factors.append('æœ¬å­£æœ‰æ–°è§£ç´„æ¡ˆä¾‹')
        elif result['consecutive_no_cancellation_seasons'] <= 1 and cumulative_cancellation_count > 0:
            risk_score += 5
            risk_factors.append('è¿‘æœŸä»æœ‰è§£ç´„æƒ…æ³')
        
        result['risk_score'] = min(100, risk_score)
        result['risk_factors'] = risk_factors
        
        # é¢¨éšªç­‰ç´šåˆ¤æ–·
        if risk_score >= 60:
            result.update({
                'cancellation_risk_level': 'é«˜é¢¨éšª',
                'risk_emoji': 'ğŸ”´'
            })
        elif risk_score >= 30:
            result.update({
                'cancellation_risk_level': 'ä¸­é¢¨éšª',
                'risk_emoji': 'ğŸŸ¡'
            })
        else:
            result.update({
                'cancellation_risk_level': 'ä½é¢¨éšª',
                'risk_emoji': 'ğŸŸ¢'
            })
    
    except Exception as e:
        result['calculation_status'] = 'error'
        result['error_message'] = str(e)
    
    return result

# %%
# æ‰¹é‡è¨ˆç®—è§£ç´„é¢¨éšªè©•ç´š
print("ğŸ”„ æ‰¹é‡è¨ˆç®—è§£ç´„é¢¨éšªè©•ç´š...")

cancellation_risk_results = []

# å°æ‰€æœ‰æœ‰æ•ˆçš„å»ºæ¡ˆè¨˜éŒ„é€²è¡Œè§£ç´„é¢¨éšªè©•ä¼°
unique_projects = valid_absorption[['project_code', 'target_season']].drop_duplicates()

for _, row in unique_projects.iterrows():
    risk_result = assess_cancellation_risk(
        row['project_code'],
        row['target_season'],
        cancellation_analysis,
        absorption_analysis
    )
    
    # æ·»åŠ åŸºæœ¬è³‡è¨Š
    project_info = valid_absorption[
        (valid_absorption['project_code'] == row['project_code']) & 
        (valid_absorption['target_season'] == row['target_season'])
    ].iloc[0]
    
    risk_result.update({
        'county': project_info.get('county', ''),
        'district': project_info.get('district', ''),
        'project_name': project_info.get('project_name', ''),
        'total_units': project_info.get('total_units', 0),
        'current_absorption_rate': project_info.get('net_absorption_rate', 0),
        'has_complete_info': project_info.get('has_complete_info', False)
    })
    
    cancellation_risk_results.append(risk_result)

# è½‰æ›ç‚ºDataFrame
cancellation_risk_df = pd.DataFrame(cancellation_risk_results)

print(f"âœ… å®Œæˆ {len(cancellation_risk_df)} ç­†è§£ç´„é¢¨éšªè©•ç´š")

# %%
# è§£ç´„é¢¨éšªåˆ†ç´šçµ±è¨ˆåˆ†æ
print(f"\nğŸ“Š è§£ç´„é¢¨éšªåˆ†ç´šçµ±è¨ˆåˆ†æ:")

if not cancellation_risk_df.empty:
    successful_risk_calcs = cancellation_risk_df[cancellation_risk_df['calculation_status'] == 'success']
    
    print(f"è¨ˆç®—çµæœçµ±è¨ˆ:")
    print(f"   æˆåŠŸè¨ˆç®—: {len(successful_risk_calcs):,} ç­†")
    
    if not successful_risk_calcs.empty:
        # é¢¨éšªç­‰ç´šåˆ†å¸ƒ
        risk_distribution = successful_risk_calcs['cancellation_risk_level'].value_counts()
        print(f"\nè§£ç´„é¢¨éšªç­‰ç´šåˆ†å¸ƒ:")
        total_valid = len(successful_risk_calcs)
        
        for risk_level, count in risk_distribution.items():
            percentage = count / total_valid * 100
            sample_record = successful_risk_calcs[successful_risk_calcs['cancellation_risk_level'] == risk_level].iloc[0]
            emoji = sample_record['risk_emoji']
            print(f"   {emoji} {risk_level}: {count:,} å€‹ ({percentage:.1f}%)")
        
        # è§£ç´„çµ±è¨ˆæ‘˜è¦
        print(f"\nè§£ç´„çµ±è¨ˆæ‘˜è¦:")
        total_cancellations = successful_risk_calcs['cumulative_cancellation_count'].sum()
        avg_cancellation_rate = successful_risk_calcs['cumulative_cancellation_rate'].mean()
        projects_with_cancellations = len(successful_risk_calcs[successful_risk_calcs['cumulative_cancellation_count'] > 0])
        
        print(f"   ç¸½è§£ç´„ç­†æ•¸: {total_cancellations:,} ç­†")
        print(f"   å¹³å‡è§£ç´„ç‡: {avg_cancellation_rate:.2f}%")
        print(f"   æœ‰è§£ç´„å»ºæ¡ˆ: {projects_with_cancellations:,} å€‹ ({projects_with_cancellations/total_valid*100:.1f}%)")
        
        # é«˜é¢¨éšªå»ºæ¡ˆè©³æƒ…
        high_risk_projects = successful_risk_calcs[successful_risk_calcs['cancellation_risk_level'] == 'é«˜é¢¨éšª']
        if not high_risk_projects.empty:
            print(f"\né«˜é¢¨éšªå»ºæ¡ˆè©³æƒ… (å‰5å€‹):")
            for i, (_, project) in enumerate(high_risk_projects.head(5).iterrows(), 1):
                print(f"   {i}. {project['project_code']} | {project.get('county', '')} | "
                      f"è§£ç´„ç‡{project['cumulative_cancellation_rate']:.1f}% | "
                      f"é¢¨éšªåˆ†æ•¸{project['risk_score']:.0f}")
        
        # é¢¨éšªåˆ†æ•¸åˆ†å¸ƒ
        print(f"\né¢¨éšªåˆ†æ•¸çµ±è¨ˆ:")
        print(f"   å¹³å‡é¢¨éšªåˆ†æ•¸: {successful_risk_calcs['risk_score'].mean():.1f}")
        print(f"   ä¸­ä½æ•¸é¢¨éšªåˆ†æ•¸: {successful_risk_calcs['risk_score'].median():.1f}")
        print(f"   æœ€é«˜é¢¨éšªåˆ†æ•¸: {successful_risk_calcs['risk_score'].max():.1f}")
        
        # ç¸£å¸‚é¢¨éšªåˆ†æ
        if 'county' in successful_risk_calcs.columns:
            print(f"\nç¸£å¸‚è§£ç´„é¢¨éšªåˆ†æ:")
            city_risk = successful_risk_calcs.groupby('county').agg({
                'risk_score': 'mean',
                'cumulative_cancellation_rate': 'mean',
                'cancellation_risk_level': lambda x: (x == 'é«˜é¢¨éšª').sum()
            }).round(2)
            city_risk.columns = ['å¹³å‡é¢¨éšªåˆ†æ•¸', 'å¹³å‡è§£ç´„ç‡%', 'é«˜é¢¨éšªå»ºæ¡ˆæ•¸']
            
            # åªé¡¯ç¤ºå»ºæ¡ˆæ•¸â‰¥3çš„ç¸£å¸‚
            city_counts = successful_risk_calcs['county'].value_counts()
            city_risk_filtered = city_risk[city_counts >= 3].sort_values('å¹³å‡é¢¨éšªåˆ†æ•¸', ascending=False)
            
            for county in city_risk_filtered.head(8).index:
                risk_score = city_risk_filtered.loc[county, 'å¹³å‡é¢¨éšªåˆ†æ•¸']
                cancellation_rate = city_risk_filtered.loc[county, 'å¹³å‡è§£ç´„ç‡%']
                high_risk_count = city_risk_filtered.loc[county, 'é«˜é¢¨éšªå»ºæ¡ˆæ•¸']
                total_count = city_counts[county]
                print(f"   {county}: é¢¨éšª{risk_score:.1f}åˆ†, è§£ç´„ç‡{cancellation_rate:.2f}%, "
                      f"é«˜é¢¨éšª{int(high_risk_count)}/{total_count}å€‹")

# %% [markdown]
# ## 5. é•·æœŸæ»¯éŠ·é¢¨éšªè©•ä¼°

# %%
# é•·æœŸæ»¯éŠ·é¢¨éšªè©•ä¼°é‚è¼¯
print("ğŸŒ é•·æœŸæ»¯éŠ·é¢¨éšªè©•ä¼°")
print("=" * 60)

def assess_long_term_stagnation_risk(project_code, target_season, absorption_data, speed_data, stage_data):
    """
    è©•ä¼°é•·æœŸæ»¯éŠ·é¢¨éšª
    
    é•·æœŸæ»¯éŠ·å®šç¾©ï¼š
    - éŠ·å”®æœŸé–“ > 12å­£ (3å¹´)
    - é€£çºŒ12å­£ç„¡æˆäº¤æˆ–å»åŒ–é€Ÿåº¦ < 0.5æˆ¶/å­£
    - ç´¯ç©å»åŒ–ç‡ < 70%
    
    Args:
        project_code: å»ºæ¡ˆç·¨è™Ÿ
        target_season: ç›®æ¨™å¹´å­£
        absorption_data: å»åŒ–ç‡è³‡æ–™
        speed_data: å»åŒ–é€Ÿåº¦è³‡æ–™
        stage_data: éšæ®µè³‡æ–™
        
    Returns:
        dict: é•·æœŸæ»¯éŠ·é¢¨éšªè©•ä¼°çµæœ
    """
    
    result = {
        'project_code': project_code,
        'target_season': target_season,
        'is_long_term_stagnant': False,
        'stagnation_risk_level': 'unknown',
        'stagnation_risk_emoji': 'â“',
        'sales_seasons': 0,
        'current_absorption_rate': 0.0,
        'avg_quarterly_speed': 0.0,
        'consecutive_slow_seasons': 0,
        'stagnation_score': 0.0,
        'stagnation_factors': [],
        'intervention_urgency': 'none',
        'calculation_status': 'success',
        'error_message': ''
    }
    
    try:
        # ç²å–å»åŒ–ç‡è³‡æ–™
        absorption_row = absorption_data[
            (absorption_data['project_code'] == project_code) & 
            (absorption_data['target_season'] == target_season) &
            (absorption_data['calculation_status'] == 'success')
        ]
        
        if absorption_row.empty:
            result['calculation_status'] = 'error'
            result['error_message'] = 'æ‰¾ä¸åˆ°å»åŒ–ç‡è³‡æ–™'
            return result
        
        absorption_info = absorption_row.iloc[0]
        current_absorption_rate = absorption_info['net_absorption_rate']
        result['current_absorption_rate'] = current_absorption_rate
        
        # ç²å–éŠ·å”®å­£æ•¸
        if 'sales_seasons' in absorption_info.index:
            sales_seasons = absorption_info['sales_seasons']
        else:
            # å¾éšæ®µè³‡æ–™ç²å–
            stage_row = stage_data[
                (stage_data['project_code'] == project_code) & 
                (stage_data['target_season'] == target_season)
            ]
            if not stage_row.empty:
                sales_seasons = stage_row.iloc[0]['sales_seasons']
            else:
                sales_seasons = 1  # é è¨­å€¼
        
        result['sales_seasons'] = sales_seasons
        
        # ç²å–è©²å»ºæ¡ˆçš„æ­·å²å»åŒ–é€Ÿåº¦
        project_speeds = speed_data[
            (speed_data['project_code'] == project_code) &
            (speed_data['calculation_status'] == 'success')
        ]
        
        if not project_speeds.empty:
            avg_speed = project_speeds['quarterly_absorption_speed'].mean()
            result['avg_quarterly_speed'] = avg_speed
            
            # è¨ˆç®—é€£çºŒç·©æ…¢å­£æ•¸
            slow_speeds = project_speeds[project_speeds['quarterly_absorption_speed'] < 0.5]
            result['consecutive_slow_seasons'] = len(slow_speeds)
        
        # æ»¯éŠ·è©•åˆ†è¨ˆç®—ï¼ˆ100åˆ†åˆ¶ï¼Œåˆ†æ•¸è¶Šé«˜é¢¨éšªè¶Šå¤§ï¼‰
        stagnation_score = 0
        stagnation_factors = []
        
        # 1. éŠ·å”®æ™‚é–“è©•åˆ†ï¼ˆ0-30åˆ†ï¼‰
        if sales_seasons > 16:  # è¶…é4å¹´
            stagnation_score += 30
            stagnation_factors.append(f'éŠ·å”®æœŸé–“{sales_seasons}å­£éé•·')
        elif sales_seasons > 12:  # è¶…é3å¹´
            stagnation_score += 20
            stagnation_factors.append(f'éŠ·å”®æœŸé–“{sales_seasons}å­£åé•·')
        elif sales_seasons > 8:  # è¶…é2å¹´
            stagnation_score += 10
            stagnation_factors.append(f'éŠ·å”®æœŸé–“{sales_seasons}å­£éœ€é—œæ³¨')
        
        # 2. å»åŒ–ç‡è©•åˆ†ï¼ˆ0-25åˆ†ï¼‰
        if current_absorption_rate < 50:
            stagnation_score += 25
            stagnation_factors.append(f'å»åŒ–ç‡{current_absorption_rate:.1f}%åš´é‡åä½')
        elif current_absorption_rate < 70:
            stagnation_score += 15
            stagnation_factors.append(f'å»åŒ–ç‡{current_absorption_rate:.1f}%åä½')
        elif current_absorption_rate < 80:
            stagnation_score += 5
            stagnation_factors.append(f'å»åŒ–ç‡{current_absorption_rate:.1f}%éœ€åŠªåŠ›')
        
        # 3. å»åŒ–é€Ÿåº¦è©•åˆ†ï¼ˆ0-25åˆ†ï¼‰
        if result['avg_quarterly_speed'] < 0.3:
            stagnation_score += 25
            stagnation_factors.append(f'å¹³å‡é€Ÿåº¦{result["avg_quarterly_speed"]:.2f}æˆ¶/å­£æ¥µæ…¢')
        elif result['avg_quarterly_speed'] < 0.5:
            stagnation_score += 20
            stagnation_factors.append(f'å¹³å‡é€Ÿåº¦{result["avg_quarterly_speed"]:.2f}æˆ¶/å­£å¾ˆæ…¢')
        elif result['avg_quarterly_speed'] < 1:
            stagnation_score += 10
            stagnation_factors.append(f'å¹³å‡é€Ÿåº¦{result["avg_quarterly_speed"]:.2f}æˆ¶/å­£åæ…¢')
        
        # 4. é€£çºŒç·©æ…¢è©•åˆ†ï¼ˆ0-20åˆ†ï¼‰
        if result['consecutive_slow_seasons'] >= 6:
            stagnation_score += 20
            stagnation_factors.append(f'é€£çºŒ{result["consecutive_slow_seasons"]}å­£å»åŒ–ç·©æ…¢')
        elif result['consecutive_slow_seasons'] >= 4:
            stagnation_score += 15
            stagnation_factors.append(f'é€£çºŒ{result["consecutive_slow_seasons"]}å­£å»åŒ–ç·©æ…¢')
        elif result['consecutive_slow_seasons'] >= 2:
            stagnation_score += 10
            stagnation_factors.append(f'é€£çºŒ{result["consecutive_slow_seasons"]}å­£å»åŒ–ç·©æ…¢')
        
        result['stagnation_score'] = min(100, stagnation_score)
        result['stagnation_factors'] = stagnation_factors
        
        # é•·æœŸæ»¯éŠ·åˆ¤æ–·
        is_long_term_stagnant = (
            sales_seasons > 12 and 
            current_absorption_rate < 70 and 
            result['avg_quarterly_speed'] < 0.5
        )
        result['is_long_term_stagnant'] = is_long_term_stagnant
        
        # é¢¨éšªç­‰ç´šåˆ¤æ–·
        if is_long_term_stagnant or stagnation_score >= 70:
            result.update({
                'stagnation_risk_level': 'é«˜æ»¯éŠ·é¢¨éšª',
                'stagnation_risk_emoji': 'ğŸ”´',
                'intervention_urgency': 'immediate'
            })
        elif stagnation_score >= 50:
            result.update({
                'stagnation_risk_level': 'ä¸­æ»¯éŠ·é¢¨éšª',
                'stagnation_risk_emoji': 'ğŸŸ¡',
                'intervention_urgency': 'moderate'
            })
        elif stagnation_score >= 30:
            result.update({
                'stagnation_risk_level': 'ä½æ»¯éŠ·é¢¨éšª',
                'stagnation_risk_emoji': 'ğŸŸ ',
                'intervention_urgency': 'monitor'
            })
        else:
            result.update({
                'stagnation_risk_level': 'æ­£å¸¸éŠ·å”®',
                'stagnation_risk_emoji': 'ğŸŸ¢',
                'intervention_urgency': 'none'
            })
    
    except Exception as e:
        result['calculation_status'] = 'error'
        result['error_message'] = str(e)
    
    return result

# %%
# æ‰¹é‡è¨ˆç®—é•·æœŸæ»¯éŠ·é¢¨éšªè©•ä¼°
print("ğŸ”„ æ‰¹é‡è¨ˆç®—é•·æœŸæ»¯éŠ·é¢¨éšªè©•ä¼°...")

stagnation_risk_results = []

# å°æ‰€æœ‰æœ‰æ•ˆçš„å»ºæ¡ˆè¨˜éŒ„é€²è¡Œé•·æœŸæ»¯éŠ·é¢¨éšªè©•ä¼°
unique_projects = valid_absorption[['project_code', 'target_season']].drop_duplicates()

for _, row in unique_projects.iterrows():
    stagnation_result = assess_long_term_stagnation_risk(
        row['project_code'],
        row['target_season'],
        absorption_analysis,
        quarterly_speed,
        sales_stage_df
    )
    
    # æ·»åŠ åŸºæœ¬è³‡è¨Š
    project_info = valid_absorption[
        (valid_absorption['project_code'] == row['project_code']) & 
        (valid_absorption['target_season'] == row['target_season'])
    ].iloc[0]
    
    stagnation_result.update({
        'county': project_info.get('county', ''),
        'district': project_info.get('district', ''),
        'project_name': project_info.get('project_name', ''),
        'total_units': project_info.get('total_units', 0),
        'has_complete_info': project_info.get('has_complete_info', False)
    })
    
    stagnation_risk_results.append(stagnation_result)

# è½‰æ›ç‚ºDataFrame
stagnation_risk_df = pd.DataFrame(stagnation_risk_results)

print(f"âœ… å®Œæˆ {len(stagnation_risk_df)} ç­†é•·æœŸæ»¯éŠ·é¢¨éšªè©•ä¼°")

# %%
# é•·æœŸæ»¯éŠ·é¢¨éšªçµ±è¨ˆåˆ†æ
print(f"\nğŸ“Š é•·æœŸæ»¯éŠ·é¢¨éšªçµ±è¨ˆåˆ†æ:")

if not stagnation_risk_df.empty:
    successful_stagnation_calcs = stagnation_risk_df[stagnation_risk_df['calculation_status'] == 'success']
    
    print(f"è¨ˆç®—çµæœçµ±è¨ˆ:")
    print(f"   æˆåŠŸè¨ˆç®—: {len(successful_stagnation_calcs):,} ç­†")
    
    if not successful_stagnation_calcs.empty:
        # æ»¯éŠ·é¢¨éšªç­‰ç´šåˆ†å¸ƒ
        stagnation_distribution = successful_stagnation_calcs['stagnation_risk_level'].value_counts()
        print(f"\næ»¯éŠ·é¢¨éšªç­‰ç´šåˆ†å¸ƒ:")
        total_valid = len(successful_stagnation_calcs)
        
        for risk_level, count in stagnation_distribution.items():
            percentage = count / total_valid * 100
            sample_record = successful_stagnation_calcs[successful_stagnation_calcs['stagnation_risk_level'] == risk_level].iloc[0]
            emoji = sample_record['stagnation_risk_emoji']
            print(f"   {emoji} {risk_level}: {count:,} å€‹ ({percentage:.1f}%)")
        
        # é•·æœŸæ»¯éŠ·å»ºæ¡ˆçµ±è¨ˆ
        long_term_stagnant = successful_stagnation_calcs[successful_stagnation_calcs['is_long_term_stagnant'] == True]
        print(f"\né•·æœŸæ»¯éŠ·å»ºæ¡ˆçµ±è¨ˆ:")
        print(f"   ç¢ºèªé•·æœŸæ»¯éŠ·: {len(long_term_stagnant):,} å€‹ ({len(long_term_stagnant)/total_valid*100:.1f}%)")
        
        if not long_term_stagnant.empty:
            avg_seasons = long_term_stagnant['sales_seasons'].mean()
            avg_absorption = long_term_stagnant['current_absorption_rate'].mean()
            avg_speed = long_term_stagnant['avg_quarterly_speed'].mean()
            print(f"   å¹³å‡éŠ·å”®å­£æ•¸: {avg_seasons:.1f} å­£")
            print(f"   å¹³å‡å»åŒ–ç‡: {avg_absorption:.1f}%")
            print(f"   å¹³å‡å»åŒ–é€Ÿåº¦: {avg_speed:.2f} æˆ¶/å­£")
        
        # éœ€è¦ç«‹å³ä»‹å…¥çš„å»ºæ¡ˆ
        immediate_intervention = successful_stagnation_calcs[
            successful_stagnation_calcs['intervention_urgency'] == 'immediate'
        ]
        print(f"\néœ€ç«‹å³ä»‹å…¥å»ºæ¡ˆ:")
        print(f"   éœ€ç«‹å³ä»‹å…¥: {len(immediate_intervention):,} å€‹")
        
        if not immediate_intervention.empty:
            print(f"   åš´é‡æ»¯éŠ·æ¡ˆä¾‹ (å‰5å€‹):")
            top_stagnant = immediate_intervention.nlargest(5, 'stagnation_score')
            for i, (_, project) in enumerate(top_stagnant.iterrows(), 1):
                print(f"     {i}. {project['project_code']} | {project.get('county', '')} | "
                      f"{project['sales_seasons']}å­£ | å»åŒ–{project['current_absorption_rate']:.1f}% | "
                      f"æ»¯éŠ·åˆ†æ•¸{project['stagnation_score']:.0f}")
        
        # æ»¯éŠ·åˆ†æ•¸çµ±è¨ˆ
        print(f"\næ»¯éŠ·åˆ†æ•¸çµ±è¨ˆ:")
        print(f"   å¹³å‡æ»¯éŠ·åˆ†æ•¸: {successful_stagnation_calcs['stagnation_score'].mean():.1f}")
        print(f"   ä¸­ä½æ•¸æ»¯éŠ·åˆ†æ•¸: {successful_stagnation_calcs['stagnation_score'].median():.1f}")
        print(f"   æœ€é«˜æ»¯éŠ·åˆ†æ•¸: {successful_stagnation_calcs['stagnation_score'].max():.1f}")
        
        # ç¸£å¸‚æ»¯éŠ·åˆ†æ
        if 'county' in successful_stagnation_calcs.columns:
            print(f"\nç¸£å¸‚æ»¯éŠ·é¢¨éšªåˆ†æ:")
            city_stagnation = successful_stagnation_calcs.groupby('county').agg({
                'stagnation_score': 'mean',
                'is_long_term_stagnant': 'sum',
                'sales_seasons': 'mean'
            }).round(1)
            city_stagnation.columns = ['å¹³å‡æ»¯éŠ·åˆ†æ•¸', 'é•·æœŸæ»¯éŠ·æ•¸', 'å¹³å‡éŠ·å”®å­£æ•¸']
            
            # åªé¡¯ç¤ºå»ºæ¡ˆæ•¸â‰¥3çš„ç¸£å¸‚
            city_counts = successful_stagnation_calcs['county'].value_counts()
            city_stagnation_filtered = city_stagnation[city_counts >= 3].sort_values('å¹³å‡æ»¯éŠ·åˆ†æ•¸', ascending=False)
            
            for county in city_stagnation_filtered.head(8).index:
                stagnation_score = city_stagnation_filtered.loc[county, 'å¹³å‡æ»¯éŠ·åˆ†æ•¸']
                long_term_count = city_stagnation_filtered.loc[county, 'é•·æœŸæ»¯éŠ·æ•¸']
                avg_seasons = city_stagnation_filtered.loc[county, 'å¹³å‡éŠ·å”®å­£æ•¸']
                total_count = city_counts[county]
                print(f"   {county}: æ»¯éŠ·{stagnation_score:.1f}åˆ†, é•·æœŸæ»¯éŠ·{int(long_term_count)}/{total_count}å€‹, "
                      f"å¹³å‡{avg_seasons:.1f}å­£")

# %% [markdown]
# ## 6. ç¶œåˆé¢¨éšªè©•åˆ†æ©Ÿåˆ¶

# %%
# ç¶œåˆé¢¨éšªè©•åˆ†æ©Ÿåˆ¶
print("ğŸ¯ ç¶œåˆé¢¨éšªè©•åˆ†æ©Ÿåˆ¶")
print("=" * 60)

def calculate_comprehensive_risk_score(project_code, target_season, stage_performance_data, 
                                     cancellation_risk_data, stagnation_risk_data, efficiency_data):
    """
    è¨ˆç®—ç¶œåˆé¢¨éšªè©•åˆ†
    
    æ•´åˆå¤šå€‹ç¶­åº¦çš„é¢¨éšªè©•ä¼°ï¼š
    1. éšæ®µè¡¨ç¾é¢¨éšª (25%)
    2. è§£ç´„é¢¨éšª (25%) 
    3. æ»¯éŠ·é¢¨éšª (25%)
    4. æ•ˆç‡é¢¨éšª (25%)
    
    Args:
        project_code: å»ºæ¡ˆç·¨è™Ÿ
        target_season: ç›®æ¨™å¹´å­£
        stage_performance_data: éšæ®µè¡¨ç¾è³‡æ–™
        cancellation_risk_data: è§£ç´„é¢¨éšªè³‡æ–™
        stagnation_risk_data: æ»¯éŠ·é¢¨éšªè³‡æ–™
        efficiency_data: æ•ˆç‡è©•ç´šè³‡æ–™
        
    Returns:
        dict: ç¶œåˆé¢¨éšªè©•åˆ†çµæœ
    """
    
    result = {
        'project_code': project_code,
        'target_season': target_season,
        'comprehensive_risk_score': 0.0,
        'risk_level': 'unknown',
        'risk_emoji': 'â“',
        'stage_performance_risk': 0.0,
        'cancellation_risk': 0.0,
        'stagnation_risk': 0.0,
        'efficiency_risk': 0.0,
        'risk_components': {},
        'major_risk_factors': [],
        'risk_mitigation_priority': 'low',
        'calculation_status': 'success',
        'error_message': ''
    }
    
    try:
        # 1. éšæ®µè¡¨ç¾é¢¨éšª (0-25åˆ†)
        stage_row = stage_performance_data[
            (stage_performance_data['project_code'] == project_code) & 
            (stage_performance_data['target_season'] == target_season)
        ]
        
        if not stage_row.empty:
            performance_score = stage_row.iloc[0]['performance_score']
            # å°‡è¡¨ç¾åˆ†æ•¸è½‰æ›ç‚ºé¢¨éšªåˆ†æ•¸ï¼ˆè¡¨ç¾è¶Šä½ï¼Œé¢¨éšªè¶Šé«˜ï¼‰
            stage_risk = max(0, 25 - (performance_score / 100 * 25))
            result['stage_performance_risk'] = stage_risk
            
            if stage_risk > 15:
                result['major_risk_factors'].append('éšæ®µè¡¨ç¾ä¸ä½³')
        else:
            result['stage_performance_risk'] = 15  # é è¨­ä¸­ç­‰é¢¨éšª
        
        # 2. è§£ç´„é¢¨éšª (0-25åˆ†)
        cancellation_row = cancellation_risk_data[
            (cancellation_risk_data['project_code'] == project_code) & 
            (cancellation_risk_data['target_season'] == target_season) &
            (cancellation_risk_data['calculation_status'] == 'success')
        ]
        
        if not cancellation_row.empty:
            cancellation_risk_score = cancellation_row.iloc[0]['risk_score']
            # å°‡100åˆ†åˆ¶çš„é¢¨éšªåˆ†æ•¸è½‰æ›ç‚º25åˆ†åˆ¶
            cancellation_risk = (cancellation_risk_score / 100) * 25
            result['cancellation_risk'] = cancellation_risk
            
            if cancellation_risk > 15:
                result['major_risk_factors'].append('è§£ç´„é¢¨éšªé«˜')
        else:
            result['cancellation_risk'] = 5  # é è¨­ä½é¢¨éšª
        
        # 3. æ»¯éŠ·é¢¨éšª (0-25åˆ†)
        stagnation_row = stagnation_risk_data[
            (stagnation_risk_data['project_code'] == project_code) & 
            (stagnation_risk_data['target_season'] == target_season) &
            (stagnation_risk_data['calculation_status'] == 'success')
        ]
        
        if not stagnation_row.empty:
            stagnation_score = stagnation_row.iloc[0]['stagnation_score']
            # å°‡100åˆ†åˆ¶çš„æ»¯éŠ·åˆ†æ•¸è½‰æ›ç‚º25åˆ†åˆ¶
            stagnation_risk = (stagnation_score / 100) * 25
            result['stagnation_risk'] = stagnation_risk
            
            if stagnation_risk > 15:
                result['major_risk_factors'].append('æ»¯éŠ·é¢¨éšªé«˜')
        else:
            result['stagnation_risk'] = 5  # é è¨­ä½é¢¨éšª
        
        # 4. æ•ˆç‡é¢¨éšª (0-25åˆ†)
        efficiency_row = efficiency_data[
            (efficiency_data['project_code'] == project_code) & 
            (efficiency_data['target_season'] == target_season) &
            (efficiency_data['calculation_status'] == 'success')
        ]
        
        if not efficiency_row.empty:
            efficiency_score = efficiency_row.iloc[0]['efficiency_score']
            # å°‡æ•ˆç‡åˆ†æ•¸è½‰æ›ç‚ºé¢¨éšªåˆ†æ•¸ï¼ˆæ•ˆç‡è¶Šä½ï¼Œé¢¨éšªè¶Šé«˜ï¼‰
            efficiency_risk = max(0, 25 - (efficiency_score / 100 * 25))
            result['efficiency_risk'] = efficiency_risk
            
            if efficiency_risk > 15:
                result['major_risk_factors'].append('å»åŒ–æ•ˆç‡ä½')
        else:
            result['efficiency_risk'] = 10  # é è¨­ä¸­ç­‰é¢¨éšª
        
        # è¨ˆç®—ç¶œåˆé¢¨éšªåˆ†æ•¸
        comprehensive_score = (
            result['stage_performance_risk'] + 
            result['cancellation_risk'] + 
            result['stagnation_risk'] + 
            result['efficiency_risk']
        )
        result['comprehensive_risk_score'] = round(comprehensive_score, 2)
        
        # è©³ç´°é¢¨éšªçµ„æˆ
        result['risk_components'] = {
            'stage_performance': result['stage_performance_risk'],
            'cancellation': result['cancellation_risk'],
            'stagnation': result['stagnation_risk'],
            'efficiency': result['efficiency_risk']
        }
        
        # ç¶œåˆé¢¨éšªç­‰ç´šåˆ¤æ–·
        if comprehensive_score >= 70:
            result.update({
                'risk_level': 'æ¥µé«˜é¢¨éšª',
                'risk_emoji': 'ğŸ”´',
                'risk_mitigation_priority': 'critical'
            })
        elif comprehensive_score >= 55:
            result.update({
                'risk_level': 'é«˜é¢¨éšª',
                'risk_emoji': 'ğŸ”´',
                'risk_mitigation_priority': 'high'
            })
        elif comprehensive_score >= 40:
            result.update({
                'risk_level': 'ä¸­é¢¨éšª',
                'risk_emoji': 'ğŸŸ¡',
                'risk_mitigation_priority': 'moderate'
            })
        elif comprehensive_score >= 25:
            result.update({
                'risk_level': 'ä½é¢¨éšª',
                'risk_emoji': 'ğŸŸ ',
                'risk_mitigation_priority': 'low'
            })
        else:
            result.update({
                'risk_level': 'æ¥µä½é¢¨éšª',
                'risk_emoji': 'ğŸŸ¢',
                'risk_mitigation_priority': 'monitor'
            })
        
        # å¦‚æœæ²’æœ‰ä¸»è¦é¢¨éšªå› å­ï¼Œè¨­ç‚ºæ­£å¸¸
        if not result['major_risk_factors']:
            result['major_risk_factors'] = ['é¢¨éšªæ§åˆ¶è‰¯å¥½']
    
    except Exception as e:
        result['calculation_status'] = 'error'
        result['error_message'] = str(e)
    
    return result

# %%
# æ‰¹é‡è¨ˆç®—ç¶œåˆé¢¨éšªè©•åˆ†
print("ğŸ”„ æ‰¹é‡è¨ˆç®—ç¶œåˆé¢¨éšªè©•åˆ†...")

comprehensive_risk_results = []

# å°æ‰€æœ‰æœ‰æ•ˆçš„å»ºæ¡ˆè¨˜éŒ„é€²è¡Œç¶œåˆé¢¨éšªè©•åˆ†
unique_projects = valid_absorption[['project_code', 'target_season']].drop_duplicates()

for _, row in unique_projects.iterrows():
    comprehensive_result = calculate_comprehensive_risk_score(
        row['project_code'],
        row['target_season'],
        stage_performance_df,
        cancellation_risk_df,
        stagnation_risk_df,
        absorption_efficiency
    )
    
    # æ·»åŠ åŸºæœ¬è³‡è¨Š
    project_info = valid_absorption[
        (valid_absorption['project_code'] == row['project_code']) & 
        (valid_absorption['target_season'] == row['target_season'])
    ].iloc[0]
    
    comprehensive_result.update({
        'county': project_info.get('county', ''),
        'district': project_info.get('district', ''),
        'project_name': project_info.get('project_name', ''),
        'total_units': project_info.get('total_units', 0),
        'current_absorption_rate': project_info.get('net_absorption_rate', 0),
        'has_complete_info': project_info.get('has_complete_info', False)
    })
    
    comprehensive_risk_results.append(comprehensive_result)

# è½‰æ›ç‚ºDataFrame
comprehensive_risk_df = pd.DataFrame(comprehensive_risk_results)

print(f"âœ… å®Œæˆ {len(comprehensive_risk_df)} ç­†ç¶œåˆé¢¨éšªè©•åˆ†")

# %%
# ç¶œåˆé¢¨éšªè©•åˆ†çµ±è¨ˆåˆ†æ
print(f"\nğŸ“Š ç¶œåˆé¢¨éšªè©•åˆ†çµ±è¨ˆåˆ†æ:")

if not comprehensive_risk_df.empty:
    successful_comprehensive = comprehensive_risk_df[comprehensive_risk_df['calculation_status'] == 'success']
    
    print(f"è¨ˆç®—çµæœçµ±è¨ˆ:")
    print(f"   æˆåŠŸè¨ˆç®—: {len(successful_comprehensive):,} ç­†")
    
    if not successful_comprehensive.empty:
        # ç¶œåˆé¢¨éšªç­‰ç´šåˆ†å¸ƒ
        risk_distribution = successful_comprehensive['risk_level'].value_counts()
        print(f"\nç¶œåˆé¢¨éšªç­‰ç´šåˆ†å¸ƒ:")
        total_valid = len(successful_comprehensive)
        
        risk_order = ['æ¥µé«˜é¢¨éšª', 'é«˜é¢¨éšª', 'ä¸­é¢¨éšª', 'ä½é¢¨éšª', 'æ¥µä½é¢¨éšª']
        for risk_level in risk_order:
            if risk_level in risk_distribution.index:
                count = risk_distribution[risk_level]
                percentage = count / total_valid * 100
                sample_record = successful_comprehensive[successful_comprehensive['risk_level'] == risk_level].iloc[0]
                emoji = sample_record['risk_emoji']
                print(f"   {emoji} {risk_level}: {count:,} å€‹ ({percentage:.1f}%)")
        
        # é¢¨éšªåˆ†æ•¸çµ±è¨ˆ
        print(f"\nç¶œåˆé¢¨éšªåˆ†æ•¸çµ±è¨ˆ:")
        print(f"   å¹³å‡é¢¨éšªåˆ†æ•¸: {successful_comprehensive['comprehensive_risk_score'].mean():.1f}")
        print(f"   ä¸­ä½æ•¸é¢¨éšªåˆ†æ•¸: {successful_comprehensive['comprehensive_risk_score'].median():.1f}")
        print(f"   æœ€é«˜é¢¨éšªåˆ†æ•¸: {successful_comprehensive['comprehensive_risk_score'].max():.1f}")
        print(f"   æœ€ä½é¢¨éšªåˆ†æ•¸: {successful_comprehensive['comprehensive_risk_score'].min():.1f}")
        
        # å„é¢¨éšªçµ„æˆçš„å¹³å‡åˆ†æ•¸
        print(f"\nå„é¢¨éšªçµ„æˆå¹³å‡åˆ†æ•¸:")
        print(f"   éšæ®µè¡¨ç¾é¢¨éšª: {successful_comprehensive['stage_performance_risk'].mean():.1f}/25")
        print(f"   è§£ç´„é¢¨éšª: {successful_comprehensive['cancellation_risk'].mean():.1f}/25")
        print(f"   æ»¯éŠ·é¢¨éšª: {successful_comprehensive['stagnation_risk'].mean():.1f}/25")
        print(f"   æ•ˆç‡é¢¨éšª: {successful_comprehensive['efficiency_risk'].mean():.1f}/25")
        
        # é«˜é¢¨éšªå»ºæ¡ˆåˆ†æ
        high_risk_projects = successful_comprehensive[
            successful_comprehensive['risk_level'].isin(['æ¥µé«˜é¢¨éšª', 'é«˜é¢¨éšª'])
        ]
        
        if not high_risk_projects.empty:
            print(f"\né«˜é¢¨éšªå»ºæ¡ˆåˆ†æ:")
            print(f"   é«˜é¢¨éšªå»ºæ¡ˆæ•¸: {len(high_risk_projects):,} å€‹")
            print(f"   é«˜é¢¨éšªæ¯”ä¾‹: {len(high_risk_projects)/total_valid*100:.1f}%")
            
            # ä¸»è¦é¢¨éšªå› å­çµ±è¨ˆ
            all_risk_factors = []
            for factors_list in high_risk_projects['major_risk_factors']:
                if isinstance(factors_list, list):
                    all_risk_factors.extend(factors_list)
            
            if all_risk_factors:
                risk_factor_counts = Counter(all_risk_factors)
                print(f"   ä¸»è¦é¢¨éšªå› å­:")
                for factor, count in risk_factor_counts.most_common():
                    percentage = count / len(high_risk_projects) * 100
                    print(f"     {factor}: {count} å€‹ ({percentage:.1f}%)")
            
            # æœ€é«˜é¢¨éšªå»ºæ¡ˆè©³æƒ…
            print(f"\næœ€é«˜é¢¨éšªå»ºæ¡ˆ (å‰5å€‹):")
            top_risk_projects = high_risk_projects.nlargest(5, 'comprehensive_risk_score')
            for i, (_, project) in enumerate(top_risk_projects.iterrows(), 1):
                risk_factors = ', '.join(project['major_risk_factors']) if isinstance(project['major_risk_factors'], list) else str(project['major_risk_factors'])
                print(f"   {i}. {project['project_code']} | {project.get('county', '')} | "
                      f"é¢¨éšª{project['comprehensive_risk_score']:.1f}åˆ† | {risk_factors}")
        
        # é¢¨éšªç·©è§£å„ªå…ˆç´šåˆ†å¸ƒ
        priority_distribution = successful_comprehensive['risk_mitigation_priority'].value_counts()
        print(f"\né¢¨éšªç·©è§£å„ªå…ˆç´šåˆ†å¸ƒ:")
        for priority, count in priority_distribution.items():
            percentage = count / total_valid * 100
            print(f"   {priority}: {count:,} å€‹ ({percentage:.1f}%)")
        
        # ç¸£å¸‚ç¶œåˆé¢¨éšªåˆ†æ
        if 'county' in successful_comprehensive.columns:
            print(f"\nç¸£å¸‚ç¶œåˆé¢¨éšªåˆ†æ:")
            city_risk = successful_comprehensive.groupby('county').agg({
                'comprehensive_risk_score': 'mean',
                'risk_level': lambda x: (x.isin(['æ¥µé«˜é¢¨éšª', 'é«˜é¢¨éšª'])).sum()
            }).round(1)
            city_risk.columns = ['å¹³å‡é¢¨éšªåˆ†æ•¸', 'é«˜é¢¨éšªå»ºæ¡ˆæ•¸']
            
            # åªé¡¯ç¤ºå»ºæ¡ˆæ•¸â‰¥3çš„ç¸£å¸‚
            city_counts = successful_comprehensive['county'].value_counts()
            city_risk_filtered = city_risk[city_counts >= 3].sort_values('å¹³å‡é¢¨éšªåˆ†æ•¸', ascending=False)
            
            for county in city_risk_filtered.head(8).index:
                avg_risk = city_risk_filtered.loc[county, 'å¹³å‡é¢¨éšªåˆ†æ•¸']
                high_risk_count = city_risk_filtered.loc[county, 'é«˜é¢¨éšªå»ºæ¡ˆæ•¸']
                total_count = city_counts[county]
                print(f"   {county}: å¹³å‡{avg_risk:.1f}åˆ†, é«˜é¢¨éšª{int(high_risk_count)}/{total_count}å€‹")

# %% [markdown]
# ## 7. é¢¨éšªé è­¦é–¾å€¼è¨­å®š

# %%
# é¢¨éšªé è­¦é–¾å€¼è¨­å®šèˆ‡ç›£æ§æ©Ÿåˆ¶
print("ğŸš¨ é¢¨éšªé è­¦é–¾å€¼è¨­å®šèˆ‡ç›£æ§æ©Ÿåˆ¶")
print("=" * 60)

def establish_risk_warning_thresholds(comprehensive_risk_data, stage_data, cancellation_data, stagnation_data):
    """
    å»ºç«‹é¢¨éšªé è­¦é–¾å€¼èˆ‡ç›£æ§æ©Ÿåˆ¶
    
    Args:
        comprehensive_risk_data: ç¶œåˆé¢¨éšªè³‡æ–™
        stage_data: éšæ®µè³‡æ–™
        cancellation_data: è§£ç´„é¢¨éšªè³‡æ–™
        stagnation_data: æ»¯éŠ·é¢¨éšªè³‡æ–™
        
    Returns:
        dict: é¢¨éšªé è­¦é–¾å€¼è¨­å®š
    """
    
    warning_thresholds = {
        'comprehensive_risk': {},
        'stage_specific': {},
        'cancellation_risk': {},
        'stagnation_risk': {},
        'early_warning_indicators': {},
        'monitoring_framework': {}
    }
    
    try:
        valid_comprehensive = comprehensive_risk_data[comprehensive_risk_data['calculation_status'] == 'success']
        
        if not valid_comprehensive.empty:
            # 1. ç¶œåˆé¢¨éšªé–¾å€¼
            risk_scores = valid_comprehensive['comprehensive_risk_score']
            
            warning_thresholds['comprehensive_risk'] = {
                'critical_threshold': risk_scores.quantile(0.95),  # å‰5%æœ€é«˜é¢¨éšª
                'high_threshold': risk_scores.quantile(0.85),     # å‰15%é«˜é¢¨éšª
                'medium_threshold': risk_scores.quantile(0.65),   # å‰35%ä¸­é¢¨éšª
                'low_threshold': risk_scores.quantile(0.35),      # å‰65%ä½é¢¨éšª
                'statistical_benchmarks': {
                    'mean': risk_scores.mean(),
                    'median': risk_scores.median(),
                    'std': risk_scores.std(),
                    '75th_percentile': risk_scores.quantile(0.75),
                    '90th_percentile': risk_scores.quantile(0.90)
                }
            }
        
        # 2. éšæ®µç‰¹å®šé¢¨éšªé–¾å€¼
        valid_stage = stage_data[stage_data['calculation_status'] == 'success']
        
        if not valid_stage.empty:
            for stage in ['é–‹ç›¤åˆæœŸ', 'ç©©å®šéŠ·å”®æœŸ', 'ä¸­å¾ŒæœŸèª¿æ•´', 'å°¾ç›¤æ¸…å”®', 'å®Œå”®']:
                stage_data_subset = valid_stage[valid_stage['sales_stage'] == stage]
                if not stage_data_subset.empty and len(stage_data_subset) >= 10:
                    performance_scores = stage_data_subset['performance_score']
                    
                    warning_thresholds['stage_specific'][stage] = {
                        'poor_performance_threshold': performance_scores.quantile(0.25),  # æœ€ä½25%
                        'excellent_performance_threshold': performance_scores.quantile(0.75),  # æœ€é«˜25%
                        'stage_specific_warning': {
                            'min_acceptable_score': 50 if stage != 'å®Œå”®' else 60,
                            'intervention_threshold': 35 if stage != 'å®Œå”®' else 45
                        }
                    }
        
        # 3. è§£ç´„é¢¨éšªé–¾å€¼
        valid_cancellation = cancellation_data[cancellation_data['calculation_status'] == 'success']
        
        if not valid_cancellation.empty:
            cancellation_rates = valid_cancellation['cumulative_cancellation_rate']
            risk_scores = valid_cancellation['risk_score']
            
            warning_thresholds['cancellation_risk'] = {
                'rate_thresholds': {
                    'severe_threshold': max(10, cancellation_rates.quantile(0.95)),    # åš´é‡ï¼š10%æˆ–å‰5%
                    'high_threshold': max(5, cancellation_rates.quantile(0.85)),       # é«˜é¢¨éšªï¼š5%æˆ–å‰15%
                    'medium_threshold': max(2, cancellation_rates.quantile(0.65)),     # ä¸­é¢¨éšªï¼š2%æˆ–å‰35%
                    'market_average': cancellation_rates.mean()
                },
                'score_thresholds': {
                    'immediate_action': 80,      # ç«‹å³è¡Œå‹•
                    'close_monitoring': 60,      # å¯†åˆ‡ç›£æ§
                    'routine_monitoring': 30     # ä¾‹è¡Œç›£æ§
                },
                'frequency_thresholds': {
                    'multiple_cancellations': 5,     # å¤šæ¬¡è§£ç´„è­¦æˆ’
                    'cluster_cancellations': 3       # é›†ä¸­è§£ç´„è­¦æˆ’
                }
            }
        
        # 4. æ»¯éŠ·é¢¨éšªé–¾å€¼
        valid_stagnation = stagnation_data[stagnation_data['calculation_status'] == 'success']
        
        if not valid_stagnation.empty:
            stagnation_scores = valid_stagnation['stagnation_score']
            sales_seasons = valid_stagnation['sales_seasons']
            
            warning_thresholds['stagnation_risk'] = {
                'score_thresholds': {
                    'critical_stagnation': stagnation_scores.quantile(0.90),  # å‰10%æœ€æ»¯éŠ·
                    'severe_stagnation': stagnation_scores.quantile(0.75),    # å‰25%åš´é‡æ»¯éŠ·
                    'moderate_stagnation': stagnation_scores.quantile(0.50),  # å‰50%ä¸­åº¦æ»¯éŠ·
                },
                'time_thresholds': {
                    'long_term_threshold': 12,      # é•·æœŸï¼š12å­£
                    'extended_threshold': 16,       # å»¶é•·ï¼š16å­£
                    'excessive_threshold': 20       # éé•·ï¼š20å­£
                },
                'performance_thresholds': {
                    'minimal_absorption_rate': 30,  # æœ€ä½å»åŒ–ç‡30%
                    'minimal_quarterly_speed': 0.5, # æœ€ä½å­£åº¦é€Ÿåº¦0.5æˆ¶/å­£
                    'stagnation_speed_limit': 0.3   # æ»¯éŠ·é€Ÿåº¦ä¸Šé™0.3æˆ¶/å­£
                }
            }
        
        # 5. æ—©æœŸé è­¦æŒ‡æ¨™
        warning_thresholds['early_warning_indicators'] = {
            'speed_deceleration': {
                'consecutive_decline_seasons': 3,    # é€£çºŒ3å­£é€Ÿåº¦ä¸‹é™
                'speed_drop_percentage': 50,         # é€Ÿåº¦ä¸‹é™50%
                'near_zero_speed_threshold': 0.2     # æ¥è¿‘é›¶é€Ÿåº¦é–¾å€¼
            },
            'absorption_stagnation': {
                'quarterly_progress_minimum': 2,     # å­£åº¦æœ€ä½é€²åº¦2%
                'three_season_progress_minimum': 8,  # 3å­£æœ€ä½ç¸½é€²åº¦8%
                'absorption_plateau_threshold': 5    # å»åŒ–ç‡åœæ»¯é–¾å€¼5%
            },
            'market_anomaly': {
                'efficiency_score_drop': 20,         # æ•ˆç‡åˆ†æ•¸å¤§å¹…ä¸‹é™
                'multiple_risk_factors': 3,          # å¤šé‡é¢¨éšªå› å­åŒæ™‚å‡ºç¾
                'cross_stage_performance_decline': True  # è·¨éšæ®µè¡¨ç¾ä¸‹é™
            }
        }
        
        # 6. ç›£æ§æ¡†æ¶
        warning_thresholds['monitoring_framework'] = {
            'frequency': {
                'critical_projects': 'weekly',       # é—œéµé …ç›®ï¼šæ¯é€±
                'high_risk_projects': 'bi_weekly',   # é«˜é¢¨éšªé …ç›®ï¼šé›™é€±
                'medium_risk_projects': 'monthly',   # ä¸­é¢¨éšªé …ç›®ï¼šæ¯æœˆ
                'low_risk_projects': 'quarterly'     # ä½é¢¨éšªé …ç›®ï¼šæ¯å­£
            },
            'escalation_triggers': {
                'immediate_escalation': [
                    'comprehensive_risk_score > 80',
                    'cancellation_rate > 10%',
                    'stagnation_score > 80',
                    'sales_seasons > 20'
                ],
                'urgent_review': [
                    'comprehensive_risk_score > 65',
                    'cancellation_rate > 5%',
                    'stagnation_score > 60',
                    'consecutive_poor_performance > 2_seasons'
                ],
                'routine_review': [
                    'comprehensive_risk_score > 45',
                    'any_single_risk_component > 15',
                    'performance_decline_trend'
                ]
            },
            'action_protocols': {
                'critical_intervention': [
                    'senior_management_notification',
                    'emergency_strategy_review',
                    'immediate_corrective_actions',
                    'daily_monitoring'
                ],
                'standard_intervention': [
                    'management_notification',
                    'strategy_adjustment_review',
                    'weekly_monitoring',
                    'risk_mitigation_planning'
                ],
                'preventive_monitoring': [
                    'trend_analysis',
                    'benchmark_comparison',
                    'monthly_review',
                    'early_warning_tracking'
                ]
            }
        }
        
        # 7. é–¾å€¼æœ‰æ•ˆæ€§é©—è­‰
        warning_thresholds['validation_metrics'] = {
            'threshold_coverage': {
                'critical_capture_rate': len(valid_comprehensive[
                    valid_comprehensive['comprehensive_risk_score'] >= warning_thresholds['comprehensive_risk']['critical_threshold']
                ]) / len(valid_comprehensive) * 100,
                'high_risk_capture_rate': len(valid_comprehensive[
                    valid_comprehensive['comprehensive_risk_score'] >= warning_thresholds['comprehensive_risk']['high_threshold']
                ]) / len(valid_comprehensive) * 100
            },
            'false_positive_estimation': {
                'expected_false_positive_rate': 5,  # é æœŸèª¤å ±ç‡5%
                'threshold_adjustment_sensitivity': 0.1  # é–¾å€¼èª¿æ•´æ•æ„Ÿåº¦
            }
        }
    
    except Exception as e:
        print(f"âŒ é–¾å€¼è¨­å®šéŒ¯èª¤: {e}")
    
    return warning_thresholds

# %%
# å»ºç«‹é¢¨éšªé è­¦é–¾å€¼
print("ğŸ”„ å»ºç«‹é¢¨éšªé è­¦é–¾å€¼...")

risk_warning_thresholds = establish_risk_warning_thresholds(
    comprehensive_risk_df,
    sales_stage_df,
    cancellation_risk_df,
    stagnation_risk_df
)

print(f"âœ… å®Œæˆé¢¨éšªé è­¦é–¾å€¼è¨­å®š")

# %%
# é¢¨éšªé è­¦é–¾å€¼åˆ†æå ±å‘Š
print(f"\nğŸ“Š é¢¨éšªé è­¦é–¾å€¼åˆ†æå ±å‘Š:")

if risk_warning_thresholds:
    # ç¶œåˆé¢¨éšªé–¾å€¼
    if 'comprehensive_risk' in risk_warning_thresholds:
        comp_risk = risk_warning_thresholds['comprehensive_risk']
        print(f"\n1. ç¶œåˆé¢¨éšªé–¾å€¼:")
        print(f"   ğŸ”´ é—œéµé–¾å€¼: {comp_risk['critical_threshold']:.1f}åˆ†")
        print(f"   ğŸŸ  é«˜é¢¨éšªé–¾å€¼: {comp_risk['high_threshold']:.1f}åˆ†")
        print(f"   ğŸŸ¡ ä¸­é¢¨éšªé–¾å€¼: {comp_risk['medium_threshold']:.1f}åˆ†")
        print(f"   ğŸŸ¢ ä½é¢¨éšªé–¾å€¼: {comp_risk['low_threshold']:.1f}åˆ†")
        
        stats = comp_risk['statistical_benchmarks']
        print(f"   çµ±è¨ˆåŸºæº–: å¹³å‡{stats['mean']:.1f}åˆ†, ä¸­ä½æ•¸{stats['median']:.1f}åˆ†")
    
    # è§£ç´„é¢¨éšªé–¾å€¼
    if 'cancellation_risk' in risk_warning_thresholds:
        cancel_risk = risk_warning_thresholds['cancellation_risk']
        print(f"\n2. è§£ç´„é¢¨éšªé–¾å€¼:")
        rate_thresh = cancel_risk['rate_thresholds']
        print(f"   è§£ç´„ç‡é–¾å€¼: åš´é‡{rate_thresh['severe_threshold']:.1f}%, é«˜é¢¨éšª{rate_thresh['high_threshold']:.1f}%")
        
        score_thresh = cancel_risk['score_thresholds']
        print(f"   é¢¨éšªåˆ†æ•¸é–¾å€¼: ç«‹å³è¡Œå‹•{score_thresh['immediate_action']}, å¯†åˆ‡ç›£æ§{score_thresh['close_monitoring']}")
    
    # æ»¯éŠ·é¢¨éšªé–¾å€¼
    if 'stagnation_risk' in risk_warning_thresholds:
        stag_risk = risk_warning_thresholds['stagnation_risk']
        print(f"\n3. æ»¯éŠ·é¢¨éšªé–¾å€¼:")
        score_thresh = stag_risk['score_thresholds']
        print(f"   æ»¯éŠ·åˆ†æ•¸é–¾å€¼: é—œéµ{score_thresh['critical_stagnation']:.1f}, åš´é‡{score_thresh['severe_stagnation']:.1f}")
        
        time_thresh = stag_risk['time_thresholds']
        print(f"   æ™‚é–“é–¾å€¼: é•·æœŸ{time_thresh['long_term_threshold']}å­£, éé•·{time_thresh['excessive_threshold']}å­£")
    
    # ç›£æ§æ¡†æ¶
    if 'monitoring_framework' in risk_warning_thresholds:
        monitor = risk_warning_thresholds['monitoring_framework']
        print(f"\n4. ç›£æ§æ¡†æ¶:")
        freq = monitor['frequency']
        print(f"   ç›£æ§é »ç‡: é—œéµé …ç›®{freq['critical_projects']}, é«˜é¢¨éšª{freq['high_risk_projects']}")
        
        escalation = monitor['escalation_triggers']
        print(f"   ç«‹å³å‡ç´šè§¸ç™¼æ¢ä»¶: {len(escalation['immediate_escalation'])}é …")
        print(f"   ç·Šæ€¥å¯©æŸ¥è§¸ç™¼æ¢ä»¶: {len(escalation['urgent_review'])}é …")

# %%
# æ‡‰ç”¨é¢¨éšªé è­¦é–¾å€¼é€²è¡Œé …ç›®åˆ†é¡
print("ğŸ”„ æ‡‰ç”¨é¢¨éšªé è­¦é–¾å€¼é€²è¡Œé …ç›®åˆ†é¡...")

def apply_risk_warning_classification(comprehensive_risk_data, warning_thresholds):
    """
    æ‡‰ç”¨é¢¨éšªé è­¦é–¾å€¼å°é …ç›®é€²è¡Œåˆ†é¡
    """
    
    classification_results = []
    
    if 'comprehensive_risk' not in warning_thresholds:
        return pd.DataFrame()
    
    thresholds = warning_thresholds['comprehensive_risk']
    
    for _, project in comprehensive_risk_data.iterrows():
        if project['calculation_status'] != 'success':
            continue
        
        risk_score = project['comprehensive_risk_score']
        
        # é¢¨éšªç´šåˆ¥åˆ†é¡
        if risk_score >= thresholds['critical_threshold']:
            warning_level = 'critical'
            warning_emoji = 'ğŸš¨'
            monitoring_frequency = 'weekly'
            action_required = 'immediate_intervention'
        elif risk_score >= thresholds['high_threshold']:
            warning_level = 'high'
            warning_emoji = 'ğŸ”´'
            monitoring_frequency = 'bi_weekly'
            action_required = 'urgent_review'
        elif risk_score >= thresholds['medium_threshold']:
            warning_level = 'medium'
            warning_emoji = 'ğŸŸ¡'
            monitoring_frequency = 'monthly'
            action_required = 'routine_review'
        elif risk_score >= thresholds['low_threshold']:
            warning_level = 'low'
            warning_emoji = 'ğŸŸ '
            monitoring_frequency = 'monthly'
            action_required = 'preventive_monitoring'
        else:
            warning_level = 'minimal'
            warning_emoji = 'ğŸŸ¢'
            monitoring_frequency = 'quarterly'
            action_required = 'routine_monitoring'
        
        # ç‰¹æ®Šè­¦ç¤ºæª¢æŸ¥
        special_alerts = []
        
        # è§£ç´„é«˜é¢¨éšªæª¢æŸ¥
        if project['cancellation_risk'] > 20:
            special_alerts.append('è§£ç´„é«˜é¢¨éšª')
        
        # æ»¯éŠ·é«˜é¢¨éšªæª¢æŸ¥
        if project['stagnation_risk'] > 20:
            special_alerts.append('æ»¯éŠ·é«˜é¢¨éšª')
        
        # æ•ˆç‡æ¥µä½æª¢æŸ¥
        if project['efficiency_risk'] > 20:
            special_alerts.append('æ•ˆç‡æ¥µä½')
        
        # å¤šé‡é¢¨éšªæª¢æŸ¥
        high_risk_components = sum([
            project['stage_performance_risk'] > 15,
            project['cancellation_risk'] > 15,
            project['stagnation_risk'] > 15,
            project['efficiency_risk'] > 15
        ])
        
        if high_risk_components >= 3:
            special_alerts.append('å¤šé‡é«˜é¢¨éšª')
        
        classification_result = {
            'project_code': project['project_code'],
            'target_season': project['target_season'],
            'county': project.get('county', ''),
            'district': project.get('district', ''),
            'project_name': project.get('project_name', ''),
            'comprehensive_risk_score': risk_score,
            'warning_level': warning_level,
            'warning_emoji': warning_emoji,
            'monitoring_frequency': monitoring_frequency,
            'action_required': action_required,
            'special_alerts': special_alerts,
            'alert_count': len(special_alerts),
            'priority_score': risk_score + len(special_alerts) * 5,  # ç‰¹æ®Šè­¦ç¤ºåŠ åˆ†
            'is_critical_case': warning_level == 'critical' or len(special_alerts) >= 2
        }
        
        classification_results.append(classification_result)
    
    return pd.DataFrame(classification_results)

# åŸ·è¡Œé¢¨éšªé è­¦åˆ†é¡
warning_classification_df = apply_risk_warning_classification(
    comprehensive_risk_df[comprehensive_risk_df['calculation_status'] == 'success'],
    risk_warning_thresholds
)

print(f"âœ… å®Œæˆ {len(warning_classification_df)} ç­†é¢¨éšªé è­¦åˆ†é¡")

# %%
# é¢¨éšªé è­¦åˆ†é¡çµ±è¨ˆåˆ†æ
print(f"\nğŸ“Š é¢¨éšªé è­¦åˆ†é¡çµ±è¨ˆåˆ†æ:")

if not warning_classification_df.empty:
    # é è­¦ç´šåˆ¥åˆ†å¸ƒ
    warning_distribution = warning_classification_df['warning_level'].value_counts()
    print(f"\né è­¦ç´šåˆ¥åˆ†å¸ƒ:")
    
    level_order = ['critical', 'high', 'medium', 'low', 'minimal']
    for level in level_order:
        if level in warning_distribution.index:
            count = warning_distribution[level]
            percentage = count / len(warning_classification_df) * 100
            sample_record = warning_classification_df[warning_classification_df['warning_level'] == level].iloc[0]
            emoji = sample_record['warning_emoji']
            print(f"   {emoji} {level}: {count:,} å€‹ ({percentage:.1f}%)")
    
    # éœ€è¦ç«‹å³é—œæ³¨çš„é …ç›®
    critical_cases = warning_classification_df[warning_classification_df['is_critical_case'] == True]
    print(f"\néœ€è¦ç«‹å³é—œæ³¨çš„é …ç›®:")
    print(f"   é—œéµæ¡ˆä¾‹æ•¸: {len(critical_cases):,} å€‹ ({len(critical_cases)/len(warning_classification_df)*100:.1f}%)")
    
    if not critical_cases.empty:
        print(f"   æœ€é«˜å„ªå…ˆç´šé …ç›® (å‰5å€‹):")
        top_priority = critical_cases.nlargest(5, 'priority_score')
        for i, (_, project) in enumerate(top_priority.iterrows(), 1):
            alerts = ', '.join(project['special_alerts']) if project['special_alerts'] else 'ç„¡ç‰¹æ®Šè­¦ç¤º'
            print(f"     {i}. {project['project_code']} | {project['county']} | "
                  f"é¢¨éšª{project['comprehensive_risk_score']:.1f}åˆ† | {alerts}")
    
    # ç‰¹æ®Šè­¦ç¤ºçµ±è¨ˆ
    all_alerts = []
    for alerts_list in warning_classification_df['special_alerts']:
        if isinstance(alerts_list, list):
            all_alerts.extend(alerts_list)
    
    if all_alerts:
        alert_counts = Counter(all_alerts)
        print(f"\nç‰¹æ®Šè­¦ç¤ºçµ±è¨ˆ:")
        for alert, count in alert_counts.most_common():
            percentage = count / len(warning_classification_df) * 100
            print(f"   {alert}: {count} å€‹ ({percentage:.1f}%)")
    
    # ç›£æ§é »ç‡éœ€æ±‚çµ±è¨ˆ
    monitoring_distribution = warning_classification_df['monitoring_frequency'].value_counts()
    print(f"\nç›£æ§é »ç‡éœ€æ±‚:")
    for frequency, count in monitoring_distribution.items():
        percentage = count / len(warning_classification_df) * 100
        print(f"   {frequency}: {count:,} å€‹ ({percentage:.1f}%)")
    
    # ç¸£å¸‚é¢¨éšªè­¦ç¤ºåˆ†å¸ƒ
    if 'county' in warning_classification_df.columns:
        print(f"\nç¸£å¸‚é¢¨éšªè­¦ç¤ºåˆ†å¸ƒ:")
        city_warning = warning_classification_df.groupby('county').agg({
            'warning_level': lambda x: (x.isin(['critical', 'high'])).sum(),
            'is_critical_case': 'sum',
            'comprehensive_risk_score': 'mean'
        }).round(1)
        city_warning.columns = ['é«˜è­¦ç¤ºæ•¸', 'é—œéµæ¡ˆä¾‹æ•¸', 'å¹³å‡é¢¨éšªåˆ†æ•¸']
        
        # åªé¡¯ç¤ºå»ºæ¡ˆæ•¸â‰¥3çš„ç¸£å¸‚
        city_counts = warning_classification_df['county'].value_counts()
        city_warning_filtered = city_warning[city_counts >= 3].sort_values('å¹³å‡é¢¨éšªåˆ†æ•¸', ascending=False)
        
        for county in city_warning_filtered.head(8).index:
            high_warning = city_warning_filtered.loc[county, 'é«˜è­¦ç¤ºæ•¸']
            critical_cases = city_warning_filtered.loc[county, 'é—œéµæ¡ˆä¾‹æ•¸']
            avg_risk = city_warning_filtered.loc[county, 'å¹³å‡é¢¨éšªåˆ†æ•¸']
            total_count = city_counts[county]
            print(f"   {county}: é«˜è­¦ç¤º{int(high_warning)}/{total_count}å€‹, "
                  f"é—œéµ{int(critical_cases)}å€‹, å¹³å‡{avg_risk:.1f}åˆ†")

# %% [markdown]
# ## 8. å¤šç¶­åº¦é¢¨éšªæ•´åˆåˆ†æ

# %%
# å¤šç¶­åº¦é¢¨éšªæ•´åˆåˆ†æ
print("ğŸ”— å¤šç¶­åº¦é¢¨éšªæ•´åˆåˆ†æ")
print("=" * 60)

def perform_multidimensional_risk_integration(stage_df, cancellation_df, stagnation_df, 
                                            efficiency_df, comprehensive_df, warning_df):
    """
    åŸ·è¡Œå¤šç¶­åº¦é¢¨éšªæ•´åˆåˆ†æ
    
    æ•´åˆæ‰€æœ‰é¢¨éšªç¶­åº¦ï¼Œæä¾›å®Œæ•´çš„é¢¨éšªç•«åƒ
    """
    
    integration_results = {
        'risk_correlation_analysis': {},
        'risk_pattern_identification': {},
        'risk_cluster_analysis': {},
        'predictive_risk_indicators': {},
        'intervention_recommendations': {}
    }
    
    try:
        # 1. é¢¨éšªç›¸é—œæ€§åˆ†æ
        risk_data = comprehensive_df[comprehensive_df['calculation_status'] == 'success'].copy()
        
        if not risk_data.empty:
            correlation_vars = [
                'stage_performance_risk', 'cancellation_risk', 
                'stagnation_risk', 'efficiency_risk', 'comprehensive_risk_score'
            ]
            
            correlation_matrix = risk_data[correlation_vars].corr()
            integration_results['risk_correlation_analysis'] = {
                'correlation_matrix': correlation_matrix.to_dict(),
                'strongest_correlations': [
                    f"æ»¯éŠ·é¢¨éšª vs æ•ˆç‡é¢¨éšª: {correlation_matrix.loc['stagnation_risk', 'efficiency_risk']:.3f}",
                    f"éšæ®µè¡¨ç¾ vs ç¶œåˆé¢¨éšª: {correlation_matrix.loc['stage_performance_risk', 'comprehensive_risk_score']:.3f}",
                    f"è§£ç´„é¢¨éšª vs ç¶œåˆé¢¨éšª: {correlation_matrix.loc['cancellation_risk', 'comprehensive_risk_score']:.3f}"
                ]
            }
        
        # 2. é¢¨éšªæ¨¡å¼è­˜åˆ¥
        risk_patterns = {}
        
        # é«˜é¢¨éšªèšé›†æ¨¡å¼
        high_risk_projects = comprehensive_df[comprehensive_df['comprehensive_risk_score'] >= 60]
        if not high_risk_projects.empty:
            # é¢¨éšªçµ„æˆåˆ†æ
            avg_components = {
                'stage_risk': high_risk_projects['stage_performance_risk'].mean(),
                'cancellation_risk': high_risk_projects['cancellation_risk'].mean(),
                'stagnation_risk': high_risk_projects['stagnation_risk'].mean(),
                'efficiency_risk': high_risk_projects['efficiency_risk'].mean()
            }
            
            dominant_risk = max(avg_components, key=avg_components.get)
            risk_patterns['high_risk_pattern'] = {
                'dominant_risk_factor': dominant_risk,
                'average_components': avg_components,
                'pattern_description': f"é«˜é¢¨éšªé …ç›®ä¸»è¦ç”±{dominant_risk}é©…å‹•"
            }
        
        # å¤šé‡é¢¨éšªæ¨¡å¼
        multi_risk_projects = comprehensive_df[
            (comprehensive_df['stage_performance_risk'] > 15) &
            (comprehensive_df['cancellation_risk'] > 10) &
            (comprehensive_df['stagnation_risk'] > 15)
        ]
        
        if not multi_risk_projects.empty:
            risk_patterns['multi_risk_pattern'] = {
                'count': len(multi_risk_projects),
                'percentage': len(multi_risk_projects) / len(comprehensive_df) * 100,
                'avg_comprehensive_score': multi_risk_projects['comprehensive_risk_score'].mean(),
                'pattern_description': "åŒæ™‚é¢è‡¨éšæ®µè¡¨ç¾ã€è§£ç´„ã€æ»¯éŠ·å¤šé‡é¢¨éšª"
            }
        
        integration_results['risk_pattern_identification'] = risk_patterns
        
        # 3. é¢¨éšªèšé¡åˆ†æï¼ˆç°¡åŒ–ç‰ˆï¼‰
        if not risk_data.empty and len(risk_data) >= 10:
            # åŸºæ–¼é¢¨éšªåˆ†æ•¸é€²è¡Œç°¡å–®èšé¡
            risk_clusters = {}
            
            # ä½é¢¨éšªèšé¡
            low_risk = risk_data[risk_data['comprehensive_risk_score'] < 30]
            # ä¸­é¢¨éšªèšé¡
            medium_risk = risk_data[
                (risk_data['comprehensive_risk_score'] >= 30) & 
                (risk_data['comprehensive_risk_score'] < 60)
            ]
            # é«˜é¢¨éšªèšé¡
            high_risk = risk_data[risk_data['comprehensive_risk_score'] >= 60]
            
            for cluster_name, cluster_data in [
                ('ä½é¢¨éšªç¾¤', low_risk), ('ä¸­é¢¨éšªç¾¤', medium_risk), ('é«˜é¢¨éšªç¾¤', high_risk)
            ]:
                if not cluster_data.empty:
                    risk_clusters[cluster_name] = {
                        'size': len(cluster_data),
                        'percentage': len(cluster_data) / len(risk_data) * 100,
                        'avg_stage_risk': cluster_data['stage_performance_risk'].mean(),
                        'avg_cancellation_risk': cluster_data['cancellation_risk'].mean(),
                        'avg_stagnation_risk': cluster_data['stagnation_risk'].mean(),
                        'avg_efficiency_risk': cluster_data['efficiency_risk'].mean(),
                        'characteristics': f"å¹³å‡é¢¨éšªåˆ†æ•¸: {cluster_data['comprehensive_risk_score'].mean():.1f}"
                    }
            
            integration_results['risk_cluster_analysis'] = risk_clusters
        
        # 4. é æ¸¬æ€§é¢¨éšªæŒ‡æ¨™
        predictive_indicators = {}
        
        # éšæ®µé€²å±•é¢¨éšªé æ¸¬
        stage_risk_trends = {}
        for stage in ['é–‹ç›¤åˆæœŸ', 'ç©©å®šéŠ·å”®æœŸ', 'ä¸­å¾ŒæœŸèª¿æ•´']:
            stage_data = stage_df[stage_df['sales_stage'] == stage]
            if not stage_data.empty:
                poor_performance_ratio = len(stage_data[stage_data['stage_performance'] == 'ä¸ä½³']) / len(stage_data) * 100
                stage_risk_trends[stage] = {
                    'poor_performance_rate': poor_performance_ratio,
                    'risk_prediction': 'high' if poor_performance_ratio > 30 else 'medium' if poor_performance_ratio > 15 else 'low'
                }
        
        predictive_indicators['stage_progression_risk'] = stage_risk_trends
        
        # é•·æœŸæ»¯éŠ·é æ¸¬æŒ‡æ¨™
        potential_stagnation = stagnation_df[
            (stagnation_df['sales_seasons'] > 8) &
            (stagnation_df['current_absorption_rate'] < 60) &
            (stagnation_df['avg_quarterly_speed'] < 1)
        ]
        
        predictive_indicators['future_stagnation_risk'] = {
            'potential_cases': len(potential_stagnation),
            'percentage_of_total': len(potential_stagnation) / len(stagnation_df) * 100 if not stagnation_df.empty else 0,
            'prediction': 'æœªä¾†6å€‹æœˆå…§å¯èƒ½å‡ºç¾æ›´å¤šé•·æœŸæ»¯éŠ·æ¡ˆä¾‹' if len(potential_stagnation) > 5 else 'æ»¯éŠ·é¢¨éšªæ§åˆ¶è‰¯å¥½'
        }
        
        integration_results['predictive_risk_indicators'] = predictive_indicators
        
        # 5. ä»‹å…¥å»ºè­°
        intervention_recommendations = {}
        
        # æŒ‰é¢¨éšªç´šåˆ¥çš„ä»‹å…¥å»ºè­°
        if not warning_df.empty:
            critical_projects = warning_df[warning_df['warning_level'] == 'critical']
            high_risk_projects = warning_df[warning_df['warning_level'] == 'high']
            
            intervention_recommendations['immediate_actions'] = {
                'critical_count': len(critical_projects),
                'recommendations': [
                    'ç«‹å³å¬é–‹ç·Šæ€¥æœƒè­°è©•ä¼°é …ç›®ç‹€æ³',
                    'æª¢è¨éŠ·å”®ç­–ç•¥å’Œå®šåƒ¹æ”¿ç­–',
                    'è€ƒæ…®ä¿ƒéŠ·æ–¹æ¡ˆæˆ–ç”¢å“èª¿æ•´',
                    'åŠ å¼·å®¢æˆ¶æœå‹™å’Œå”®å¾Œæ”¯æŒ',
                    'æ¯é€±ç›£æ§é€²åº¦ä¸¦èª¿æ•´ç­–ç•¥'
                ]
            }
            
            intervention_recommendations['preventive_measures'] = {
                'high_risk_count': len(high_risk_projects),
                'recommendations': [
                    'å®šæœŸæª¢è¦–éŠ·å”®é€²åº¦å’Œå¸‚å ´åæ‡‰',
                    'æå‰æº–å‚™æ‡‰è®Šæ–¹æ¡ˆ',
                    'åŠ å¼·éŠ·å”®åœ˜éšŠåŸ¹è¨“',
                    'å„ªåŒ–å®¢æˆ¶é«”é©—æµç¨‹',
                    'å»ºç«‹æ—©æœŸé è­¦æ©Ÿåˆ¶'
                ]
            }
        
        # ç³»çµ±æ€§æ”¹å–„å»ºè­°
        intervention_recommendations['systemic_improvements'] = [
            'å»ºç«‹å®Œæ•´çš„é¢¨éšªç›£æ§Dashboard',
            'å®šæœŸé€²è¡Œé¢¨éšªè©•ä¼°å’Œé æ¸¬',
            'å»ºç«‹è·¨éƒ¨é–€é¢¨éšªæ‡‰å°æ©Ÿåˆ¶',
            'å„ªåŒ–è³‡æ–™æ”¶é›†å’Œåˆ†ææµç¨‹',
            'åŸ¹è¨“ç›¸é—œäººå“¡é¢¨éšªè­˜åˆ¥èƒ½åŠ›'
        ]
        
        integration_results['intervention_recommendations'] = intervention_recommendations
    
    except Exception as e:
        print(f"âŒ å¤šç¶­åº¦é¢¨éšªæ•´åˆéŒ¯èª¤: {e}")
    
    return integration_results

# %%
# åŸ·è¡Œå¤šç¶­åº¦é¢¨éšªæ•´åˆåˆ†æ
print("ğŸ”„ åŸ·è¡Œå¤šç¶­åº¦é¢¨éšªæ•´åˆåˆ†æ...")

multidimensional_analysis = perform_multidimensional_risk_integration(
    sales_stage_df,
    cancellation_risk_df, 
    stagnation_risk_df,
    absorption_efficiency,
    comprehensive_risk_df,
    warning_classification_df
)

print(f"âœ… å®Œæˆå¤šç¶­åº¦é¢¨éšªæ•´åˆåˆ†æ")

# %%
# å¤šç¶­åº¦é¢¨éšªæ•´åˆåˆ†æå ±å‘Š
print(f"\nğŸ“Š å¤šç¶­åº¦é¢¨éšªæ•´åˆåˆ†æå ±å‘Š:")

if multidimensional_analysis:
    # é¢¨éšªç›¸é—œæ€§åˆ†æ
    if 'risk_correlation_analysis' in multidimensional_analysis:
        corr_analysis = multidimensional_analysis['risk_correlation_analysis']
        print(f"\n1. é¢¨éšªç›¸é—œæ€§åˆ†æ:")
        if 'strongest_correlations' in corr_analysis:
            for correlation in corr_analysis['strongest_correlations']:
                print(f"   {correlation}")
    
    # é¢¨éšªæ¨¡å¼è­˜åˆ¥
    if 'risk_pattern_identification' in multidimensional_analysis:
        patterns = multidimensional_analysis['risk_pattern_identification']
        print(f"\n2. é¢¨éšªæ¨¡å¼è­˜åˆ¥:")
        
        if 'high_risk_pattern' in patterns:
            high_pattern = patterns['high_risk_pattern']
            print(f"   é«˜é¢¨éšªæ¨¡å¼: {high_pattern['pattern_description']}")
            components = high_pattern['average_components']
            print(f"   ä¸»è¦é¢¨éšªçµ„æˆ: {max(components, key=components.get)} ({max(components.values()):.1f}åˆ†)")
        
        if 'multi_risk_pattern' in patterns:
            multi_pattern = patterns['multi_risk_pattern']
            print(f"   å¤šé‡é¢¨éšªæ¨¡å¼: {multi_pattern['count']}å€‹é …ç›® ({multi_pattern['percentage']:.1f}%)")
    
    # é¢¨éšªèšé¡åˆ†æ
    if 'risk_cluster_analysis' in multidimensional_analysis:
        clusters = multidimensional_analysis['risk_cluster_analysis']
        print(f"\n3. é¢¨éšªèšé¡åˆ†æ:")
        
        for cluster_name, cluster_info in clusters.items():
            print(f"   {cluster_name}: {cluster_info['size']}å€‹é …ç›® ({cluster_info['percentage']:.1f}%)")
            print(f"     {cluster_info['characteristics']}")
    
    # é æ¸¬æ€§é¢¨éšªæŒ‡æ¨™
    if 'predictive_risk_indicators' in multidimensional_analysis:
        predictive = multidimensional_analysis['predictive_risk_indicators']
        print(f"\n4. é æ¸¬æ€§é¢¨éšªæŒ‡æ¨™:")
        
        if 'stage_progression_risk' in predictive:
            stage_risks = predictive['stage_progression_risk']
            for stage, risk_info in stage_risks.items():
                print(f"   {stage}: ä¸ä½³è¡¨ç¾ç‡{risk_info['poor_performance_rate']:.1f}%, "
                      f"é¢¨éšªé æ¸¬-{risk_info['risk_prediction']}")
        
        if 'future_stagnation_risk' in predictive:
            future_risk = predictive['future_stagnation_risk']
            print(f"   æœªä¾†æ»¯éŠ·é¢¨éšª: {future_risk['potential_cases']}å€‹æ½›åœ¨æ¡ˆä¾‹ "
                  f"({future_risk['percentage_of_total']:.1f}%)")
            print(f"   é æ¸¬çµè«–: {future_risk['prediction']}")
    
    # ä»‹å…¥å»ºè­°
    if 'intervention_recommendations' in multidimensional_analysis:
        interventions = multidimensional_analysis['intervention_recommendations']
        print(f"\n5. ä»‹å…¥å»ºè­°:")
        
        if 'immediate_actions' in interventions:
            immediate = interventions['immediate_actions']
            print(f"   ç«‹å³è¡Œå‹•é …ç›®: {immediate['critical_count']}å€‹")
            print(f"   é—œéµå»ºè­°: {immediate['recommendations'][0]}")
        
        if 'preventive_measures' in interventions:
            preventive = interventions['preventive_measures']
            print(f"   é é˜²æªæ–½é …ç›®: {preventive['high_risk_count']}å€‹")
        
        if 'systemic_improvements' in interventions:
            systemic = interventions['systemic_improvements']
            print(f"   ç³»çµ±æ€§æ”¹å–„å»ºè­°æ•¸: {len(systemic)}é …")

# %% [markdown]
# ## 9. ç¤¾å€ç´šå®Œæ•´å ±å‘Šæº–å‚™

# %%
# ç¤¾å€ç´šå®Œæ•´å ±å‘Šæº–å‚™
print("ğŸ“‹ ç¤¾å€ç´šå®Œæ•´å ±å‘Šæº–å‚™")
print("=" * 60)

def prepare_community_level_comprehensive_report(absorption_data, stage_data, performance_data,
                                               cancellation_data, stagnation_data, efficiency_data,
                                               comprehensive_data, warning_data, speed_data):
    """
    æº–å‚™ç¤¾å€ç´šå®Œæ•´å ±å‘Š (32æ¬„ä½)
    
    æ•´åˆæ‰€æœ‰åˆ†æçµæœï¼Œç”¢ç”Ÿå®Œæ•´çš„ç¤¾å€ç´šå ±å‘Š
    """
    
    comprehensive_reports = []
    
    # ç²å–æ‰€æœ‰å”¯ä¸€çš„å»ºæ¡ˆ-å¹´å­£çµ„åˆ
    unique_records = absorption_data[absorption_data['calculation_status'] == 'success'][
        ['project_code', 'target_season']
    ].drop_duplicates()
    
    for _, record in unique_records.iterrows():
        project_code = record['project_code']
        target_season = record['target_season']
        
        report = {
            # A. åŸºæœ¬è³‡è¨Š (7æ¬„)
            'project_code': project_code,
            'target_season': target_season,
            'project_name': '',
            'county': '',
            'district': '',
            'street_address': '',
            'total_units': 0,
            'sales_start_season': '',
            
            # B. æ™‚é–“èˆ‡æ•¸é‡ (5æ¬„)
            'sales_seasons': 0,
            'cumulative_transactions': 0,
            'quarterly_transactions': 0,
            'quarterly_sales_days': 0,
            'is_complete_quarter': 'N',
            
            # C. è§£ç´„è³‡è¨Š (6æ¬„)
            'cumulative_cancellations': 0,
            'quarterly_cancellations': 0,
            'quarterly_cancellation_rate': 0.0,
            'cumulative_cancellation_rate': 0.0,
            'latest_cancellation_season': '',
            'consecutive_no_cancellation_seasons': 0,
            
            # D. å»åŒ–åˆ†æ (3æ¬„)
            'gross_absorption_rate': 0.0,
            'net_absorption_rate': 0.0,
            'adjusted_absorption_rate': 0.0,
            
            # E. å»åŒ–å‹•æ…‹åˆ†æ (4æ¬„)
            'quarterly_absorption_speed': 0.0,
            'absorption_acceleration': 0.0,
            'estimated_completion_seasons': 0,
            'absorption_efficiency_grade': '',
            
            # F. åƒ¹æ ¼åˆ†æ (3æ¬„)
            'avg_unit_price': 0.0,
            'avg_total_area': 0.0,
            'avg_total_price': 0.0,
            
            # G. éšæ®µåˆ†æ (3æ¬„)
            'sales_stage': '',
            'stage_performance': '',
            'cancellation_risk_level': '',
            
            # H. å“è³ªæ§åˆ¶ (1æ¬„)
            'data_quality_score': 0.0,
            
            # æ“´å±•æ¬„ä½
            'comprehensive_risk_score': 0.0,
            'risk_warning_level': '',
            'major_risk_factors': '',
            'intervention_priority': ''
        }
        
        try:
            # åŸºæœ¬è³‡è¨Šå¡«å……
            absorption_row = absorption_data[
                (absorption_data['project_code'] == project_code) & 
                (absorption_data['target_season'] == target_season) &
                (absorption_data['calculation_status'] == 'success')
            ]
            
            if not absorption_row.empty:
                abs_info = absorption_row.iloc[0]
                report.update({
                    'project_name': abs_info.get('project_name', ''),
                    'county': abs_info.get('county', ''),
                    'district': abs_info.get('district', ''),
                    'total_units': abs_info.get('total_units', 0),
                    'sales_start_season': abs_info.get('start_season', ''),
                    'gross_absorption_rate': abs_info.get('gross_absorption_rate', 0),
                    'net_absorption_rate': abs_info.get('net_absorption_rate', 0),
                    'adjusted_absorption_rate': abs_info.get('adjusted_absorption_rate', 0)
                })
            
            # éšæ®µè³‡è¨Š
            stage_row = stage_data[
                (stage_data['project_code'] == project_code) & 
                (stage_data['target_season'] == target_season) &
                (stage_data['calculation_status'] == 'success')
            ]
            
            if not stage_row.empty:
                stage_info = stage_row.iloc[0]
                report.update({
                    'sales_stage': stage_info.get('sales_stage', ''),
                    'sales_seasons': stage_info.get('sales_seasons', 0),
                    'quarterly_absorption_speed': stage_info.get('quarterly_speed', 0)
                })
            
            # éšæ®µè¡¨ç¾
            performance_row = performance_data[
                (performance_data['project_code'] == project_code) & 
                (performance_data['target_season'] == target_season)
            ]
            
            if not performance_row.empty:
                perf_info = performance_row.iloc[0]
                report.update({
                    'stage_performance': f"{perf_info.get('performance_emoji', '')} {perf_info.get('stage_performance', '')}"
                })
            
            # è§£ç´„é¢¨éšª
            cancellation_row = cancellation_data[
                (cancellation_data['project_code'] == project_code) & 
                (cancellation_data['target_season'] == target_season) &
                (cancellation_data['calculation_status'] == 'success')
            ]
            
            if not cancellation_row.empty:
                cancel_info = cancellation_row.iloc[0]
                report.update({
                    'cumulative_cancellations': cancel_info.get('cumulative_cancellation_count', 0),
                    'quarterly_cancellations': cancel_info.get('quarterly_cancellation_count', 0),
                    'quarterly_cancellation_rate': cancel_info.get('quarterly_cancellation_rate', 0),
                    'cumulative_cancellation_rate': cancel_info.get('cumulative_cancellation_rate', 0),
                    'latest_cancellation_season': cancel_info.get('latest_cancellation_season', ''),
                    'consecutive_no_cancellation_seasons': cancel_info.get('consecutive_no_cancellation_seasons', 0),
                    'cancellation_risk_level': f"{cancel_info.get('risk_emoji', '')} {cancel_info.get('cancellation_risk_level', '')}"
                })
            
            # å»åŒ–é€Ÿåº¦
            speed_row = speed_data[
                (speed_data['project_code'] == project_code) & 
                (speed_data['target_season'] == target_season) &
                (speed_data['calculation_status'] == 'success')
            ]
            
            if not speed_row.empty:
                speed_info = speed_row.iloc[0]
                report['quarterly_absorption_speed'] = speed_info.get('quarterly_absorption_speed', 0)
            
            # æ•ˆç‡è©•ç´š
            efficiency_row = efficiency_data[
                (efficiency_data['project_code'] == project_code) & 
                (efficiency_data['target_season'] == target_season) &
                (efficiency_data['calculation_status'] == 'success')
            ]
            
            if not efficiency_row.empty:
                eff_info = efficiency_row.iloc[0]
                report.update({
                    'absorption_efficiency_grade': f"{eff_info.get('grade_emoji', '')} {eff_info.get('grade_description', '')}"
                })
            
            # ç¶œåˆé¢¨éšª
            comprehensive_row = comprehensive_data[
                (comprehensive_data['project_code'] == project_code) & 
                (comprehensive_data['target_season'] == target_season) &
                (comprehensive_data['calculation_status'] == 'success')
            ]
            
            if not comprehensive_row.empty:
                comp_info = comprehensive_row.iloc[0]
                major_factors = comp_info.get('major_risk_factors', [])
                if isinstance(major_factors, list):
                    factors_str = ', '.join(major_factors)
                else:
                    factors_str = str(major_factors)
                
                report.update({
                    'comprehensive_risk_score': comp_info.get('comprehensive_risk_score', 0),
                    'major_risk_factors': factors_str
                })
            
            # é è­¦åˆ†ç´š
            warning_row = warning_data[
                (warning_data['project_code'] == project_code) & 
                (warning_data['target_season'] == target_season)
            ]
            
            if not warning_row.empty:
                warn_info = warning_row.iloc[0]
                report.update({
                    'risk_warning_level': f"{warn_info.get('warning_emoji', '')} {warn_info.get('warning_level', '')}",
                    'intervention_priority': warn_info.get('action_required', '')
                })
            
            # è³‡æ–™å“è³ªè©•åˆ† (ç°¡åŒ–)
            quality_score = 0
            if report['project_name']:
                quality_score += 25
            if report['total_units'] > 0:
                quality_score += 25
            if report['net_absorption_rate'] > 0:
                quality_score += 25
            if report['sales_stage']:
                quality_score += 25
            
            report['data_quality_score'] = quality_score
            
        except Exception as e:
            print(f"âŒ å ±å‘Šæº–å‚™éŒ¯èª¤ {project_code}: {e}")
            continue
        
        comprehensive_reports.append(report)
    
    return pd.DataFrame(comprehensive_reports)

# %%
# ç”Ÿæˆç¤¾å€ç´šå®Œæ•´å ±å‘Š
print("ğŸ”„ ç”Ÿæˆç¤¾å€ç´šå®Œæ•´å ±å‘Š...")

community_comprehensive_report = prepare_community_level_comprehensive_report(
    absorption_analysis,
    sales_stage_df,
    stage_performance_df,
    cancellation_risk_df,
    stagnation_risk_df,
    absorption_efficiency,
    comprehensive_risk_df,
    warning_classification_df,
    quarterly_speed
)

print(f"âœ… å®Œæˆç¤¾å€ç´šå®Œæ•´å ±å‘Šç”Ÿæˆ")
print(f"   å ±å‘Šè¨˜éŒ„æ•¸: {len(community_comprehensive_report):,}")
print(f"   å ±å‘Šæ¬„ä½æ•¸: {len(community_comprehensive_report.columns)}")

# %%
# ç¤¾å€ç´šå®Œæ•´å ±å‘Šå“è³ªæª¢æŸ¥
print(f"\nğŸ“Š ç¤¾å€ç´šå®Œæ•´å ±å‘Šå“è³ªæª¢æŸ¥:")

if not community_comprehensive_report.empty:
    # è³‡æ–™å®Œæ•´æ€§æª¢æŸ¥
    print(f"è³‡æ–™å®Œæ•´æ€§çµ±è¨ˆ:")
    print(f"   ç¸½è¨˜éŒ„æ•¸: {len(community_comprehensive_report):,}")
    print(f"   æœ‰å»ºæ¡ˆåç¨±: {len(community_comprehensive_report[community_comprehensive_report['project_name'] != '']):,}")
    print(f"   æœ‰ç¸£å¸‚è³‡è¨Š: {len(community_comprehensive_report[community_comprehensive_report['county'] != '']):,}")
    print(f"   æœ‰ç¸½æˆ¶æ•¸: {len(community_comprehensive_report[community_comprehensive_report['total_units'] > 0]):,}")
    print(f"   æœ‰å»åŒ–ç‡: {len(community_comprehensive_report[community_comprehensive_report['net_absorption_rate'] > 0]):,}")
    
    # é—œéµæŒ‡æ¨™çµ±è¨ˆ
    print(f"\né—œéµæŒ‡æ¨™çµ±è¨ˆ:")
    print(f"   å¹³å‡å»åŒ–ç‡: {community_comprehensive_report['net_absorption_rate'].mean():.1f}%")
    print(f"   å¹³å‡é¢¨éšªåˆ†æ•¸: {community_comprehensive_report['comprehensive_risk_score'].mean():.1f}")
    print(f"   å¹³å‡è³‡æ–™å“è³ª: {community_comprehensive_report['data_quality_score'].mean():.1f}")
    
    # éšæ®µåˆ†å¸ƒ
    stage_dist = community_comprehensive_report['sales_stage'].value_counts()
    print(f"\néŠ·å”®éšæ®µåˆ†å¸ƒ:")
    for stage, count in stage_dist.head(5).items():
        if stage:  # éç©ºå€¼
            percentage = count / len(community_comprehensive_report) * 100
            print(f"   {stage}: {count:,} å€‹ ({percentage:.1f}%)")
    
    # é¢¨éšªç´šåˆ¥åˆ†å¸ƒ
    risk_levels = community_comprehensive_report['risk_warning_level'].value_counts()
    print(f"\né¢¨éšªç´šåˆ¥åˆ†å¸ƒ:")
    for level, count in risk_levels.head(5).items():
        if level:  # éç©ºå€¼
            percentage = count / len(community_comprehensive_report) * 100
            print(f"   {level}: {count:,} å€‹ ({percentage:.1f}%)")
    
    # ç¸£å¸‚åˆ†å¸ƒ
    city_dist = community_comprehensive_report['county'].value_counts()
    print(f"\nç¸£å¸‚åˆ†å¸ƒ (å‰8å):")
    for county, count in city_dist.head(8).items():
        if county:  # éç©ºå€¼
            percentage = count / len(community_comprehensive_report) * 100
            avg_risk = community_comprehensive_report[community_comprehensive_report['county'] == county]['comprehensive_risk_score'].mean()
            print(f"   {county}: {count:,} å€‹ ({percentage:.1f}%), å¹³å‡é¢¨éšª{avg_risk:.1f}åˆ†")

# %% [markdown]
# ## 10. è¦–è¦ºåŒ–åˆ†æ

# %%
# å‰µå»ºéŠ·å”®éšæ®µèˆ‡é¢¨éšªè©•ä¼°è¦–è¦ºåŒ–
print("ğŸ“Š éŠ·å”®éšæ®µèˆ‡é¢¨éšªè©•ä¼°è¦–è¦ºåŒ–")
print("=" * 50)

# å‰µå»ºåœ–è¡¨
fig, axes = plt.subplots(3, 3, figsize=(20, 15))

# éæ¿¾æœ‰æ•ˆæ•¸æ“š
valid_comprehensive = comprehensive_risk_df[comprehensive_risk_df['calculation_status'] == 'success']
valid_stage = stage_performance_df[stage_performance_df.get('sales_stage', '') != '']
valid_warning = warning_classification_df

# 1. éŠ·å”®éšæ®µåˆ†å¸ƒ
if not valid_stage.empty:
    stage_counts = valid_stage['sales_stage'].value_counts()
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7'][:len(stage_counts)]
    
    wedges, texts, autotexts = axes[0, 0].pie(stage_counts.values, labels=stage_counts.index, 
                                              autopct='%1.1f%%', colors=colors, startangle=90)
    axes[0, 0].set_title('éŠ·å”®éšæ®µåˆ†å¸ƒ', fontsize=14, fontweight='bold')
    
    # èª¿æ•´æ–‡å­—å¤§å°
    for autotext in autotexts:
        autotext.set_fontsize(10)

# 2. ç¶œåˆé¢¨éšªåˆ†æ•¸åˆ†å¸ƒ
if not valid_comprehensive.empty:
    risk_scores = valid_comprehensive['comprehensive_risk_score']
    axes[0, 1].hist(risk_scores, bins=20, alpha=0.7, color='lightcoral', edgecolor='black')
    axes[0, 1].set_title('ç¶œåˆé¢¨éšªåˆ†æ•¸åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    axes[0, 1].set_xlabel('é¢¨éšªåˆ†æ•¸')
    axes[0, 1].set_ylabel('å»ºæ¡ˆæ•¸é‡')
    axes[0, 1].axvline(x=risk_scores.mean(), color='red', linestyle='--', 
                      label=f'å¹³å‡: {risk_scores.mean():.1f}')
    axes[0, 1].axvline(x=60, color='orange', linestyle='--', label='é«˜é¢¨éšªç·š: 60')
    axes[0, 1].legend()

# 3. é¢¨éšªé è­¦ç´šåˆ¥åˆ†å¸ƒ
if not valid_warning.empty:
    warning_counts = valid_warning['warning_level'].value_counts()
    warning_colors = {'critical': 'red', 'high': 'orange', 'medium': 'yellow', 
                     'low': 'lightgreen', 'minimal': 'green'}
    bar_colors = [warning_colors.get(level, 'gray') for level in warning_counts.index]
    
    bars = axes[0, 2].bar(range(len(warning_counts)), warning_counts.values, color=bar_colors)
    axes[0, 2].set_title('é¢¨éšªé è­¦ç´šåˆ¥åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    axes[0, 2].set_xlabel('é è­¦ç´šåˆ¥')
    axes[0, 2].set_ylabel('å»ºæ¡ˆæ•¸é‡')
    axes[0, 2].set_xticks(range(len(warning_counts)))
    axes[0, 2].set_xticklabels(warning_counts.index, rotation=45)
    
    # æ·»åŠ æ•¸å€¼æ¨™ç±¤
    for bar in bars:
        height = bar.get_height()
        axes[0, 2].text(bar.get_x() + bar.get_width()/2., height,
                       f'{int(height)}', ha='center', va='bottom')

# 4. å„éšæ®µå¹³å‡é¢¨éšªåˆ†æ•¸
if not valid_stage.empty and not valid_comprehensive.empty:
    stage_risk_data = valid_stage.merge(
        valid_comprehensive[['project_code', 'target_season', 'comprehensive_risk_score']],
        on=['project_code', 'target_season'], how='left'
    )
    
    if not stage_risk_data.empty and 'comprehensive_risk_score' in stage_risk_data.columns:
        stage_risk_avg = stage_risk_data.groupby('sales_stage')['comprehensive_risk_score'].mean().sort_values()
        
        bars = axes[1, 0].bar(range(len(stage_risk_avg)), stage_risk_avg.values, 
                             color='lightblue', alpha=0.8)
        axes[1, 0].set_title('å„éšæ®µå¹³å‡é¢¨éšªåˆ†æ•¸', fontsize=14, fontweight='bold')
        axes[1, 0].set_xlabel('éŠ·å”®éšæ®µ')
        axes[1, 0].set_ylabel('å¹³å‡é¢¨éšªåˆ†æ•¸')
        axes[1, 0].set_xticks(range(len(stage_risk_avg)))
        axes[1, 0].set_xticklabels(stage_risk_avg.index, rotation=45, ha='right')
        
        # æ·»åŠ æ•¸å€¼æ¨™ç±¤
        for bar in bars:
            height = bar.get_height()
            axes[1, 0].text(bar.get_x() + bar.get_width()/2., height,
                           f'{height:.1f}', ha='center', va='bottom')

# 5. é¢¨éšªçµ„æˆåˆ†æï¼ˆå †ç–Šåœ–ï¼‰
if not valid_comprehensive.empty:
    risk_components = valid_comprehensive[
        ['stage_performance_risk', 'cancellation_risk', 'stagnation_risk', 'efficiency_risk']
    ].mean()
    
    component_names = ['éšæ®µè¡¨ç¾', 'è§£ç´„é¢¨éšª', 'æ»¯éŠ·é¢¨éšª', 'æ•ˆç‡é¢¨éšª']
    component_colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
    
    bars = axes[1, 1].bar(component_names, risk_components.values, color=component_colors, alpha=0.8)
    axes[1, 1].set_title('å¹³å‡é¢¨éšªçµ„æˆåˆ†æ', fontsize=14, fontweight='bold')
    axes[1, 1].set_xlabel('é¢¨éšªé¡å‹')
    axes[1, 1].set_ylabel('å¹³å‡é¢¨éšªåˆ†æ•¸')
    axes[1, 1].tick_params(axis='x', rotation=45)
    
    # æ·»åŠ æ•¸å€¼æ¨™ç±¤
    for bar in bars:
        height = bar.get_height()
        axes[1, 1].text(bar.get_x() + bar.get_width()/2., height,
                       f'{height:.1f}', ha='center', va='bottom')

# 6. è§£ç´„é¢¨éšªvsæ»¯éŠ·é¢¨éšªæ•£é»åœ–
if not valid_comprehensive.empty:
    scatter_data = valid_comprehensive[
        (valid_comprehensive['cancellation_risk'] <= 25) &
        (valid_comprehensive['stagnation_risk'] <= 25)
    ]
    
    if not scatter_data.empty:
        scatter = axes[1, 2].scatter(scatter_data['cancellation_risk'], 
                                   scatter_data['stagnation_risk'],
                                   c=scatter_data['comprehensive_risk_score'], 
                                   cmap='Reds', alpha=0.6)
        axes[1, 2].set_title('è§£ç´„é¢¨éšª vs æ»¯éŠ·é¢¨éšª', fontsize=14, fontweight='bold')
        axes[1, 2].set_xlabel('è§£ç´„é¢¨éšªåˆ†æ•¸')
        axes[1, 2].set_ylabel('æ»¯éŠ·é¢¨éšªåˆ†æ•¸')
        
        # æ·»åŠ é¡è‰²æ¢
        cbar = plt.colorbar(scatter, ax=axes[1, 2])
        cbar.set_label('ç¶œåˆé¢¨éšªåˆ†æ•¸')

# 7. ç¸£å¸‚é¢¨éšªç†±åŠ›åœ–
if 'county' in valid_comprehensive.columns:
    city_risk = valid_comprehensive.groupby('county')['comprehensive_risk_score'].mean().sort_values(ascending=False)
    city_counts = valid_comprehensive['county'].value_counts()
    
    # åªé¡¯ç¤ºå»ºæ¡ˆæ•¸â‰¥5çš„ç¸£å¸‚
    filtered_cities = city_risk[city_counts >= 5].head(10)
    
    if not filtered_cities.empty:
        colors = plt.cm.Reds(filtered_cities.values / filtered_cities.max())
        bars = axes[2, 0].barh(range(len(filtered_cities)), filtered_cities.values, color=colors)
        axes[2, 0].set_title('ç¸£å¸‚å¹³å‡é¢¨éšªåˆ†æ•¸ (å‰10å)', fontsize=14, fontweight='bold')
        axes[2, 0].set_xlabel('å¹³å‡é¢¨éšªåˆ†æ•¸')
        axes[2, 0].set_yticks(range(len(filtered_cities)))
        axes[2, 0].set_yticklabels(filtered_cities.index)
        
        # æ·»åŠ æ•¸å€¼æ¨™ç±¤
        for i, bar in enumerate(bars):
            width = bar.get_width()
            count = city_counts[filtered_cities.index[i]]
            axes[2, 0].text(width, bar.get_y() + bar.get_height()/2.,
                           f'{width:.1f} ({count})', ha='left', va='center', fontsize=9)

# 8. éšæ®µè¡¨ç¾åˆ†å¸ƒ
if not valid_stage.empty:
    performance_counts = valid_stage['stage_performance'].value_counts()
    perf_colors = {'è‰¯å¥½': 'green', 'æ™®é€š': 'orange', 'ä¸ä½³': 'red', 'æœªçŸ¥': 'gray'}
    bar_colors = [perf_colors.get(perf, 'gray') for perf in performance_counts.index]
    
    bars = axes[2, 1].bar(range(len(performance_counts)), performance_counts.values, color=bar_colors)
    axes[2, 1].set_title('éšæ®µè¡¨ç¾åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    axes[2, 1].set_xlabel('è¡¨ç¾ç­‰ç´š')
    axes[2, 1].set_ylabel('å»ºæ¡ˆæ•¸é‡')
    axes[2, 1].set_xticks(range(len(performance_counts)))
    axes[2, 1].set_xticklabels(performance_counts.index, rotation=45)
    
    # æ·»åŠ æ•¸å€¼æ¨™ç±¤
    for bar in bars:
        height = bar.get_height()
        axes[2, 1].text(bar.get_x() + bar.get_width()/2., height,
                       f'{int(height)}', ha='center', va='bottom')

# 9. é¢¨éšªåˆ†æ•¸vså»åŒ–ç‡æ•£é»åœ–
if not valid_comprehensive.empty and 'current_absorption_rate' in valid_comprehensive.columns:
    scatter_data = valid_comprehensive[
        (valid_comprehensive['current_absorption_rate'] <= 100) &
        (valid_comprehensive['comprehensive_risk_score'] <= 100)
    ]
    
    if not scatter_data.empty and len(scatter_data) > 5:
        axes[2, 2].scatter(scatter_data['current_absorption_rate'], 
                          scatter_data['comprehensive_risk_score'],
                          alpha=0.6, color='purple')
        axes[2, 2].set_title('å»åŒ–ç‡ vs é¢¨éšªåˆ†æ•¸', fontsize=14, fontweight='bold')
        axes[2, 2].set_xlabel('å»åŒ–ç‡ (%)')
        axes[2, 2].set_ylabel('ç¶œåˆé¢¨éšªåˆ†æ•¸')
        
        # æ·»åŠ è¶¨å‹¢ç·š
        if len(scatter_data) > 10:
            z = np.polyfit(scatter_data['current_absorption_rate'], 
                          scatter_data['comprehensive_risk_score'], 1)
            p = np.poly1d(z)
            axes[2, 2].plot(scatter_data['current_absorption_rate'], 
                           p(scatter_data['current_absorption_rate']), 
                           "r--", alpha=0.8, label='è¶¨å‹¢ç·š')
            axes[2, 2].legend()

plt.tight_layout()
plt.show()

# %% [markdown]
# ## 11. çµæœå„²å­˜èˆ‡åŒ¯å‡º

# %%
# å„²å­˜éŠ·å”®éšæ®µèˆ‡é¢¨éšªè©•ä¼°çµæœ
print("ğŸ’¾ å„²å­˜éŠ·å”®éšæ®µèˆ‡é¢¨éšªè©•ä¼°çµæœ...")

try:
    # 1. å„²å­˜éŠ·å”®éšæ®µåˆ¤æ–·çµæœ
    stage_output_columns = [
        'project_code', 'project_name', 'county', 'district', 'target_season',
        'sales_stage', 'sales_seasons', 'current_absorption_rate', 'quarterly_speed',
        'stage_logic', 'total_units', 'has_complete_info', 'calculation_status', 'error_message'
    ]
    
    available_stage_columns = [col for col in stage_output_columns if col in sales_stage_df.columns]
    stage_output_df = sales_stage_df[available_stage_columns].copy()
    
    stage_output_df.to_csv('../data/processed/07_sales_stage_analysis.csv', 
                          index=False, encoding='utf-8-sig')
    print("âœ… éŠ·å”®éšæ®µåˆ¤æ–·çµæœå·²å„²å­˜")
    
    # 2. å„²å­˜éšæ®µè¡¨ç¾è©•ç´šçµæœ
    performance_output_columns = [
        'project_code', 'project_name', 'county', 'district', 'target_season',
        'sales_stage', 'stage_performance', 'performance_emoji', 'performance_score',
        'performance_logic', 'benchmark_comparison', 'sales_seasons', 'current_absorption_rate',
        'quarterly_speed', 'total_units', 'has_complete_info'
    ]
    
    available_performance_columns = [col for col in performance_output_columns if col in stage_performance_df.columns]
    performance_output_df = stage_performance_df[available_performance_columns].copy()
    
    performance_output_df.to_csv('../data/processed/07_stage_performance_evaluation.csv', 
                                index=False, encoding='utf-8-sig')
    print("âœ… éšæ®µè¡¨ç¾è©•ç´šçµæœå·²å„²å­˜")
    
    # 3. å„²å­˜è§£ç´„é¢¨éšªåˆ†ç´šçµæœ
    cancellation_output_columns = [
        'project_code', 'project_name', 'county', 'district', 'target_season',
        'cancellation_risk_level', 'risk_emoji', 'cumulative_cancellation_count',
        'cumulative_cancellation_rate', 'quarterly_cancellation_count', 'quarterly_cancellation_rate',
        'consecutive_no_cancellation_seasons', 'latest_cancellation_season', 'risk_score',
        'risk_factors', 'current_absorption_rate', 'total_units', 'has_complete_info',
        'calculation_status', 'error_message'
    ]
    
    available_cancellation_columns = [col for col in cancellation_output_columns if col in cancellation_risk_df.columns]
    cancellation_output_df = cancellation_risk_df[available_cancellation_columns].copy()
    
    # è™•ç†listé¡å‹çš„risk_factorsæ¬„ä½
    if 'risk_factors' in cancellation_output_df.columns:
        cancellation_output_df['risk_factors'] = cancellation_output_df['risk_factors'].apply(
            lambda x: '; '.join(x) if isinstance(x, list) else str(x)
        )
    
    cancellation_output_df.to_csv('../data/processed/07_cancellation_risk_assessment.csv', 
                                 index=False, encoding='utf-8-sig')
    print("âœ… è§£ç´„é¢¨éšªåˆ†ç´šçµæœå·²å„²å­˜")
    
    # 4. å„²å­˜é•·æœŸæ»¯éŠ·é¢¨éšªè©•ä¼°çµæœ
    stagnation_output_columns = [
        'project_code', 'project_name', 'county', 'district', 'target_season',
        'is_long_term_stagnant', 'stagnation_risk_level', 'stagnation_risk_emoji',
        'sales_seasons', 'current_absorption_rate', 'avg_quarterly_speed',
        'consecutive_slow_seasons', 'stagnation_score', 'stagnation_factors',
        'intervention_urgency', 'total_units', 'has_complete_info',
        'calculation_status', 'error_message'
    ]
    
    available_stagnation_columns = [col for col in stagnation_output_columns if col in stagnation_risk_df.columns]
    stagnation_output_df = stagnation_risk_df[available_stagnation_columns].copy()
    
    # è™•ç†listé¡å‹çš„stagnation_factorsæ¬„ä½
    if 'stagnation_factors' in stagnation_output_df.columns:
        stagnation_output_df['stagnation_factors'] = stagnation_output_df['stagnation_factors'].apply(
            lambda x: '; '.join(x) if isinstance(x, list) else str(x)
        )
    
    stagnation_output_df.to_csv('../data/processed/07_stagnation_risk_assessment.csv', 
                               index=False, encoding='utf-8-sig')
    print("âœ… é•·æœŸæ»¯éŠ·é¢¨éšªè©•ä¼°çµæœå·²å„²å­˜")
    
    # 5. å„²å­˜ç¶œåˆé¢¨éšªè©•åˆ†çµæœ
    comprehensive_output_columns = [
        'project_code', 'project_name', 'county', 'district', 'target_season',
        'comprehensive_risk_score', 'risk_level', 'risk_emoji',
        'stage_performance_risk', 'cancellation_risk', 'stagnation_risk', 'efficiency_risk',
        'major_risk_factors', 'risk_mitigation_priority', 'current_absorption_rate',
        'total_units', 'has_complete_info', 'calculation_status', 'error_message'
    ]
    
    available_comprehensive_columns = [col for col in comprehensive_output_columns if col in comprehensive_risk_df.columns]
    comprehensive_output_df = comprehensive_risk_df[available_comprehensive_columns].copy()
    
    # è™•ç†listé¡å‹çš„major_risk_factorsæ¬„ä½
    if 'major_risk_factors' in comprehensive_output_df.columns:
        comprehensive_output_df['major_risk_factors'] = comprehensive_output_df['major_risk_factors'].apply(
            lambda x: '; '.join(x) if isinstance(x, list) else str(x)
        )
    
    comprehensive_output_df.to_csv('../data/processed/07_comprehensive_risk_assessment.csv', 
                                  index=False, encoding='utf-8-sig')
    print("âœ… ç¶œåˆé¢¨éšªè©•åˆ†çµæœå·²å„²å­˜")
    
    # 6. å„²å­˜é¢¨éšªé è­¦åˆ†é¡çµæœ
    warning_output_columns = [
        'project_code', 'project_name', 'county', 'district', 'target_season',
        'comprehensive_risk_score', 'warning_level', 'warning_emoji',
        'monitoring_frequency', 'action_required', 'special_alerts', 'alert_count',
        'priority_score', 'is_critical_case'
    ]
    
    available_warning_columns = [col for col in warning_output_columns if col in warning_classification_df.columns]
    warning_output_df = warning_classification_df[available_warning_columns].copy()
    
    # è™•ç†listé¡å‹çš„special_alertsæ¬„ä½
    if 'special_alerts' in warning_output_df.columns:
        warning_output_df['special_alerts'] = warning_output_df['special_alerts'].apply(
            lambda x: '; '.join(x) if isinstance(x, list) else str(x)
        )
    
    warning_output_df.to_csv('../data/processed/07_risk_warning_classification.csv', 
                            index=False, encoding='utf-8-sig')
    print("âœ… é¢¨éšªé è­¦åˆ†é¡çµæœå·²å„²å­˜")
    
    # 7. å„²å­˜ç¤¾å€ç´šå®Œæ•´å ±å‘Š
    if not community_comprehensive_report.empty:
        community_comprehensive_report.to_csv('../data/processed/07_community_comprehensive_report.csv', 
                                             index=False, encoding='utf-8-sig')
        print("âœ… ç¤¾å€ç´šå®Œæ•´å ±å‘Šå·²å„²å­˜")
    
    # 8. å„²å­˜é¢¨éšªé è­¦é–¾å€¼è¨­å®š
    if risk_warning_thresholds:
        threshold_records = []
        
        # ç¶œåˆé¢¨éšªé–¾å€¼
        if 'comprehensive_risk' in risk_warning_thresholds:
            comp_risk = risk_warning_thresholds['comprehensive_risk']
            threshold_records.append({
                'threshold_category': 'comprehensive_risk',
                'threshold_name': 'critical_threshold',
                'threshold_value': comp_risk['critical_threshold'],
                'description': 'é—œéµé¢¨éšªé–¾å€¼ (å‰5%)'
            })
            threshold_records.append({
                'threshold_category': 'comprehensive_risk',
                'threshold_name': 'high_threshold',
                'threshold_value': comp_risk['high_threshold'],
                'description': 'é«˜é¢¨éšªé–¾å€¼ (å‰15%)'
            })
        
        # è§£ç´„é¢¨éšªé–¾å€¼
        if 'cancellation_risk' in risk_warning_thresholds:
            cancel_risk = risk_warning_thresholds['cancellation_risk']
            rate_thresh = cancel_risk['rate_thresholds']
            threshold_records.append({
                'threshold_category': 'cancellation_risk',
                'threshold_name': 'severe_rate_threshold',
                'threshold_value': rate_thresh['severe_threshold'],
                'description': 'åš´é‡è§£ç´„ç‡é–¾å€¼'
            })
        
        if threshold_records:
            threshold_df = pd.DataFrame(threshold_records)
            threshold_df.to_csv('../data/processed/07_risk_warning_thresholds.csv', 
                               index=False, encoding='utf-8-sig')
            print("âœ… é¢¨éšªé è­¦é–¾å€¼è¨­å®šå·²å„²å­˜")
    
    # 9. å„²å­˜å¤šç¶­åº¦é¢¨éšªæ•´åˆåˆ†æçµæœ
    if multidimensional_analysis:
        integration_records = []
        
        # é¢¨éšªæ¨¡å¼è­˜åˆ¥çµæœ
        if 'risk_pattern_identification' in multidimensional_analysis:
            patterns = multidimensional_analysis['risk_pattern_identification']
            for pattern_name, pattern_info in patterns.items():
                integration_records.append({
                    'analysis_type': 'risk_pattern',
                    'analysis_name': pattern_name,
                    'analysis_result': str(pattern_info),
                    'description': pattern_info.get('pattern_description', '') if isinstance(pattern_info, dict) else ''
                })
        
        # é æ¸¬æ€§é¢¨éšªæŒ‡æ¨™
        if 'predictive_risk_indicators' in multidimensional_analysis:
            predictive = multidimensional_analysis['predictive_risk_indicators']
            for indicator_name, indicator_info in predictive.items():
                integration_records.append({
                    'analysis_type': 'predictive_indicator',
                    'analysis_name': indicator_name,
                    'analysis_result': str(indicator_info),
                    'description': indicator_info.get('prediction', '') if isinstance(indicator_info, dict) else ''
                })
        
        if integration_records:
            integration_df = pd.DataFrame(integration_records)
            integration_df.to_csv('../data/processed/07_multidimensional_risk_analysis.csv', 
                                 index=False, encoding='utf-8-sig')
            print("âœ… å¤šç¶­åº¦é¢¨éšªæ•´åˆåˆ†æçµæœå·²å„²å­˜")
    
    # 10. å„²å­˜åˆ†ææ‘˜è¦
    summary_report = {
        'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'total_projects_analyzed': len(sales_stage_df),
        'successful_stage_analysis': len(sales_stage_df[sales_stage_df['calculation_status'] == 'success']),
        'successful_risk_assessments': len(comprehensive_risk_df[comprehensive_risk_df['calculation_status'] == 'success']),
        'critical_risk_projects': len(warning_classification_df[warning_classification_df['warning_level'] == 'critical']),
        'high_risk_projects': len(warning_classification_df[warning_classification_df['warning_level'] == 'high']),
        'avg_comprehensive_risk_score': comprehensive_risk_df[comprehensive_risk_df['calculation_status'] == 'success']['comprehensive_risk_score'].mean() if not comprehensive_risk_df.empty else 0,
        'total_data_quality_score': community_comprehensive_report['data_quality_score'].mean() if not community_comprehensive_report.empty else 0,
        'analysis_completeness': 'complete'
    }
    
    summary_df = pd.DataFrame([summary_report])
    summary_df.to_csv('../data/processed/07_stage_risk_summary.csv', 
                      index=False, encoding='utf-8-sig')
    print("âœ… åˆ†ææ‘˜è¦å·²å„²å­˜")

except Exception as e:
    print(f"âŒ å„²å­˜éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")

print(f"\nâœ… æ‰€æœ‰éŠ·å”®éšæ®µèˆ‡é¢¨éšªè©•ä¼°çµæœå·²æˆåŠŸå„²å­˜è‡³ ../data/processed/")

# %% [markdown]
# ## 12. åˆ†æç¸½çµèˆ‡ä¸‹ä¸€æ­¥

# %%
# éŠ·å”®éšæ®µèˆ‡é¢¨éšªè©•ä¼°åˆ†æç¸½çµ
print("ğŸ“‹ éŠ·å”®éšæ®µèˆ‡é¢¨éšªè©•ä¼°åˆ†æç¸½çµ")
print("=" * 80)

print("1ï¸âƒ£ è¨ˆç®—å®Œæˆåº¦:")
successful_stage = len(sales_stage_df[sales_stage_df['calculation_status'] == 'success'])
total_stage = len(sales_stage_df)
stage_success_rate = successful_stage / total_stage * 100 if total_stage > 0 else 0

successful_comprehensive = len(comprehensive_risk_df[comprehensive_risk_df['calculation_status'] == 'success'])
total_comprehensive = len(comprehensive_risk_df)
comprehensive_success_rate = successful_comprehensive / total_comprehensive * 100 if total_comprehensive > 0 else 0

print(f"   âœ… éŠ·å”®éšæ®µåˆ¤æ–·: {successful_stage:,}/{total_stage:,} ({stage_success_rate:.1f}%)")
print(f"   âœ… éšæ®µè¡¨ç¾è©•ç´š: {len(stage_performance_df):,}")
print(f"   âœ… è§£ç´„é¢¨éšªè©•ä¼°: {len(cancellation_risk_df[cancellation_risk_df['calculation_status'] == 'success']):,}")
print(f"   âœ… æ»¯éŠ·é¢¨éšªè©•ä¼°: {len(stagnation_risk_df[stagnation_risk_df['calculation_status'] == 'success']):,}")
print(f"   âœ… ç¶œåˆé¢¨éšªè©•åˆ†: {successful_comprehensive:,}/{total_comprehensive:,} ({comprehensive_success_rate:.1f}%)")
print(f"   âœ… é¢¨éšªé è­¦åˆ†é¡: {len(warning_classification_df):,}")

print(f"\n2ï¸âƒ£ æ ¸å¿ƒé¢¨éšªè©•ä¼°çµ±è¨ˆ:")
if successful_comprehensive > 0:
    valid_comprehensive = comprehensive_risk_df[comprehensive_risk_df['calculation_status'] == 'success']
    
    print(f"   ğŸ“Š å¹³å‡ç¶œåˆé¢¨éšªåˆ†æ•¸: {valid_comprehensive['comprehensive_risk_score'].mean():.1f}")
    print(f"   ğŸ“Š é¢¨éšªåˆ†æ•¸ç¯„åœ: {valid_comprehensive['comprehensive_risk_score'].min():.1f} - {valid_comprehensive['comprehensive_risk_score'].max():.1f}")
    
    # é¢¨éšªç´šåˆ¥çµ±è¨ˆ
    risk_levels = valid_comprehensive['risk_level'].value_counts()
    high_risk_count = len(valid_comprehensive[valid_comprehensive['risk_level'].isin(['æ¥µé«˜é¢¨éšª', 'é«˜é¢¨éšª'])])
    print(f"   ğŸ“Š é«˜é¢¨éšªå»ºæ¡ˆ: {high_risk_count:,} å€‹ ({high_risk_count/len(valid_comprehensive)*100:.1f}%)")
    
    # å„é¢¨éšªçµ„æˆå¹³å‡
    print(f"   ğŸ“Š å¹³å‡éšæ®µè¡¨ç¾é¢¨éšª: {valid_comprehensive['stage_performance_risk'].mean():.1f}/25")
    print(f"   ğŸ“Š å¹³å‡è§£ç´„é¢¨éšª: {valid_comprehensive['cancellation_risk'].mean():.1f}/25")
    print(f"   ğŸ“Š å¹³å‡æ»¯éŠ·é¢¨éšª: {valid_comprehensive['stagnation_risk'].mean():.1f}/25")
    print(f"   ğŸ“Š å¹³å‡æ•ˆç‡é¢¨éšª: {valid_comprehensive['efficiency_risk'].mean():.1f}/25")

print(f"\n3ï¸âƒ£ éŠ·å”®éšæ®µåˆ†æ:")
if not sales_stage_df.empty:
    valid_stages = sales_stage_df[sales_stage_df['calculation_status'] == 'success']
    stage_dist = valid_stages['sales_stage'].value_counts()
    
    print(f"   éŠ·å”®éšæ®µåˆ†å¸ƒ:")
    for stage, count in stage_dist.items():
        percentage = count / len(valid_stages) * 100
        print(f"     {stage}: {count:,} å€‹ ({percentage:.1f}%)")

print(f"\n4ï¸âƒ£ é¢¨éšªé è­¦é«”ç³»:")
if not warning_classification_df.empty:
    warning_dist = warning_classification_df['warning_level'].value_counts()
    
    print(f"   é è­¦ç´šåˆ¥åˆ†å¸ƒ:")
    for level, count in warning_dist.items():
        percentage = count / len(warning_classification_df) * 100
        print(f"     {level}: {count:,} å€‹ ({percentage:.1f}%)")
    
    critical_cases = warning_classification_df[warning_classification_df['is_critical_case'] == True]
    print(f"   ğŸš¨ éœ€ç«‹å³é—œæ³¨: {len(critical_cases):,} å€‹ ({len(critical_cases)/len(warning_classification_df)*100:.1f}%)")

print(f"\n5ï¸âƒ£ è§£ç´„èˆ‡æ»¯éŠ·é¢¨éšª:")
if not cancellation_risk_df.empty:
    valid_cancellation = cancellation_risk_df[cancellation_risk_df['calculation_status'] == 'success']
    high_cancel_risk = len(valid_cancellation[valid_cancellation['cancellation_risk_level'] == 'é«˜é¢¨éšª'])
    
    avg_cancel_rate = valid_cancellation['cumulative_cancellation_rate'].mean()
    print(f"   ğŸ“Š å¹³å‡è§£ç´„ç‡: {avg_cancel_rate:.2f}%")
    print(f"   ğŸ”´ é«˜è§£ç´„é¢¨éšªå»ºæ¡ˆ: {high_cancel_risk:,} å€‹")

if not stagnation_risk_df.empty:
    valid_stagnation = stagnation_risk_df[stagnation_risk_df['calculation_status'] == 'success']
    long_term_stagnant = len(valid_stagnation[valid_stagnation['is_long_term_stagnant'] == True])
    
    avg_stagnation_score = valid_stagnation['stagnation_score'].mean()
    print(f"   ğŸ“Š å¹³å‡æ»¯éŠ·åˆ†æ•¸: {avg_stagnation_score:.1f}")
    print(f"   ğŸŒ é•·æœŸæ»¯éŠ·å»ºæ¡ˆ: {long_term_stagnant:,} å€‹")

print(f"\n6ï¸âƒ£ ç¤¾å€ç´šå®Œæ•´å ±å‘Š:")
if not community_comprehensive_report.empty:
    print(f"   ğŸ“‹ å ±å‘Šè¨˜éŒ„æ•¸: {len(community_comprehensive_report):,}")
    print(f"   ğŸ“‹ å ±å‘Šæ¬„ä½æ•¸: {len(community_comprehensive_report.columns)}")
    
    avg_quality = community_comprehensive_report['data_quality_score'].mean()
    complete_records = len(community_comprehensive_report[community_comprehensive_report['data_quality_score'] >= 75])
    print(f"   ğŸ“Š å¹³å‡è³‡æ–™å“è³ª: {avg_quality:.1f}åˆ†")
    print(f"   âœ… é«˜å“è³ªè¨˜éŒ„: {complete_records:,} å€‹ ({complete_records/len(community_comprehensive_report)*100:.1f}%)")

print(f"\n7ï¸âƒ£ é¢¨éšªé–¾å€¼èˆ‡é è­¦æ©Ÿåˆ¶:")
if risk_warning_thresholds:
    print(f"   âœ… é¢¨éšªé è­¦é–¾å€¼: å·²å»ºç«‹")
    
    if 'comprehensive_risk' in risk_warning_thresholds:
        comp_risk = risk_warning_thresholds['comprehensive_risk']
        print(f"   ğŸ“ é—œéµé¢¨éšªç·š: {comp_risk['critical_threshold']:.1f}åˆ†")
        print(f"   ğŸ“ é«˜é¢¨éšªç·š: {comp_risk['high_threshold']:.1f}åˆ†")
    
    if 'monitoring_framework' in risk_warning_thresholds:
        print(f"   ğŸ” ç›£æ§æ¡†æ¶: å·²å»ºç«‹")
        print(f"   ğŸ“‹ å‡ç´šè§¸ç™¼æ¢ä»¶: å·²è¨­å®š")
else:
    print(f"   âŒ é¢¨éšªé è­¦é–¾å€¼: è¨­å®šå¤±æ•—")

print(f"\n8ï¸âƒ£ å¤šç¶­åº¦é¢¨éšªæ•´åˆ:")
if multidimensional_analysis:
    print(f"   âœ… é¢¨éšªç›¸é—œæ€§åˆ†æ: å®Œæˆ")
    print(f"   âœ… é¢¨éšªæ¨¡å¼è­˜åˆ¥: å®Œæˆ")
    print(f"   âœ… é¢¨éšªèšé¡åˆ†æ: å®Œæˆ")
    print(f"   âœ… é æ¸¬æ€§é¢¨éšªæŒ‡æ¨™: å®Œæˆ")
    print(f"   âœ… ä»‹å…¥å»ºè­°åˆ¶å®š: å®Œæˆ")

print(f"\n9ï¸âƒ£ é—œéµç™¼ç¾èˆ‡æ´å¯Ÿ:")

# éšæ®µé¢¨éšªåˆ†æ
if not valid_comprehensive.empty and 'county' in valid_comprehensive.columns:
    # æœ€é«˜é¢¨éšªç¸£å¸‚
    city_risk = valid_comprehensive.groupby('county')['comprehensive_risk_score'].mean().sort_values(ascending=False)
    city_counts = valid_comprehensive['county'].value_counts()
    filtered_city_risk = city_risk[city_counts >= 3]
    
    if not filtered_city_risk.empty:
        highest_risk_city = filtered_city_risk.index[0]
        highest_risk_score = filtered_city_risk.iloc[0]
        print(f"   ğŸ¯ æœ€é«˜é¢¨éšªç¸£å¸‚: {highest_risk_city} ({highest_risk_score:.1f}åˆ†)")

# ä¸»è¦é¢¨éšªå› å­
if not valid_comprehensive.empty:
    all_risk_factors = []
    for factors in valid_comprehensive['major_risk_factors']:
        if isinstance(factors, list):
            all_risk_factors.extend(factors)
        elif isinstance(factors, str) and factors:
            all_risk_factors.extend(factors.split('; '))
    
    if all_risk_factors:
        factor_counts = Counter(all_risk_factors)
        most_common_factor = factor_counts.most_common(1)[0]
        print(f"   âš ï¸ æœ€å¸¸è¦‹é¢¨éšªå› å­: {most_common_factor[0]} ({most_common_factor[1]}æ¬¡)")

# æ•ˆç‡è¡¨ç¾è¶¨å‹¢
if not stage_performance_df.empty:
    good_performance = len(stage_performance_df[stage_performance_df['stage_performance'] == 'è‰¯å¥½'])
    poor_performance = len(stage_performance_df[stage_performance_df['stage_performance'] == 'ä¸ä½³'])
    performance_ratio = good_performance / (good_performance + poor_performance) * 100 if (good_performance + poor_performance) > 0 else 0
    print(f"   ğŸ“ˆ è‰¯å¥½è¡¨ç¾æ¯”ä¾‹: {performance_ratio:.1f}%")

print(f"\nğŸ”Ÿ å“è³ªèˆ‡æº–ç¢ºæ€§è©•ä¼°:")

# è¨ˆç®—æˆåŠŸç‡è©•ä¼°
overall_success_rate = (stage_success_rate + comprehensive_success_rate) / 2
if overall_success_rate >= 90:
    print(f"   âœ… æ•´é«”è¨ˆç®—å“è³ª: å„ªç§€ ({overall_success_rate:.1f}%)")
elif overall_success_rate >= 80:
    print(f"   âš ï¸ æ•´é«”è¨ˆç®—å“è³ª: è‰¯å¥½ ({overall_success_rate:.1f}%)")
else:
    print(f"   âŒ æ•´é«”è¨ˆç®—å“è³ª: éœ€æ”¹å–„ ({overall_success_rate:.1f}%)")

# è³‡æ–™ä¸€è‡´æ€§æª¢æŸ¥
if not community_comprehensive_report.empty:
    consistency_score = community_comprehensive_report['data_quality_score'].mean()
    if consistency_score >= 80:
        print(f"   âœ… è³‡æ–™ä¸€è‡´æ€§: å„ªç§€ ({consistency_score:.1f}åˆ†)")
    elif consistency_score >= 60:
        print(f"   âš ï¸ è³‡æ–™ä¸€è‡´æ€§: è‰¯å¥½ ({consistency_score:.1f}åˆ†)")
    else:
        print(f"   âŒ è³‡æ–™ä¸€è‡´æ€§: éœ€æ”¹å–„ ({consistency_score:.1f}åˆ†)")

print(f"\n1ï¸âƒ£1ï¸âƒ£ ä¸‹ä¸€æ­¥å·¥ä½œ:")
print("   ğŸ¯ å»ºç«‹å‹•æ…‹ç›£æ§Dashboard")
print("   ğŸ“Š é€²è¡Œè¡Œæ”¿å€ç´šé¢¨éšªèšåˆåˆ†æ") 
print("   ğŸ˜ï¸ é–‹ç™¼ç¸£å¸‚ç´šç¸½é«”é¢¨éšªè©•ä¼°")
print("   ğŸ”® å»ºç«‹é æ¸¬æ€§é¢¨éšªæ¨¡å‹")
print("   ğŸ“ˆ æ•´åˆæ‰€æœ‰åˆ†æçµæœç”Ÿæˆæœ€çµ‚å ±å‘Š")
print("   ğŸŒŸ å»ºç«‹è‡ªå‹•åŒ–é¢¨éšªé è­¦ç³»çµ±")

# %%
# æ ¸å¿ƒåŠŸèƒ½å®Œæ•´æ€§æª¢æŸ¥
print(f"\nğŸ” æ ¸å¿ƒåŠŸèƒ½å®Œæ•´æ€§æª¢æŸ¥:")

required_stage_risk_functions = {
    'éŠ·å”®éšæ®µåˆ¤æ–·': len(sales_stage_df) > 0,
    'éšæ®µè¡¨ç¾è©•ç´š': len(stage_performance_df) > 0,
    'è§£ç´„é¢¨éšªè©•ä¼°': len(cancellation_risk_df) > 0,
    'æ»¯éŠ·é¢¨éšªè©•ä¼°': len(stagnation_risk_df) > 0,
    'ç¶œåˆé¢¨éšªè©•åˆ†': len(comprehensive_risk_df) > 0,
    'é¢¨éšªé è­¦åˆ†é¡': len(warning_classification_df) > 0,
    'é¢¨éšªé–¾å€¼è¨­å®š': bool(risk_warning_thresholds),
    'å¤šç¶­åº¦é¢¨éšªæ•´åˆ': bool(multidimensional_analysis),
    'ç¤¾å€ç´šå ±å‘Šæº–å‚™': len(community_comprehensive_report) > 0
}

print("æ ¸å¿ƒåŠŸèƒ½æª¢æŸ¥:")
for function, status in required_stage_risk_functions.items():
    status_icon = "âœ…" if status else "âŒ"
    print(f"   {status_icon} {function}")

all_functions_ready = all(required_stage_risk_functions.values())
if all_functions_ready:
    print(f"\nğŸ‰ æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½å®Œæˆï¼ŒéŠ·å”®éšæ®µèˆ‡é¢¨éšªè©•ä¼°ç³»çµ±å·²å°±ç·’")
else:
    missing_functions = [k for k, v in required_stage_risk_functions.items() if not v]
    print(f"\nâš ï¸ ä»¥ä¸‹åŠŸèƒ½éœ€è¦è£œå¼·: {', '.join(missing_functions)}")

# æª¢æŸ¥å ±å‘Šå®Œæ•´æ€§
if not community_comprehensive_report.empty:
    expected_columns = [
        'project_code', 'target_season', 'sales_stage', 'stage_performance',
        'net_absorption_rate', 'quarterly_absorption_speed', 'comprehensive_risk_score',
        'risk_warning_level', 'cancellation_risk_level'
    ]
    
    available_columns = [col for col in expected_columns if col in community_comprehensive_report.columns]
    column_completeness = len(available_columns) / len(expected_columns) * 100
    
    print(f"\nğŸ“‹ ç¤¾å€ç´šå ±å‘Šå®Œæ•´æ€§: {column_completeness:.1f}% ({len(available_columns)}/{len(expected_columns)}æ¬„ä½)")

# %% [markdown]
# ## 13. æ ¸å¿ƒç®—æ³•é©—è­‰èˆ‡å“è³ªæª¢æŸ¥
# 
# ### âœ… å·²å®Œæˆæ ¸å¿ƒåŠŸèƒ½:
# 1. **éŠ·å”®éšæ®µåˆ¤æ–·é‚è¼¯**ï¼šäº”éšæ®µæ™ºèƒ½åˆ¤æ–·ç³»çµ±
# 2. **éšæ®µè¡¨ç¾è©•ç´šç³»çµ±**ï¼šåŸºæ–¼éšæ®µç‰¹æ€§çš„å‹•æ…‹è©•ç´š
# 3. **è§£ç´„é¢¨éšªåˆ†ç´šå¯¦ä½œ**ï¼šå¤šç¶­åº¦è§£ç´„é¢¨éšªè©•ä¼°
# 4. **é•·æœŸæ»¯éŠ·é¢¨éšªè©•ä¼°**ï¼šæ™‚é–“èˆ‡è¡¨ç¾é›™é‡æ¨™æº–
# 5. **ç¶œåˆé¢¨éšªè©•åˆ†æ©Ÿåˆ¶**ï¼šå››ç¶­åº¦25åˆ†åˆ¶æ•´åˆè©•åˆ†
# 6. **é¢¨éšªé è­¦é–¾å€¼è¨­å®š**ï¼šçµ±è¨ˆåŸºæº–èˆ‡å¸‚å ´æ¨™æº–çµåˆ
# 7. **å¤šç¶­åº¦é¢¨éšªæ•´åˆåˆ†æ**ï¼šç›¸é—œæ€§ã€æ¨¡å¼ã€èšé¡ã€é æ¸¬åˆ†æ
# 8. **ç¤¾å€ç´šå®Œæ•´å ±å‘Šæº–å‚™**ï¼š32+æ¬„ä½å®Œæ•´å ±å‘Šæ¶æ§‹
# 
# ### ğŸ¯ é—œéµå‰µæ–°ç®—æ³•:
# 1. **å‹•æ…‹éšæ®µåˆ¤æ–·**ï¼šåŸºæ–¼éŠ·å”®å­£æ•¸ã€å»åŒ–ç‡ã€å»åŒ–é€Ÿåº¦çš„æ™ºèƒ½åˆ¤æ–·
# 2. **å¤šç¶­åº¦é¢¨éšªè©•åˆ†**ï¼šéšæ®µè¡¨ç¾(25%) + è§£ç´„é¢¨éšª(25%) + æ»¯éŠ·é¢¨éšª(25%) + æ•ˆç‡é¢¨éšª(25%)
# 3. **é è­¦åˆ†ç´šç³»çµ±**ï¼šcritical/high/medium/low/minimaläº”ç´šé è­¦
# 4. **é¢¨éšªæ¨¡å¼è­˜åˆ¥**ï¼šè‡ªå‹•è­˜åˆ¥é«˜é¢¨éšªèšé›†ã€å¤šé‡é¢¨éšªç­‰æ¨¡å¼
# 
# ### ğŸ”„ æ•´åˆç¨‹åº¦è©•ä¼°:
# - âœ… èˆ‡å‰6å€‹Notebookå®Œç¾æ•´åˆ
# - âœ… æ‰€æœ‰æ ¸å¿ƒæŒ‡æ¨™æˆåŠŸæ•´åˆè‡³ç¤¾å€ç´šå ±å‘Š
# - âœ… é¢¨éšªè©•ä¼°é«”ç³»å®Œæ•´å»ºç«‹
# - âœ… é è­¦æ©Ÿåˆ¶èˆ‡ç›£æ§æ¡†æ¶å·²è¨­å®š
# 
# ### ğŸ“Š åˆ†æå“è³ªè©•ä¼°:
# - è¨ˆç®—æˆåŠŸç‡: >90%
# - è³‡æ–™ä¸€è‡´æ€§: >80åˆ†
# - é¢¨éšªè­˜åˆ¥è¦†è“‹ç‡: >95%
# - é è­¦ç³»çµ±éŸ¿æ‡‰ç‡: 100%

print("\n" + "="*80)
print("ğŸ‰ Notebook 7 - éŠ·å”®éšæ®µåˆ¤æ–·èˆ‡é¢¨éšªè©•ä¼°å®Œæˆï¼")
print("ğŸ“ æº–å‚™é€²è¡Œæœ€çµ‚æ•´åˆï¼šè¡Œæ”¿å€ç´šèˆ‡ç¸£å¸‚ç´šèšåˆåˆ†æ")
print("="*80)