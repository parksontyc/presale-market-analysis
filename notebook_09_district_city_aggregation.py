# é å”®å±‹å¸‚å ´åˆ†æç³»çµ± - 09_è¡Œæ”¿å€ç´šèˆ‡ç¸£å¸‚ç´šèšåˆåˆ†æ
# åŸºæ–¼ PRD v2.3 è¦æ ¼é€²è¡Œå¤šå±¤ç´šèšåˆåˆ†æèˆ‡å ±å‘Šç”Ÿæˆ
# ================================================================================

# %% [markdown]
# # é å”®å±‹å¸‚å ´åˆ†æç³»çµ± - è¡Œæ”¿å€ç´šèˆ‡ç¸£å¸‚ç´šèšåˆåˆ†æ
# 
# ## ğŸ“‹ ç›®æ¨™
# - âœ… å¯¦ä½œè¡Œæ”¿å€ç´š18æ¬„ä½å ±å‘Š
# - âœ… å¯¦ä½œç¸£å¸‚ç´š19æ¬„ä½å ±å‘Š  
# - âœ… é©—è­‰ä¸‰å±¤ç´šè³‡æ–™ä¸€è‡´æ€§
# - âœ… å¯¦ç¾æ•ˆç‡æ’åèˆ‡è¡¨ç¾åˆ†ç´š
# - âœ… é–‹ç™¼ç†±é»å€åŸŸè­˜åˆ¥é‚è¼¯
# - âœ… å»ºç«‹è·¨å±¤ç´šé¢¨éšªèšåˆæ©Ÿåˆ¶
# - âœ… ç”Ÿæˆå®Œæ•´ä¸‰å±¤ç´šå ±å‘Šé«”ç³»
# 
# ## ğŸ¯ å…§å®¹å¤§ç¶±
# 1. ç’°å¢ƒè¨­å®šèˆ‡è³‡æ–™è¼‰å…¥
# 2. è¡Œæ”¿å€ç´šèšåˆé‚è¼¯è¨­è¨ˆ
# 3. è¡Œæ”¿å€ç´š18æ¬„ä½å ±å‘Šå¯¦ä½œ
# 4. ç¸£å¸‚ç´šèšåˆé‚è¼¯è¨­è¨ˆ
# 5. ç¸£å¸‚ç´š19æ¬„ä½å ±å‘Šå¯¦ä½œ
# 6. æ•ˆç‡æ’åèˆ‡è¡¨ç¾åˆ†ç´šç®—æ³•
# 7. ç†±é»å€åŸŸè­˜åˆ¥èˆ‡é¢¨éšªèšåˆ
# 8. ä¸‰å±¤ç´šè³‡æ–™ä¸€è‡´æ€§æª¢æŸ¥
# 9. è·¨å±¤ç´šè¶¨å‹¢åˆ†æ
# 10. å®Œæ•´å ±å‘Šç”Ÿæˆèˆ‡é©—è­‰
# 11. è¦–è¦ºåŒ–åˆ†æèˆ‡æ´å¯Ÿ
# 12. çµæœè¼¸å‡ºèˆ‡ç¸½çµ
# 
# ## ğŸ“Š å¤šå±¤ç´šå ±å‘Šæ¶æ§‹
# - ğŸ¢ **ç¤¾å€ç´š (32æ¬„ä½)**: å€‹åˆ¥å»ºæ¡ˆè©³ç´°åˆ†æ
# - ğŸ˜ï¸ **è¡Œæ”¿å€ç´š (18æ¬„ä½)**: å€åŸŸå¸‚å ´èšåˆåˆ†æ  
# - ğŸ™ï¸ **ç¸£å¸‚ç´š (19æ¬„ä½)**: ç¸½é«”å¸‚å ´æ¦‚æ³åˆ†æ

# %% [markdown]
# ## 1. ç’°å¢ƒè¨­å®šèˆ‡è³‡æ–™è¼‰å…¥

# %%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import re
import warnings
from collections import Counter, defaultdict
import math
from scipy import stats
import json
warnings.filterwarnings('ignore')

# è¨­å®šé¡¯ç¤ºé¸é …
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)
pd.set_option('display.width', None)
pd.set_option('display.max_colwidth', 80)

# è¨­å®šä¸­æ–‡å­—å‹
plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'Arial Unicode MS', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False

# è¨­å®šåœ–è¡¨æ¨£å¼
sns.set_style("whitegrid")
plt.style.use('default')

print("âœ… ç’°å¢ƒè¨­å®šå®Œæˆ")
print(f"ğŸ“… åˆ†ææ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

# %%
# è¼‰å…¥ç¤¾å€ç´šå®Œæ•´å ±å‘Šä½œç‚ºåŸºç¤è³‡æ–™
print("ğŸ”„ è¼‰å…¥ç¤¾å€ç´šå®Œæ•´å ±å‘Š...")

try:
    # å°‹æ‰¾æœ€æ–°çš„ç¤¾å€ç´šå ±å‘Šæª”æ¡ˆ
    import glob
    import os
    
    # æœå°‹ç¬¦åˆå‘½åæ¨¡å¼çš„æª”æ¡ˆ
    community_report_files = glob.glob('../data/processed/community_level_comprehensive_report_*.csv')
    
    if community_report_files:
        # å–æœ€æ–°çš„æª”æ¡ˆ
        latest_file = max(community_report_files, key=os.path.getctime)
        community_report = pd.read_csv(latest_file, encoding='utf-8')
        print(f"âœ… è¼‰å…¥ç¤¾å€ç´šå ±å‘Š: {os.path.basename(latest_file)}")
        print(f"   è¨˜éŒ„æ•¸: {len(community_report):,}")
        print(f"   æ¬„ä½æ•¸: {len(community_report.columns)}")
    else:
        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå˜—è©¦è¼‰å…¥é è¨­æª”å
        community_report = pd.read_csv('../data/processed/community_level_comprehensive_report.csv', encoding='utf-8')
        print(f"âœ… è¼‰å…¥é è¨­ç¤¾å€ç´šå ±å‘Š")
        print(f"   è¨˜éŒ„æ•¸: {len(community_report):,}")
        print(f"   æ¬„ä½æ•¸: {len(community_report.columns)}")
    
    # è¼‰å…¥å…¶ä»–è¼”åŠ©è³‡æ–™
    try:
        # è¼‰å…¥å ±å‘Šæ‘˜è¦
        summary_files = glob.glob('../data/processed/community_report_summary_*.json')
        if summary_files:
            latest_summary = max(summary_files, key=os.path.getctime)
            with open(latest_summary, 'r', encoding='utf-8') as f:
                community_summary = json.load(f)
            print(f"âœ… è¼‰å…¥å ±å‘Šæ‘˜è¦: {os.path.basename(latest_summary)}")
        else:
            community_summary = {}
    except:
        community_summary = {}
        print("âš ï¸ ç„¡æ³•è¼‰å…¥å ±å‘Šæ‘˜è¦ï¼Œä½¿ç”¨ç©ºç™½æ‘˜è¦")

except FileNotFoundError as e:
    print(f"âŒ æª”æ¡ˆè¼‰å…¥å¤±æ•—: {e}")
    print("ğŸ“ è«‹ç¢ºèªæ˜¯å¦å·²åŸ·è¡Œ Notebook 8 ç”Ÿæˆç¤¾å€ç´šå ±å‘Š")
    raise
except Exception as e:
    print(f"âŒ è¼‰å…¥éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
    raise

# %%
# æª¢æŸ¥ç¤¾å€ç´šå ±å‘Šçµæ§‹
print(f"\nğŸ“Š ç¤¾å€ç´šå ±å‘Šçµæ§‹æª¢æŸ¥:")

if not community_report.empty:
    print(f"åŸºæœ¬çµ±è¨ˆ:")
    print(f"   ç¸½è¨˜éŒ„æ•¸: {len(community_report):,}")
    print(f"   ç¸½æ¬„ä½æ•¸: {len(community_report.columns)}")
    print(f"   å”¯ä¸€å»ºæ¡ˆæ•¸: {community_report['å‚™æŸ¥ç·¨è™Ÿ'].nunique():,}")
    print(f"   æ¶µè“‹å¹´å­£æ•¸: {community_report['å¹´å­£'].nunique()}")
    print(f"   æ¶µè“‹ç¸£å¸‚æ•¸: {community_report['ç¸£å¸‚'].nunique()}")
    print(f"   æ¶µè“‹è¡Œæ”¿å€æ•¸: {community_report['è¡Œæ”¿å€'].nunique()}")
    
    print(f"\né‡è¦æ¬„ä½å®Œæ•´åº¦:")
    key_columns = ['å‚™æŸ¥ç·¨è™Ÿ', 'ç¸£å¸‚', 'è¡Œæ”¿å€', 'å¹´å­£', 'ç¸½æˆ¶æ•¸', 'æ·¨å»åŒ–ç‡(%)', 'ç´¯ç©æˆäº¤ç­†æ•¸']
    for col in key_columns:
        if col in community_report.columns:
            non_null_count = len(community_report[community_report[col].notna() & (community_report[col] != '')])
            completeness = non_null_count / len(community_report) * 100
            print(f"   {col}: {completeness:.1f}% ({non_null_count:,}/{len(community_report):,})")
    
    # ç¸£å¸‚åˆ†å¸ƒ
    county_dist = community_report['ç¸£å¸‚'].value_counts()
    print(f"\nç¸£å¸‚åˆ†å¸ƒ (å‰8å):")
    for county, count in county_dist.head(8).items():
        percentage = count / len(community_report) * 100
        print(f"   {county}: {count:,} å€‹ ({percentage:.1f}%)")
    
    # å¹´å­£åˆ†å¸ƒ
    season_dist = community_report['å¹´å­£'].value_counts().sort_index()
    print(f"\nå¹´å­£åˆ†å¸ƒ:")
    for season, count in season_dist.items():
        percentage = count / len(community_report) * 100
        print(f"   {season}: {count:,} å€‹ ({percentage:.1f}%)")

# %% [markdown]
# ## 2. å·¥å…·å‡½æ•¸èˆ‡åŸºç¤è¨­å®š

# %%
# å®šç¾©è¡Œæ”¿å€ç´šèˆ‡ç¸£å¸‚ç´šå ±å‘Šæ¶æ§‹
print("ğŸ“‹ å®šç¾©å¤šå±¤ç´šå ±å‘Šæ¶æ§‹")
print("=" * 60)

# è¡Œæ”¿å€ç´šå ±å‘Šæ ¼å¼ï¼ˆ18æ¬„ä½ï¼‰
DISTRICT_REPORT_SCHEMA = {
    'basic_info': ['ç¸£å¸‚', 'è¡Œæ”¿å€', 'å¹´å­£'],
    'project_statistics': ['æ´»èºå»ºæ¡ˆæ•¸', 'æ­£å¸¸æ´»èºå»ºæ¡ˆæ•¸', 'é•·æœŸæ»¯éŠ·å»ºæ¡ˆæ•¸'],
    'absorption_metrics': ['å€åŸŸç¸½æˆ¶æ•¸', 'æ•´é«”æ·¨å»åŒ–ç‡(%)', 'æ­£å¸¸å»ºæ¡ˆå»åŒ–ç‡(%)'],
    'cancellation_metrics': ['å€åŸŸç¸½è§£ç´„ç­†æ•¸', 'å€åŸŸè§£ç´„ç‡(%)', 'å€åŸŸè§£ç´„é¢¨éšªç­‰ç´š'],
    'dynamics_metrics': ['å€åŸŸå¹³å‡å»åŒ–é€Ÿåº¦(æˆ¶/å­£)', 'å€åŸŸå»åŒ–æ•ˆç‡æ’å', 'å€åŸŸå»åŒ–è¶¨å‹¢'],
    'market_metrics': ['åŠ æ¬Šå¹³å‡å–®åƒ¹(è¬/åª)', 'é•·æœŸæ»¯éŠ·å½±éŸ¿åº¦(%)', 'å€åŸŸéšæ®µ', 'é¢¨éšªç­‰ç´š']
}

# ç¸£å¸‚ç´šå ±å‘Šæ ¼å¼ï¼ˆ19æ¬„ä½ï¼‰
CITY_REPORT_SCHEMA = {
    'basic_info': ['ç¸£å¸‚', 'å¹´å­£'],
    'project_statistics': ['æ´»èºè¡Œæ”¿å€æ•¸', 'ç¸£å¸‚ç¸½å»ºæ¡ˆæ•¸', 'æ–°æ¨æ¡ˆæ•¸é‡', 'å®Œå”®å»ºæ¡ˆæ•¸é‡'],
    'absorption_metrics': ['ç¸£å¸‚ç¸½æˆ¶æ•¸', 'ç¸£å¸‚ç¸½æˆäº¤æ•¸', 'ç¸£å¸‚åŠ æ¬Šå»åŒ–ç‡(%)', 'é•·æœŸæ»¯éŠ·å»ºæ¡ˆå æ¯”(%)'],
    'cancellation_metrics': ['ç¸£å¸‚ç¸½è§£ç´„ç­†æ•¸', 'ç¸£å¸‚è§£ç´„ç‡(%)', 'ç¸£å¸‚è§£ç´„é¢¨éšªç­‰ç´š'],
    'performance_metrics': ['ç¸£å¸‚å¹³å‡å»åŒ–é€Ÿåº¦(æˆ¶/å­£)', 'ç¸£å¸‚å»åŒ–è¡¨ç¾åˆ†ç´š'],
    'market_metrics': ['ç¸£å¸‚åŠ æ¬Šå¹³å‡å–®åƒ¹(è¬/åª)', 'åƒ¹æ ¼æ¼²è·Œå¹…(%)', 'ä¸»è¦ç†±é»è¡Œæ”¿å€', 'é«˜é¢¨éšªè¡Œæ”¿å€æ•¸', 'ç¸£å¸‚é¢¨éšªç­‰ç´š']
}

print(f"âœ… è¡Œæ”¿å€ç´šå ±å‘Š: {sum(len(fields) for fields in DISTRICT_REPORT_SCHEMA.values())} æ¬„ä½")
print(f"âœ… ç¸£å¸‚ç´šå ±å‘Š: {sum(len(fields) for fields in CITY_REPORT_SCHEMA.values())} æ¬„ä½")

# %%
# å·¥å…·å‡½æ•¸å®šç¾©
def season_to_number(season_str):
    """å°‡å¹´å­£å­—ä¸²è½‰æ›ç‚ºå¯æ¯”è¼ƒçš„æ•¸å­—"""
    try:
        if not season_str or pd.isna(season_str):
            return 0
        season_str = str(season_str).strip()
        if 'Y' not in season_str or 'S' not in season_str:
            return 0
        year_part = season_str.split('Y')[0]
        season_part = season_str.split('Y')[1].replace('S', '')
        return int(year_part) * 10 + int(season_part)
    except:
        return 0

def calculate_weighted_average(values, weights):
    """è¨ˆç®—åŠ æ¬Šå¹³å‡å€¼"""
    try:
        if len(values) == 0 or len(weights) == 0:
            return 0.0
        
        valid_pairs = [(v, w) for v, w in zip(values, weights) 
                      if not pd.isna(v) and not pd.isna(w) and w > 0]
        
        if not valid_pairs:
            return 0.0
        
        total_weight = sum(w for v, w in valid_pairs)
        if total_weight == 0:
            return 0.0
        
        weighted_sum = sum(v * w for v, w in valid_pairs)
        return weighted_sum / total_weight
    except:
        return 0.0

def classify_risk_level(score):
    """æ ¹æ“šåˆ†æ•¸åˆ†é¡é¢¨éšªç­‰ç´š"""
    if score >= 7:
        return "ğŸ”´ é«˜é¢¨éšª"
    elif score >= 4:
        return "ğŸŸ¡ ä¸­é¢¨éšª"
    else:
        return "ğŸŸ¢ ä½é¢¨éšª"

def classify_absorption_efficiency(speed, rate):
    """åˆ†é¡å»åŒ–æ•ˆç‡ç­‰ç´š"""
    if rate >= 100:
        if speed >= 3:
            return "ğŸš€ é«˜æ•ˆå®Œå”®"
        else:
            return "â­ æ­£å¸¸å®Œå”®"
    elif rate >= 70 and speed >= 3:
        return "ğŸš€ é«˜æ•ˆå»åŒ–"
    elif rate >= 50 and speed >= 2:
        return "â­ æ­£å¸¸å»åŒ–"
    elif rate >= 30 and speed >= 1:
        return "âš ï¸ ç·©æ…¢å»åŒ–"
    else:
        return "ğŸŒ æ»¯éŠ·ç‹€æ…‹"

print("âœ… å·¥å…·å‡½æ•¸æº–å‚™å®Œæˆ")

# %% [markdown]
# ## 3. è¡Œæ”¿å€ç´šèšåˆé‚è¼¯è¨­è¨ˆ

# %%
# è¡Œæ”¿å€ç´šèšåˆåˆ†æé‚è¼¯
print("ğŸ˜ï¸ è¡Œæ”¿å€ç´šèšåˆåˆ†æé‚è¼¯è¨­è¨ˆ")
print("=" * 50)

def calculate_district_project_statistics(district_data):
    """
    è¨ˆç®—è¡Œæ”¿å€å»ºæ¡ˆçµ±è¨ˆæŒ‡æ¨™
    
    Args:
        district_data: è©²è¡Œæ”¿å€çš„ç¤¾å€ç´šè³‡æ–™
        
    Returns:
        dict: å»ºæ¡ˆçµ±è¨ˆæŒ‡æ¨™
    """
    stats = {
        'æ´»èºå»ºæ¡ˆæ•¸': 0,
        'æ­£å¸¸æ´»èºå»ºæ¡ˆæ•¸': 0,
        'é•·æœŸæ»¯éŠ·å»ºæ¡ˆæ•¸': 0
    }
    
    try:
        # æ´»èºå»ºæ¡ˆï¼šæœ‰äº¤æ˜“è¨˜éŒ„ä¸”æœªå®Œå”®
        active_projects = district_data[
            (district_data['ç´¯ç©æˆäº¤ç­†æ•¸'] > 0) | 
            (district_data['æ·¨å»åŒ–ç‡(%)'] < 100)
        ]
        stats['æ´»èºå»ºæ¡ˆæ•¸'] = len(active_projects)
        
        # é•·æœŸæ»¯éŠ·å»ºæ¡ˆï¼šéŠ·å”®è¶…é12å­£ä¸”å»åŒ–ç‡<70%
        stagnant_projects = district_data[
            (district_data['éŠ·å”®å­£æ•¸'] > 12) & 
            (district_data['æ·¨å»åŒ–ç‡(%)'] < 70)
        ]
        stats['é•·æœŸæ»¯éŠ·å»ºæ¡ˆæ•¸'] = len(stagnant_projects)
        
        # æ­£å¸¸æ´»èºå»ºæ¡ˆ
        stats['æ­£å¸¸æ´»èºå»ºæ¡ˆæ•¸'] = stats['æ´»èºå»ºæ¡ˆæ•¸'] - stats['é•·æœŸæ»¯éŠ·å»ºæ¡ˆæ•¸']
        stats['æ­£å¸¸æ´»èºå»ºæ¡ˆæ•¸'] = max(0, stats['æ­£å¸¸æ´»èºå»ºæ¡ˆæ•¸'])
        
    except Exception as e:
        print(f"âŒ å»ºæ¡ˆçµ±è¨ˆè¨ˆç®—éŒ¯èª¤: {e}")
    
    return stats

def calculate_district_absorption_metrics(district_data):
    """
    è¨ˆç®—è¡Œæ”¿å€å»åŒ–æŒ‡æ¨™
    
    Args:
        district_data: è©²è¡Œæ”¿å€çš„ç¤¾å€ç´šè³‡æ–™
        
    Returns:
        dict: å»åŒ–æŒ‡æ¨™
    """
    metrics = {
        'å€åŸŸç¸½æˆ¶æ•¸': 0,
        'æ•´é«”æ·¨å»åŒ–ç‡(%)': 0.0,
        'æ­£å¸¸å»ºæ¡ˆå»åŒ–ç‡(%)': 0.0
    }
    
    try:
        # å€åŸŸç¸½æˆ¶æ•¸
        metrics['å€åŸŸç¸½æˆ¶æ•¸'] = district_data['ç¸½æˆ¶æ•¸'].sum()
        
        # æ•´é«”æ·¨å»åŒ–ç‡ï¼ˆæˆ¶æ•¸åŠ æ¬Šï¼‰
        if metrics['å€åŸŸç¸½æˆ¶æ•¸'] > 0:
            total_transactions = district_data['ç´¯ç©æˆäº¤ç­†æ•¸'].sum()
            total_cancellations = district_data['ç´¯ç©è§£ç´„ç­†æ•¸'].sum()
            net_transactions = total_transactions - total_cancellations
            metrics['æ•´é«”æ·¨å»åŒ–ç‡(%)'] = (net_transactions / metrics['å€åŸŸç¸½æˆ¶æ•¸']) * 100
        
        # æ­£å¸¸å»ºæ¡ˆå»åŒ–ç‡ï¼ˆæ’é™¤é•·æœŸæ»¯éŠ·ï¼‰
        normal_projects = district_data[
            ~((district_data['éŠ·å”®å­£æ•¸'] > 12) & (district_data['æ·¨å»åŒ–ç‡(%)'] < 70))
        ]
        
        if not normal_projects.empty:
            normal_total_units = normal_projects['ç¸½æˆ¶æ•¸'].sum()
            if normal_total_units > 0:
                normal_total_transactions = normal_projects['ç´¯ç©æˆäº¤ç­†æ•¸'].sum()
                normal_total_cancellations = normal_projects['ç´¯ç©è§£ç´„ç­†æ•¸'].sum()
                normal_net_transactions = normal_total_transactions - normal_total_cancellations
                metrics['æ­£å¸¸å»ºæ¡ˆå»åŒ–ç‡(%)'] = (normal_net_transactions / normal_total_units) * 100
        
        # ç¢ºä¿æ•¸å€¼åˆç†æ€§
        metrics['æ•´é«”æ·¨å»åŒ–ç‡(%)'] = max(0, min(120, metrics['æ•´é«”æ·¨å»åŒ–ç‡(%)']))
        metrics['æ­£å¸¸å»ºæ¡ˆå»åŒ–ç‡(%)'] = max(0, min(120, metrics['æ­£å¸¸å»ºæ¡ˆå»åŒ–ç‡(%)']))
        
    except Exception as e:
        print(f"âŒ å»åŒ–æŒ‡æ¨™è¨ˆç®—éŒ¯èª¤: {e}")
    
    return metrics

def calculate_district_cancellation_metrics(district_data):
    """
    è¨ˆç®—è¡Œæ”¿å€è§£ç´„æŒ‡æ¨™
    
    Args:
        district_data: è©²è¡Œæ”¿å€çš„ç¤¾å€ç´šè³‡æ–™
        
    Returns:
        dict: è§£ç´„æŒ‡æ¨™
    """
    metrics = {
        'å€åŸŸç¸½è§£ç´„ç­†æ•¸': 0,
        'å€åŸŸè§£ç´„ç‡(%)': 0.0,
        'å€åŸŸè§£ç´„é¢¨éšªç­‰ç´š': 'ğŸŸ¢ ä½é¢¨éšª'
    }
    
    try:
        # å€åŸŸç¸½è§£ç´„ç­†æ•¸
        metrics['å€åŸŸç¸½è§£ç´„ç­†æ•¸'] = district_data['ç´¯ç©è§£ç´„ç­†æ•¸'].sum()
        
        # å€åŸŸè§£ç´„ç‡
        total_transactions = district_data['ç´¯ç©æˆäº¤ç­†æ•¸'].sum()
        if total_transactions > 0:
            metrics['å€åŸŸè§£ç´„ç‡(%)'] = (metrics['å€åŸŸç¸½è§£ç´„ç­†æ•¸'] / total_transactions) * 100
        
        # è§£ç´„é¢¨éšªç­‰ç´šè©•ä¼°
        high_risk_projects = len(district_data[district_data['ç´¯ç©è§£ç´„ç‡(%)'] > 5])
        total_projects = len(district_data)
        
        risk_score = 0
        
        # å€åŸŸæ•´é«”è§£ç´„ç‡
        if metrics['å€åŸŸè§£ç´„ç‡(%)'] > 3:
            risk_score += 3
        elif metrics['å€åŸŸè§£ç´„ç‡(%)'] > 1.5:
            risk_score += 1
        
        # é«˜é¢¨éšªå»ºæ¡ˆæ¯”ä¾‹
        if total_projects > 0:
            high_risk_ratio = high_risk_projects / total_projects * 100
            if high_risk_ratio > 30:
                risk_score += 2
            elif high_risk_ratio > 15:
                risk_score += 1
        
        metrics['å€åŸŸè§£ç´„é¢¨éšªç­‰ç´š'] = classify_risk_level(risk_score)
        
    except Exception as e:
        print(f"âŒ è§£ç´„æŒ‡æ¨™è¨ˆç®—éŒ¯èª¤: {e}")
    
    return metrics

def calculate_district_dynamics_metrics(district_data, all_district_data=None):
    """
    è¨ˆç®—è¡Œæ”¿å€å‹•æ…‹æŒ‡æ¨™
    
    Args:
        district_data: è©²è¡Œæ”¿å€çš„ç¤¾å€ç´šè³‡æ–™
        all_district_data: æ‰€æœ‰è¡Œæ”¿å€è³‡æ–™ï¼ˆç”¨æ–¼æ’åï¼‰
        
    Returns:
        dict: å‹•æ…‹æŒ‡æ¨™
    """
    metrics = {
        'å€åŸŸå¹³å‡å»åŒ–é€Ÿåº¦(æˆ¶/å­£)': 0.0,
        'å€åŸŸå»åŒ–æ•ˆç‡æ’å': 'N/A',
        'å€åŸŸå»åŒ–è¶¨å‹¢': 'ğŸ“ˆ ç©©å®š'
    }
    
    try:
        # å€åŸŸå¹³å‡å»åŒ–é€Ÿåº¦ï¼ˆæˆ¶æ•¸åŠ æ¬Šå¹³å‡ï¼‰
        valid_speed_data = district_data[district_data['å­£åº¦å»åŒ–é€Ÿåº¦(æˆ¶/å­£)'] > 0]
        if not valid_speed_data.empty:
            total_units = valid_speed_data['ç¸½æˆ¶æ•¸'].sum()
            if total_units > 0:
                weighted_speed = (valid_speed_data['ç¸½æˆ¶æ•¸'] * valid_speed_data['å­£åº¦å»åŒ–é€Ÿåº¦(æˆ¶/å­£)']).sum()
                metrics['å€åŸŸå¹³å‡å»åŒ–é€Ÿåº¦(æˆ¶/å­£)'] = weighted_speed / total_units
        
        # å€åŸŸå»åŒ–è¶¨å‹¢ï¼ˆåŸºæ–¼æ•ˆç‡è©•ç´šåˆ†å¸ƒï¼‰
        efficiency_grades = district_data['å»åŒ–æ•ˆç‡è©•ç´š'].value_counts()
        
        high_efficiency_count = 0
        for grade in efficiency_grades.index:
            if 'ğŸš€' in str(grade) or 'é«˜æ•ˆ' in str(grade):
                high_efficiency_count += efficiency_grades[grade]
        
        total_with_grade = len(district_data[district_data['å»åŒ–æ•ˆç‡è©•ç´š'] != ''])
        if total_with_grade > 0:
            high_efficiency_ratio = high_efficiency_count / total_with_grade
            
            if high_efficiency_ratio > 0.5:
                metrics['å€åŸŸå»åŒ–è¶¨å‹¢'] = 'ğŸš€ åŠ é€Ÿå»åŒ–'
            elif high_efficiency_ratio > 0.3:
                metrics['å€åŸŸå»åŒ–è¶¨å‹¢'] = 'ğŸ“ˆ ç©©å®šå»åŒ–'
            elif high_efficiency_ratio > 0.1:
                metrics['å€åŸŸå»åŒ–è¶¨å‹¢'] = 'ğŸ“‰ æ¸›ç·©å»åŒ–'
            else:
                metrics['å€åŸŸå»åŒ–è¶¨å‹¢'] = 'âš ï¸ å»åŒ–åœæ»¯'
        
        # å€åŸŸå»åŒ–æ•ˆç‡æ’åï¼ˆæš«æ™‚è¨­ç‚ºN/Aï¼Œå¾ŒçºŒæ‰¹é‡è¨ˆç®—æ™‚æ›´æ–°ï¼‰
        metrics['å€åŸŸå»åŒ–æ•ˆç‡æ’å'] = 'N/A'
        
    except Exception as e:
        print(f"âŒ å‹•æ…‹æŒ‡æ¨™è¨ˆç®—éŒ¯èª¤: {e}")
    
    return metrics

def calculate_district_market_metrics(district_data):
    """
    è¨ˆç®—è¡Œæ”¿å€å¸‚å ´æŒ‡æ¨™
    
    Args:
        district_data: è©²è¡Œæ”¿å€çš„ç¤¾å€ç´šè³‡æ–™
        
    Returns:
        dict: å¸‚å ´æŒ‡æ¨™
    """
    metrics = {
        'åŠ æ¬Šå¹³å‡å–®åƒ¹(è¬/åª)': 0.0,
        'é•·æœŸæ»¯éŠ·å½±éŸ¿åº¦(%)': 0.0,
        'å€åŸŸéšæ®µ': 'ç©©å®šéŠ·å”®æœŸ',
        'é¢¨éšªç­‰ç´š': 'ğŸŸ¢ ä½é¢¨éšª'
    }
    
    try:
        # åŠ æ¬Šå¹³å‡å–®åƒ¹ï¼ˆæˆäº¤ç­†æ•¸åŠ æ¬Šï¼‰
        valid_price_data = district_data[
            (district_data['å¹³å‡äº¤æ˜“å–®åƒ¹(è¬/åª)'] > 0) &
            (district_data['è©²å­£æˆäº¤ç­†æ•¸'] > 0)
        ]
        
        if not valid_price_data.empty:
            total_transactions = valid_price_data['è©²å­£æˆäº¤ç­†æ•¸'].sum()
            if total_transactions > 0:
                weighted_price = (valid_price_data['è©²å­£æˆäº¤ç­†æ•¸'] * valid_price_data['å¹³å‡äº¤æ˜“å–®åƒ¹(è¬/åª)']).sum()
                metrics['åŠ æ¬Šå¹³å‡å–®åƒ¹(è¬/åª)'] = weighted_price / total_transactions
        
        # é•·æœŸæ»¯éŠ·å½±éŸ¿åº¦
        stagnant_count = len(district_data[
            (district_data['éŠ·å”®å­£æ•¸'] > 12) & 
            (district_data['æ·¨å»åŒ–ç‡(%)'] < 70)
        ])
        total_projects = len(district_data)
        
        if total_projects > 0:
            metrics['é•·æœŸæ»¯éŠ·å½±éŸ¿åº¦(%)'] = (stagnant_count / total_projects) * 100
        
        # å€åŸŸéšæ®µåˆ¤æ–·ï¼ˆåŸºæ–¼ä¸»è¦å»ºæ¡ˆéšæ®µï¼‰
        stage_counts = district_data['éŠ·å”®éšæ®µ'].value_counts()
        if not stage_counts.empty:
            dominant_stage = stage_counts.index[0]
            metrics['å€åŸŸéšæ®µ'] = dominant_stage if dominant_stage else 'ç©©å®šéŠ·å”®æœŸ'
        
        # ç¶œåˆé¢¨éšªç­‰ç´š
        risk_factors = [
            metrics['é•·æœŸæ»¯éŠ·å½±éŸ¿åº¦(%)'] > 25,  # æ»¯éŠ·åš´é‡
            district_data['ç´¯ç©è§£ç´„ç‡(%)'].mean() > 5,  # å¹³å‡è§£ç´„ç‡é«˜
            metrics['å€åŸŸå¹³å‡å»åŒ–é€Ÿåº¦(æˆ¶/å­£)'] < 1,  # å»åŒ–é€Ÿåº¦æ…¢
            len(district_data[district_data['æ·¨å»åŒ–ç‡(%)'] < 30]) / len(district_data) > 0.5  # ä½å»åŒ–ç‡å»ºæ¡ˆéå¤š
        ]
        
        risk_score = sum(risk_factors)
        metrics['é¢¨éšªç­‰ç´š'] = classify_risk_level(risk_score * 2)  # èª¿æ•´æ¬Šé‡
        
    except Exception as e:
        print(f"âŒ å¸‚å ´æŒ‡æ¨™è¨ˆç®—éŒ¯èª¤: {e}")
    
    return metrics

print("âœ… è¡Œæ”¿å€ç´šèšåˆé‚è¼¯è¨­è¨ˆå®Œæˆ")

# %% [markdown]
# ## 4. è¡Œæ”¿å€ç´š18æ¬„ä½å ±å‘Šå¯¦ä½œ

# %%
# è¡Œæ”¿å€ç´šå ±å‘Šç”Ÿæˆ
print("ğŸ˜ï¸ è¡Œæ”¿å€ç´š18æ¬„ä½å ±å‘Šç”Ÿæˆ")
print("=" * 50)

def generate_district_level_report(community_data):
    """
    ç”Ÿæˆè¡Œæ”¿å€ç´šå®Œæ•´å ±å‘Š
    
    Args:
        community_data: ç¤¾å€ç´šå®Œæ•´è³‡æ–™
        
    Returns:
        DataFrame: è¡Œæ”¿å€ç´šå ±å‘Š
    """
    
    district_reports = []
    
    try:
        # æŒ‰ç¸£å¸‚ã€è¡Œæ”¿å€ã€å¹´å­£åˆ†çµ„
        district_groups = community_data.groupby(['ç¸£å¸‚', 'è¡Œæ”¿å€', 'å¹´å­£'])
        
        print(f"ğŸ”„ è™•ç† {len(district_groups)} å€‹è¡Œæ”¿å€-å¹´å­£çµ„åˆ...")
        
        for (county, district, season), group_data in district_groups:
            if group_data.empty:
                continue
            
            # å»ºç«‹åŸºæœ¬è³‡è¨Š
            district_report = {
                'ç¸£å¸‚': county,
                'è¡Œæ”¿å€': district,
                'å¹´å­£': season
            }
            
            # è¨ˆç®—å„é¡æŒ‡æ¨™
            project_stats = calculate_district_project_statistics(group_data)
            absorption_metrics = calculate_district_absorption_metrics(group_data)
            cancellation_metrics = calculate_district_cancellation_metrics(group_data)
            dynamics_metrics = calculate_district_dynamics_metrics(group_data)
            market_metrics = calculate_district_market_metrics(group_data)
            
            # æ•´åˆæ‰€æœ‰æŒ‡æ¨™
            district_report.update(project_stats)
            district_report.update(absorption_metrics)
            district_report.update(cancellation_metrics)
            district_report.update(dynamics_metrics)
            district_report.update(market_metrics)
            
            district_reports.append(district_report)
        
        # è½‰æ›ç‚ºDataFrame
        district_df = pd.DataFrame(district_reports)
        
        # è¨ˆç®—å€åŸŸå»åŒ–æ•ˆç‡æ’å
        if not district_df.empty:
            district_df = calculate_district_efficiency_ranking(district_df)
        
        print(f"âœ… å®Œæˆ {len(district_df)} ç­†è¡Œæ”¿å€ç´šå ±å‘Šç”Ÿæˆ")
        
        return district_df
    
    except Exception as e:
        print(f"âŒ è¡Œæ”¿å€ç´šå ±å‘Šç”ŸæˆéŒ¯èª¤: {e}")
        return pd.DataFrame()

def calculate_district_efficiency_ranking(district_df):
    """
    è¨ˆç®—å€åŸŸå»åŒ–æ•ˆç‡æ’å
    
    Args:
        district_df: è¡Œæ”¿å€ç´šå ±å‘ŠDataFrame
        
    Returns:
        DataFrame: å«æ’åçš„è¡Œæ”¿å€ç´šå ±å‘Š
    """
    
    try:
        # æŒ‰ç¸£å¸‚å’Œå¹´å­£åˆ†çµ„è¨ˆç®—æ’å
        for (county, season), group in district_df.groupby(['ç¸£å¸‚', 'å¹´å­£']):
            if len(group) <= 1:
                # å¦‚æœè©²ç¸£å¸‚è©²å¹´å­£åªæœ‰ä¸€å€‹è¡Œæ”¿å€ï¼Œæ’åç‚ºç¬¬1å
                district_df.loc[group.index, 'å€åŸŸå»åŒ–æ•ˆç‡æ’å'] = 'ç¬¬1å'
                continue
            
            # è¨ˆç®—æ•ˆç‡åˆ†æ•¸ï¼ˆç¶œåˆå»åŒ–ç‡å’Œå»åŒ–é€Ÿåº¦ï¼‰
            efficiency_scores = []
            for idx, row in group.iterrows():
                score = 0
                
                # å»åŒ–ç‡æ¬Šé‡40%
                absorption_rate = row.get('æ•´é«”æ·¨å»åŒ–ç‡(%)', 0)
                if absorption_rate >= 70:
                    score += 40
                elif absorption_rate >= 50:
                    score += 30
                elif absorption_rate >= 30:
                    score += 20
                else:
                    score += 10
                
                # å»åŒ–é€Ÿåº¦æ¬Šé‡40%
                absorption_speed = row.get('å€åŸŸå¹³å‡å»åŒ–é€Ÿåº¦(æˆ¶/å­£)', 0)
                if absorption_speed >= 3:
                    score += 40
                elif absorption_speed >= 2:
                    score += 30
                elif absorption_speed >= 1:
                    score += 20
                else:
                    score += 10
                
                # æ»¯éŠ·å½±éŸ¿æ¬Šé‡20%ï¼ˆè² é¢åˆ†æ•¸ï¼‰
                stagnant_impact = row.get('é•·æœŸæ»¯éŠ·å½±éŸ¿åº¦(%)', 0)
                if stagnant_impact < 10:
                    score += 20
                elif stagnant_impact < 25:
                    score += 15
                elif stagnant_impact < 40:
                    score += 10
                else:
                    score += 5
                
                efficiency_scores.append((idx, score))
            
            # æ ¹æ“šåˆ†æ•¸æ’å
            efficiency_scores.sort(key=lambda x: x[1], reverse=True)
            
            for rank, (idx, score) in enumerate(efficiency_scores, 1):
                district_df.loc[idx, 'å€åŸŸå»åŒ–æ•ˆç‡æ’å'] = f'ç¬¬{rank}å'
        
        return district_df
    
    except Exception as e:
        print(f"âŒ æ•ˆç‡æ’åè¨ˆç®—éŒ¯èª¤: {e}")
        return district_df

# %%
# ç”Ÿæˆè¡Œæ”¿å€ç´šå ±å‘Š
print("ğŸ”„ é–‹å§‹ç”Ÿæˆè¡Œæ”¿å€ç´šå ±å‘Š...")

district_level_report = generate_district_level_report(community_report)

if not district_level_report.empty:
    print(f"âœ… è¡Œæ”¿å€ç´šå ±å‘Šç”Ÿæˆå®Œæˆ")
    print(f"   ç¸½è¨˜éŒ„æ•¸: {len(district_level_report):,}")
    print(f"   æ¶µè“‹ç¸£å¸‚æ•¸: {district_level_report['ç¸£å¸‚'].nunique()}")
    print(f"   æ¶µè“‹è¡Œæ”¿å€æ•¸: {district_level_report['è¡Œæ”¿å€'].nunique()}")
    print(f"   æ¶µè“‹å¹´å­£æ•¸: {district_level_report['å¹´å­£'].nunique()}")
    
    # é©—è­‰18æ¬„ä½çµæ§‹
    expected_columns = []
    for category, fields in DISTRICT_REPORT_SCHEMA.items():
        expected_columns.extend(fields)
    
    actual_columns = list(district_level_report.columns)
    print(f"   å¯¦éš›æ¬„ä½æ•¸: {len(actual_columns)}")
    print(f"   æœŸæœ›æ¬„ä½æ•¸: {len(expected_columns)}")
    
    missing_columns = set(expected_columns) - set(actual_columns)
    if missing_columns:
        print(f"   âš ï¸ ç¼ºå¤±æ¬„ä½: {missing_columns}")
    else:
        print(f"   âœ… æ¬„ä½çµæ§‹å®Œæ•´")

# %%
# è¡Œæ”¿å€ç´šå ±å‘Šçµ±è¨ˆåˆ†æ
print(f"\nğŸ“Š è¡Œæ”¿å€ç´šå ±å‘Šçµ±è¨ˆåˆ†æ:")

if not district_level_report.empty:
    # åŸºæœ¬çµ±è¨ˆ
    print(f"åŸºæœ¬çµ±è¨ˆ:")
    print(f"   å¹³å‡æ´»èºå»ºæ¡ˆæ•¸: {district_level_report['æ´»èºå»ºæ¡ˆæ•¸'].mean():.1f}")
    print(f"   å¹³å‡å€åŸŸç¸½æˆ¶æ•¸: {district_level_report['å€åŸŸç¸½æˆ¶æ•¸'].mean():.0f}")
    print(f"   å¹³å‡æ•´é«”æ·¨å»åŒ–ç‡: {district_level_report['æ•´é«”æ·¨å»åŒ–ç‡(%)'].mean():.1f}%")
    print(f"   å¹³å‡å€åŸŸè§£ç´„ç‡: {district_level_report['å€åŸŸè§£ç´„ç‡(%)'].mean():.2f}%")
    
    # é¢¨éšªåˆ†å¸ƒ
    risk_dist = district_level_report['é¢¨éšªç­‰ç´š'].value_counts()
    print(f"\né¢¨éšªç­‰ç´šåˆ†å¸ƒ:")
    for risk, count in risk_dist.items():
        percentage = count / len(district_level_report) * 100
        print(f"   {risk}: {count:,} å€‹ ({percentage:.1f}%)")
    
    # è§£ç´„é¢¨éšªåˆ†å¸ƒ
    cancellation_risk_dist = district_level_report['å€åŸŸè§£ç´„é¢¨éšªç­‰ç´š'].value_counts()
    print(f"\nè§£ç´„é¢¨éšªåˆ†å¸ƒ:")
    for risk, count in cancellation_risk_dist.items():
        percentage = count / len(district_level_report) * 100
        print(f"   {risk}: {count:,} å€‹ ({percentage:.1f}%)")
    
    # å»åŒ–è¶¨å‹¢åˆ†å¸ƒ
    trend_dist = district_level_report['å€åŸŸå»åŒ–è¶¨å‹¢'].value_counts()
    print(f"\nå»åŒ–è¶¨å‹¢åˆ†å¸ƒ:")
    for trend, count in trend_dist.items():
        percentage = count / len(district_level_report) * 100
        print(f"   {trend}: {count:,} å€‹ ({percentage:.1f}%)")
    
    # é•·æœŸæ»¯éŠ·åˆ†æ
    high_stagnant = len(district_level_report[district_level_report['é•·æœŸæ»¯éŠ·å½±éŸ¿åº¦(%)'] > 25])
    print(f"\né•·æœŸæ»¯éŠ·åˆ†æ:")
    print(f"   åš´é‡æ»¯éŠ·å€åŸŸ: {high_stagnant:,} å€‹ ({high_stagnant/len(district_level_report)*100:.1f}%)")
    print(f"   å¹³å‡æ»¯éŠ·å½±éŸ¿åº¦: {district_level_report['é•·æœŸæ»¯éŠ·å½±éŸ¿åº¦(%)'].mean():.1f}%")

# %% [markdown]
# ## 5. ç¸£å¸‚ç´šèšåˆé‚è¼¯è¨­è¨ˆ

# %%
# ç¸£å¸‚ç´šèšåˆåˆ†æé‚è¼¯
print("ğŸ™ï¸ ç¸£å¸‚ç´šèšåˆåˆ†æé‚è¼¯è¨­è¨ˆ")
print("=" * 50)

def calculate_city_project_statistics(city_data, district_data):
    """
    è¨ˆç®—ç¸£å¸‚å»ºæ¡ˆçµ±è¨ˆæŒ‡æ¨™
    
    Args:
        city_data: è©²ç¸£å¸‚çš„ç¤¾å€ç´šè³‡æ–™
        district_data: è©²ç¸£å¸‚çš„è¡Œæ”¿å€ç´šè³‡æ–™
        
    Returns:
        dict: å»ºæ¡ˆçµ±è¨ˆæŒ‡æ¨™
    """
    stats = {
        'æ´»èºè¡Œæ”¿å€æ•¸': 0,
        'ç¸£å¸‚ç¸½å»ºæ¡ˆæ•¸': 0,
        'æ–°æ¨æ¡ˆæ•¸é‡': 0,
        'å®Œå”®å»ºæ¡ˆæ•¸é‡': 0
    }
    
    try:
        # æ´»èºè¡Œæ”¿å€æ•¸
        if not district_data.empty:
            active_districts = district_data[district_data['æ´»èºå»ºæ¡ˆæ•¸'] > 0]
            stats['æ´»èºè¡Œæ”¿å€æ•¸'] = len(active_districts)
        
        # ç¸£å¸‚ç¸½å»ºæ¡ˆæ•¸
        stats['ç¸£å¸‚ç¸½å»ºæ¡ˆæ•¸'] = len(city_data)
        
        # æ–°æ¨æ¡ˆæ•¸é‡ï¼ˆå‡è¨­ç‚ºç•¶å­£éŠ·å”®èµ·å§‹çš„å»ºæ¡ˆï¼‰
        current_season = city_data['å¹´å­£'].iloc[0] if not city_data.empty else ''
        new_projects = city_data[city_data['éŠ·å”®èµ·å§‹å¹´å­£'] == current_season]
        stats['æ–°æ¨æ¡ˆæ•¸é‡'] = len(new_projects)
        
        # å®Œå”®å»ºæ¡ˆæ•¸é‡
        completed_projects = city_data[city_data['æ·¨å»åŒ–ç‡(%)'] >= 100]
        stats['å®Œå”®å»ºæ¡ˆæ•¸é‡'] = len(completed_projects)
        
    except Exception as e:
        print(f"âŒ ç¸£å¸‚å»ºæ¡ˆçµ±è¨ˆè¨ˆç®—éŒ¯èª¤: {e}")
    
    return stats

def calculate_city_absorption_metrics(city_data):
    """
    è¨ˆç®—ç¸£å¸‚å»åŒ–æŒ‡æ¨™
    
    Args:
        city_data: è©²ç¸£å¸‚çš„ç¤¾å€ç´šè³‡æ–™
        
    Returns:
        dict: å»åŒ–æŒ‡æ¨™
    """
    metrics = {
        'ç¸£å¸‚ç¸½æˆ¶æ•¸': 0,
        'ç¸£å¸‚ç¸½æˆäº¤æ•¸': 0,
        'ç¸£å¸‚åŠ æ¬Šå»åŒ–ç‡(%)': 0.0,
        'é•·æœŸæ»¯éŠ·å»ºæ¡ˆå æ¯”(%)': 0.0
    }
    
    try:
        # ç¸£å¸‚ç¸½æˆ¶æ•¸
        metrics['ç¸£å¸‚ç¸½æˆ¶æ•¸'] = city_data['ç¸½æˆ¶æ•¸'].sum()
        
        # ç¸£å¸‚ç¸½æˆäº¤æ•¸
        metrics['ç¸£å¸‚ç¸½æˆäº¤æ•¸'] = city_data['ç´¯ç©æˆäº¤ç­†æ•¸'].sum()
        
        # ç¸£å¸‚åŠ æ¬Šå»åŒ–ç‡ï¼ˆæˆ¶æ•¸åŠ æ¬Šï¼‰
        if metrics['ç¸£å¸‚ç¸½æˆ¶æ•¸'] > 0:
            total_transactions = city_data['ç´¯ç©æˆäº¤ç­†æ•¸'].sum()
            total_cancellations = city_data['ç´¯ç©è§£ç´„ç­†æ•¸'].sum()
            net_transactions = total_transactions - total_cancellations
            metrics['ç¸£å¸‚åŠ æ¬Šå»åŒ–ç‡(%)'] = (net_transactions / metrics['ç¸£å¸‚ç¸½æˆ¶æ•¸']) * 100
        
        # é•·æœŸæ»¯éŠ·å»ºæ¡ˆå æ¯”
        stagnant_projects = city_data[
            (city_data['éŠ·å”®å­£æ•¸'] > 12) & 
            (city_data['æ·¨å»åŒ–ç‡(%)'] < 70)
        ]
        
        if len(city_data) > 0:
            metrics['é•·æœŸæ»¯éŠ·å»ºæ¡ˆå æ¯”(%)'] = (len(stagnant_projects) / len(city_data)) * 100
        
        # ç¢ºä¿æ•¸å€¼åˆç†æ€§
        metrics['ç¸£å¸‚åŠ æ¬Šå»åŒ–ç‡(%)'] = max(0, min(120, metrics['ç¸£å¸‚åŠ æ¬Šå»åŒ–ç‡(%)']))
        
    except Exception as e:
        print(f"âŒ ç¸£å¸‚å»åŒ–æŒ‡æ¨™è¨ˆç®—éŒ¯èª¤: {e}")
    
    return metrics

def calculate_city_cancellation_metrics(city_data):
    """
    è¨ˆç®—ç¸£å¸‚è§£ç´„æŒ‡æ¨™
    
    Args:
        city_data: è©²ç¸£å¸‚çš„ç¤¾å€ç´šè³‡æ–™
        
    Returns:
        dict: è§£ç´„æŒ‡æ¨™
    """
    metrics = {
        'ç¸£å¸‚ç¸½è§£ç´„ç­†æ•¸': 0,
        'ç¸£å¸‚è§£ç´„ç‡(%)': 0.0,
        'ç¸£å¸‚è§£ç´„é¢¨éšªç­‰ç´š': 'ğŸŸ¢ ä½é¢¨éšª'
    }
    
    try:
        # ç¸£å¸‚ç¸½è§£ç´„ç­†æ•¸
        metrics['ç¸£å¸‚ç¸½è§£ç´„ç­†æ•¸'] = city_data['ç´¯ç©è§£ç´„ç­†æ•¸'].sum()
        
        # ç¸£å¸‚è§£ç´„ç‡
        total_transactions = city_data['ç´¯ç©æˆäº¤ç­†æ•¸'].sum()
        if total_transactions > 0:
            metrics['ç¸£å¸‚è§£ç´„ç‡(%)'] = (metrics['ç¸£å¸‚ç¸½è§£ç´„ç­†æ•¸'] / total_transactions) * 100
        
        # è§£ç´„é¢¨éšªç­‰ç´šè©•ä¼°
        risk_score = 0
        
        # ç¸£å¸‚æ•´é«”è§£ç´„ç‡
        if metrics['ç¸£å¸‚è§£ç´„ç‡(%)'] > 2:
            risk_score += 3
        elif metrics['ç¸£å¸‚è§£ç´„ç‡(%)'] > 1:
            risk_score += 1
        
        # é«˜é¢¨éšªå»ºæ¡ˆæ¯”ä¾‹
        high_risk_projects = len(city_data[city_data['ç´¯ç©è§£ç´„ç‡(%)'] > 5])
        total_projects = len(city_data)
        
        if total_projects > 0:
            high_risk_ratio = high_risk_projects / total_projects * 100
            if high_risk_ratio > 25:
                risk_score += 2
            elif high_risk_ratio > 15:
                risk_score += 1
        
        # é•·æœŸæ»¯éŠ·å½±éŸ¿
        stagnant_ratio = len(city_data[
            (city_data['éŠ·å”®å­£æ•¸'] > 12) & 
            (city_data['æ·¨å»åŒ–ç‡(%)'] < 70)
        ]) / total_projects * 100 if total_projects > 0 else 0
        
        if stagnant_ratio > 15:
            risk_score += 1
        
        metrics['ç¸£å¸‚è§£ç´„é¢¨éšªç­‰ç´š'] = classify_risk_level(risk_score)
        
    except Exception as e:
        print(f"âŒ ç¸£å¸‚è§£ç´„æŒ‡æ¨™è¨ˆç®—éŒ¯èª¤: {e}")
    
    return metrics

def calculate_city_performance_metrics(city_data):
    """
    è¨ˆç®—ç¸£å¸‚è¡¨ç¾æŒ‡æ¨™
    
    Args:
        city_data: è©²ç¸£å¸‚çš„ç¤¾å€ç´šè³‡æ–™
        
    Returns:
        dict: è¡¨ç¾æŒ‡æ¨™
    """
    metrics = {
        'ç¸£å¸‚å¹³å‡å»åŒ–é€Ÿåº¦(æˆ¶/å­£)': 0.0,
        'ç¸£å¸‚å»åŒ–è¡¨ç¾åˆ†ç´š': 'ğŸ¥ˆ æ™®é€šè¡¨ç¾'
    }
    
    try:
        # ç¸£å¸‚å¹³å‡å»åŒ–é€Ÿåº¦ï¼ˆæˆ¶æ•¸åŠ æ¬Šå¹³å‡ï¼‰
        valid_speed_data = city_data[city_data['å­£åº¦å»åŒ–é€Ÿåº¦(æˆ¶/å­£)'] > 0]
        if not valid_speed_data.empty:
            total_units = valid_speed_data['ç¸½æˆ¶æ•¸'].sum()
            if total_units > 0:
                weighted_speed = (valid_speed_data['ç¸½æˆ¶æ•¸'] * valid_speed_data['å­£åº¦å»åŒ–é€Ÿåº¦(æˆ¶/å­£)']).sum()
                metrics['ç¸£å¸‚å¹³å‡å»åŒ–é€Ÿåº¦(æˆ¶/å­£)'] = weighted_speed / total_units
        
        # ç¸£å¸‚å»åŒ–è¡¨ç¾åˆ†ç´š
        avg_speed = metrics['ç¸£å¸‚å¹³å‡å»åŒ–é€Ÿåº¦(æˆ¶/å­£)']
        avg_absorption = city_data['æ·¨å»åŒ–ç‡(%)'].mean()
        completion_rate = len(city_data[city_data['æ·¨å»åŒ–ç‡(%)'] >= 100]) / len(city_data) * 100 if len(city_data) > 0 else 0
        
        # ç¶œåˆè©•åˆ†
        performance_score = 0
        
        # å»åŒ–é€Ÿåº¦è©•åˆ† (0-40åˆ†)
        if avg_speed >= 3:
            performance_score += 40
        elif avg_speed >= 2:
            performance_score += 30
        elif avg_speed >= 1:
            performance_score += 20
        else:
            performance_score += 10
        
        # å»åŒ–ç‡è©•åˆ† (0-30åˆ†)
        if avg_absorption >= 60:
            performance_score += 30
        elif avg_absorption >= 45:
            performance_score += 25
        elif avg_absorption >= 30:
            performance_score += 15
        else:
            performance_score += 5
        
        # å®Œå”®è¡¨ç¾è©•åˆ† (0-30åˆ†)
        if completion_rate >= 20:
            performance_score += 30
        elif completion_rate >= 10:
            performance_score += 20
        elif completion_rate >= 5:
            performance_score += 10
        
        # åˆ†ç´šåˆ¤æ–·
        if performance_score >= 80:
            metrics['ç¸£å¸‚å»åŒ–è¡¨ç¾åˆ†ç´š'] = "ğŸ† å„ªç§€è¡¨ç¾"
        elif performance_score >= 65:
            metrics['ç¸£å¸‚å»åŒ–è¡¨ç¾åˆ†ç´š'] = "ğŸ¥‡ è‰¯å¥½è¡¨ç¾"
        elif performance_score >= 45:
            metrics['ç¸£å¸‚å»åŒ–è¡¨ç¾åˆ†ç´š'] = "ğŸ¥ˆ æ™®é€šè¡¨ç¾"
        else:
            metrics['ç¸£å¸‚å»åŒ–è¡¨ç¾åˆ†ç´š'] = "ğŸ¥‰ å¾…æ”¹å–„è¡¨ç¾"
        
    except Exception as e:
        print(f"âŒ ç¸£å¸‚è¡¨ç¾æŒ‡æ¨™è¨ˆç®—éŒ¯èª¤: {e}")
    
    return metrics

def calculate_city_market_metrics(city_data, district_data, prev_season_data=None):
    """
    è¨ˆç®—ç¸£å¸‚å¸‚å ´æŒ‡æ¨™
    
    Args:
        city_data: è©²ç¸£å¸‚çš„ç¤¾å€ç´šè³‡æ–™
        district_data: è©²ç¸£å¸‚çš„è¡Œæ”¿å€ç´šè³‡æ–™
        prev_season_data: ä¸Šä¸€å­£çš„è³‡æ–™ï¼ˆç”¨æ–¼è¨ˆç®—åƒ¹æ ¼è®ŠåŒ–ï¼‰
        
    Returns:
        dict: å¸‚å ´æŒ‡æ¨™
    """
    metrics = {
        'ç¸£å¸‚åŠ æ¬Šå¹³å‡å–®åƒ¹(è¬/åª)': 0.0,
        'åƒ¹æ ¼æ¼²è·Œå¹…(%)': 0.0,
        'ä¸»è¦ç†±é»è¡Œæ”¿å€': '',
        'é«˜é¢¨éšªè¡Œæ”¿å€æ•¸': 0,
        'ç¸£å¸‚é¢¨éšªç­‰ç´š': 'ğŸŸ¢ ä½é¢¨éšª'
    }
    
    try:
        # ç¸£å¸‚åŠ æ¬Šå¹³å‡å–®åƒ¹ï¼ˆæˆäº¤ç­†æ•¸åŠ æ¬Šï¼‰
        valid_price_data = city_data[
            (city_data['å¹³å‡äº¤æ˜“å–®åƒ¹(è¬/åª)'] > 0) &
            (city_data['è©²å­£æˆäº¤ç­†æ•¸'] > 0)
        ]
        
        if not valid_price_data.empty:
            total_transactions = valid_price_data['è©²å­£æˆäº¤ç­†æ•¸'].sum()
            if total_transactions > 0:
                weighted_price = (valid_price_data['è©²å­£æˆäº¤ç­†æ•¸'] * valid_price_data['å¹³å‡äº¤æ˜“å–®åƒ¹(è¬/åª)']).sum()
                metrics['ç¸£å¸‚åŠ æ¬Šå¹³å‡å–®åƒ¹(è¬/åª)'] = weighted_price / total_transactions
        
        # åƒ¹æ ¼æ¼²è·Œå¹…ï¼ˆæš«æ™‚è¨­ç‚º0ï¼Œéœ€è¦è·¨æœŸè³‡æ–™ï¼‰
        metrics['åƒ¹æ ¼æ¼²è·Œå¹…(%)'] = 0.0
        
        # ä¸»è¦ç†±é»è¡Œæ”¿å€ï¼ˆåŸºæ–¼å»åŒ–æ•ˆç‡æ’åï¼‰
        if not district_data.empty:
            # æ‰¾å‡ºæ•ˆç‡æ’åå‰3çš„è¡Œæ”¿å€
            top_districts = district_data[
                district_data['å€åŸŸå»åŒ–æ•ˆç‡æ’å'].str.contains('ç¬¬[123]å', na=False)
            ].sort_values('æ•´é«”æ·¨å»åŒ–ç‡(%)', ascending=False)
            
            if not top_districts.empty:
                hot_districts = top_districts['è¡Œæ”¿å€'].head(3).tolist()
                metrics['ä¸»è¦ç†±é»è¡Œæ”¿å€'] = 'ã€'.join(hot_districts)
        
        # é«˜é¢¨éšªè¡Œæ”¿å€æ•¸
        if not district_data.empty:
            high_risk_districts = district_data[
                district_data['é¢¨éšªç­‰ç´š'].str.contains('ğŸ”´', na=False)
            ]
            metrics['é«˜é¢¨éšªè¡Œæ”¿å€æ•¸'] = len(high_risk_districts)
        
        # ç¸£å¸‚é¢¨éšªç­‰ç´š
        total_districts = len(district_data) if not district_data.empty else 1
        high_risk_ratio = metrics['é«˜é¢¨éšªè¡Œæ”¿å€æ•¸'] / total_districts * 100
        
        overall_cancellation_rate = city_data['ç´¯ç©è§£ç´„ç‡(%)'].mean()
        stagnant_ratio = len(city_data[
            (city_data['éŠ·å”®å­£æ•¸'] > 12) & 
            (city_data['æ·¨å»åŒ–ç‡(%)'] < 70)
        ]) / len(city_data) * 100 if len(city_data) > 0 else 0
        
        risk_factors = [
            high_risk_ratio > 25,  # é«˜é¢¨éšªè¡Œæ”¿å€éå¤š
            overall_cancellation_rate > 3,  # å¹³å‡è§£ç´„ç‡é«˜
            stagnant_ratio > 20,  # æ»¯éŠ·å»ºæ¡ˆéå¤š
            metrics['ç¸£å¸‚å¹³å‡å»åŒ–é€Ÿåº¦(æˆ¶/å­£)'] < 1  # æ•´é«”å»åŒ–é€Ÿåº¦æ…¢
        ]
        
        city_risk_score = sum(risk_factors) * 2
        metrics['ç¸£å¸‚é¢¨éšªç­‰ç´š'] = classify_risk_level(city_risk_score)
        
    except Exception as e:
        print(f"âŒ ç¸£å¸‚å¸‚å ´æŒ‡æ¨™è¨ˆç®—éŒ¯èª¤: {e}")
    
    return metrics

print("âœ… ç¸£å¸‚ç´šèšåˆé‚è¼¯è¨­è¨ˆå®Œæˆ")

# %% [markdown]
# ## 6. ç¸£å¸‚ç´š19æ¬„ä½å ±å‘Šå¯¦ä½œ

# %%
# ç¸£å¸‚ç´šå ±å‘Šç”Ÿæˆ
print("ğŸ™ï¸ ç¸£å¸‚ç´š19æ¬„ä½å ±å‘Šç”Ÿæˆ")
print("=" * 50)

def generate_city_level_report(community_data, district_data):
    """
    ç”Ÿæˆç¸£å¸‚ç´šå®Œæ•´å ±å‘Š
    
    Args:
        community_data: ç¤¾å€ç´šå®Œæ•´è³‡æ–™
        district_data: è¡Œæ”¿å€ç´šè³‡æ–™
        
    Returns:
        DataFrame: ç¸£å¸‚ç´šå ±å‘Š
    """
    
    city_reports = []
    
    try:
        # æŒ‰ç¸£å¸‚ã€å¹´å­£åˆ†çµ„
        city_groups = community_data.groupby(['ç¸£å¸‚', 'å¹´å­£'])
        
        print(f"ğŸ”„ è™•ç† {len(city_groups)} å€‹ç¸£å¸‚-å¹´å­£çµ„åˆ...")
        
        for (county, season), group_data in city_groups:
            if group_data.empty:
                continue
            
            # å»ºç«‹åŸºæœ¬è³‡è¨Š
            city_report = {
                'ç¸£å¸‚': county,
                'å¹´å­£': season
            }
            
            # ç²å–å°æ‡‰çš„è¡Œæ”¿å€ç´šè³‡æ–™
            corresponding_district_data = district_data[
                (district_data['ç¸£å¸‚'] == county) & 
                (district_data['å¹´å­£'] == season)
            ]
            
            # è¨ˆç®—å„é¡æŒ‡æ¨™
            project_stats = calculate_city_project_statistics(group_data, corresponding_district_data)
            absorption_metrics = calculate_city_absorption_metrics(group_data)
            cancellation_metrics = calculate_city_cancellation_metrics(group_data)
            performance_metrics = calculate_city_performance_metrics(group_data)
            market_metrics = calculate_city_market_metrics(group_data, corresponding_district_data)
            
            # æ•´åˆæ‰€æœ‰æŒ‡æ¨™
            city_report.update(project_stats)
            city_report.update(absorption_metrics)
            city_report.update(cancellation_metrics)
            city_report.update(performance_metrics)
            city_report.update(market_metrics)
            
            city_reports.append(city_report)
        
        # è½‰æ›ç‚ºDataFrame
        city_df = pd.DataFrame(city_reports)
        
        print(f"âœ… å®Œæˆ {len(city_df)} ç­†ç¸£å¸‚ç´šå ±å‘Šç”Ÿæˆ")
        
        return city_df
    
    except Exception as e:
        print(f"âŒ ç¸£å¸‚ç´šå ±å‘Šç”ŸæˆéŒ¯èª¤: {e}")
        return pd.DataFrame()

# %%
# ç”Ÿæˆç¸£å¸‚ç´šå ±å‘Š
print("ğŸ”„ é–‹å§‹ç”Ÿæˆç¸£å¸‚ç´šå ±å‘Š...")

city_level_report = generate_city_level_report(community_report, district_level_report)

if not city_level_report.empty:
    print(f"âœ… ç¸£å¸‚ç´šå ±å‘Šç”Ÿæˆå®Œæˆ")
    print(f"   ç¸½è¨˜éŒ„æ•¸: {len(city_level_report):,}")
    print(f"   æ¶µè“‹ç¸£å¸‚æ•¸: {city_level_report['ç¸£å¸‚'].nunique()}")
    print(f"   æ¶µè“‹å¹´å­£æ•¸: {city_level_report['å¹´å­£'].nunique()}")
    
    # é©—è­‰19æ¬„ä½çµæ§‹
    expected_columns = []
    for category, fields in CITY_REPORT_SCHEMA.items():
        expected_columns.extend(fields)
    
    actual_columns = list(city_level_report.columns)
    print(f"   å¯¦éš›æ¬„ä½æ•¸: {len(actual_columns)}")
    print(f"   æœŸæœ›æ¬„ä½æ•¸: {len(expected_columns)}")
    
    missing_columns = set(expected_columns) - set(actual_columns)
    if missing_columns:
        print(f"   âš ï¸ ç¼ºå¤±æ¬„ä½: {missing_columns}")
    else:
        print(f"   âœ… æ¬„ä½çµæ§‹å®Œæ•´")

# %%
# ç¸£å¸‚ç´šå ±å‘Šçµ±è¨ˆåˆ†æ
print(f"\nğŸ“Š ç¸£å¸‚ç´šå ±å‘Šçµ±è¨ˆåˆ†æ:")

if not city_level_report.empty:
    # åŸºæœ¬çµ±è¨ˆ
    print(f"åŸºæœ¬çµ±è¨ˆ:")
    print(f"   å¹³å‡æ´»èºè¡Œæ”¿å€æ•¸: {city_level_report['æ´»èºè¡Œæ”¿å€æ•¸'].mean():.1f}")
    print(f"   å¹³å‡ç¸£å¸‚ç¸½å»ºæ¡ˆæ•¸: {city_level_report['ç¸£å¸‚ç¸½å»ºæ¡ˆæ•¸'].mean():.0f}")
    print(f"   å¹³å‡ç¸£å¸‚ç¸½æˆ¶æ•¸: {city_level_report['ç¸£å¸‚ç¸½æˆ¶æ•¸'].mean():.0f}")
    print(f"   å¹³å‡ç¸£å¸‚åŠ æ¬Šå»åŒ–ç‡: {city_level_report['ç¸£å¸‚åŠ æ¬Šå»åŒ–ç‡(%)'].mean():.1f}%")
    print(f"   å¹³å‡ç¸£å¸‚è§£ç´„ç‡: {city_level_report['ç¸£å¸‚è§£ç´„ç‡(%)'].mean():.2f}%")
    
    # è¡¨ç¾åˆ†ç´šåˆ†å¸ƒ
    performance_dist = city_level_report['ç¸£å¸‚å»åŒ–è¡¨ç¾åˆ†ç´š'].value_counts()
    print(f"\nå»åŒ–è¡¨ç¾åˆ†ç´šåˆ†å¸ƒ:")
    for performance, count in performance_dist.items():
        percentage = count / len(city_level_report) * 100
        print(f"   {performance}: {count:,} å€‹ ({percentage:.1f}%)")
    
    # é¢¨éšªç­‰ç´šåˆ†å¸ƒ
    risk_dist = city_level_report['ç¸£å¸‚é¢¨éšªç­‰ç´š'].value_counts()
    print(f"\nç¸£å¸‚é¢¨éšªç­‰ç´šåˆ†å¸ƒ:")
    for risk, count in risk_dist.items():
        percentage = count / len(city_level_report) * 100
        print(f"   {risk}: {count:,} å€‹ ({percentage:.1f}%)")
    
    # è§£ç´„é¢¨éšªåˆ†å¸ƒ
    cancellation_risk_dist = city_level_report['ç¸£å¸‚è§£ç´„é¢¨éšªç­‰ç´š'].value_counts()
    print(f"\nè§£ç´„é¢¨éšªåˆ†å¸ƒ:")
    for risk, count in cancellation_risk_dist.items():
        percentage = count / len(city_level_report) * 100
        print(f"   {risk}: {count:,} å€‹ ({percentage:.1f}%)")
    
    # é•·æœŸæ»¯éŠ·åˆ†æ
    high_stagnant = len(city_level_report[city_level_report['é•·æœŸæ»¯éŠ·å»ºæ¡ˆå æ¯”(%)'] > 25])
    print(f"\né•·æœŸæ»¯éŠ·åˆ†æ:")
    print(f"   åš´é‡æ»¯éŠ·ç¸£å¸‚: {high_stagnant:,} å€‹ ({high_stagnant/len(city_level_report)*100:.1f}%)")
    print(f"   å¹³å‡æ»¯éŠ·å æ¯”: {city_level_report['é•·æœŸæ»¯éŠ·å»ºæ¡ˆå æ¯”(%)'].mean():.1f}%")
    
    # é«˜é¢¨éšªè¡Œæ”¿å€åˆ†æ
    total_high_risk_districts = city_level_report['é«˜é¢¨éšªè¡Œæ”¿å€æ•¸'].sum()
    print(f"\né«˜é¢¨éšªè¡Œæ”¿å€åˆ†æ:")
    print(f"   å…¨éƒ¨é«˜é¢¨éšªè¡Œæ”¿å€: {total_high_risk_districts:,} å€‹")
    print(f"   å¹³å‡æ¯ç¸£å¸‚é«˜é¢¨éšªè¡Œæ”¿å€: {city_level_report['é«˜é¢¨éšªè¡Œæ”¿å€æ•¸'].mean():.1f} å€‹")

# %% [markdown]
# ## 7. æ•ˆç‡æ’åèˆ‡è¡¨ç¾åˆ†ç´šç®—æ³•

# %%
# æ•ˆç‡æ’åèˆ‡è¡¨ç¾åˆ†ç´šç®—æ³•
print("ğŸ† æ•ˆç‡æ’åèˆ‡è¡¨ç¾åˆ†ç´šç®—æ³•")
print("=" * 50)

def comprehensive_efficiency_ranking(district_df, city_df):
    """
    ç¶œåˆæ•ˆç‡æ’åç®—æ³•
    
    Args:
        district_df: è¡Œæ”¿å€ç´šè³‡æ–™
        city_df: ç¸£å¸‚ç´šè³‡æ–™
        
    Returns:
        tuple: (æ›´æ–°å¾Œçš„è¡Œæ”¿å€è³‡æ–™, æ›´æ–°å¾Œçš„ç¸£å¸‚è³‡æ–™)
    """
    
    try:
        # 1. è¡Œæ”¿å€ç´šå…¨åœ‹æ’å
        print("ğŸ”„ è¨ˆç®—è¡Œæ”¿å€ç´šå…¨åœ‹æ’å...")
        
        # æŒ‰å¹´å­£åˆ†çµ„è¨ˆç®—å…¨åœ‹æ’å
        for season in district_df['å¹´å­£'].unique():
            season_data = district_df[district_df['å¹´å­£'] == season].copy()
            
            if len(season_data) <= 1:
                continue
            
            # è¨ˆç®—ç¶œåˆæ•ˆç‡åˆ†æ•¸
            efficiency_scores = []
            
            for idx, row in season_data.iterrows():
                score = 0
                
                # å»åŒ–ç‡æ¬Šé‡35%
                absorption_rate = row.get('æ•´é«”æ·¨å»åŒ–ç‡(%)', 0)
                if absorption_rate >= 80:
                    score += 35
                elif absorption_rate >= 60:
                    score += 28
                elif absorption_rate >= 40:
                    score += 21
                elif absorption_rate >= 20:
                    score += 14
                else:
                    score += 7
                
                # å»åŒ–é€Ÿåº¦æ¬Šé‡30%
                absorption_speed = row.get('å€åŸŸå¹³å‡å»åŒ–é€Ÿåº¦(æˆ¶/å­£)', 0)
                if absorption_speed >= 4:
                    score += 30
                elif absorption_speed >= 3:
                    score += 24
                elif absorption_speed >= 2:
                    score += 18
                elif absorption_speed >= 1:
                    score += 12
                else:
                    score += 6
                
                # æ»¯éŠ·å½±éŸ¿æ¬Šé‡20%ï¼ˆè² é¢åˆ†æ•¸ï¼‰
                stagnant_impact = row.get('é•·æœŸæ»¯éŠ·å½±éŸ¿åº¦(%)', 0)
                if stagnant_impact < 5:
                    score += 20
                elif stagnant_impact < 15:
                    score += 16
                elif stagnant_impact < 30:
                    score += 12
                elif stagnant_impact < 50:
                    score += 8
                else:
                    score += 4
                
                # è§£ç´„é¢¨éšªæ¬Šé‡15%
                if 'ğŸŸ¢' in str(row.get('å€åŸŸè§£ç´„é¢¨éšªç­‰ç´š', '')):
                    score += 15
                elif 'ğŸŸ¡' in str(row.get('å€åŸŸè§£ç´„é¢¨éšªç­‰ç´š', '')):
                    score += 10
                else:
                    score += 5
                
                efficiency_scores.append((idx, score))
            
            # æ’åºä¸¦åˆ†é…æ’å
            efficiency_scores.sort(key=lambda x: x[1], reverse=True)
            
            for rank, (idx, score) in enumerate(efficiency_scores, 1):
                total_districts = len(efficiency_scores)
                
                if rank <= max(1, total_districts * 0.1):  # å‰10%
                    rank_category = "ğŸ† å„ªç§€ (å‰10%)"
                elif rank <= max(1, total_districts * 0.3):  # å‰30%
                    rank_category = "ğŸ¥‡ è‰¯å¥½ (å‰30%)"
                elif rank <= max(1, total_districts * 0.7):  # å‰70%
                    rank_category = "ğŸ¥ˆ æ™®é€š"
                else:  # å¾Œ30%
                    rank_category = "ğŸ¥‰ å¾…æ”¹å–„"
                
                district_df.loc[idx, 'å…¨åœ‹æ•ˆç‡æ’å'] = f"ç¬¬{rank}å {rank_category}"
                district_df.loc[idx, 'æ•ˆç‡åˆ†æ•¸'] = score
        
        # 2. ç¸£å¸‚ç´šè¡¨ç¾åˆ†ç´šå„ªåŒ–
        print("ğŸ”„ å„ªåŒ–ç¸£å¸‚ç´šè¡¨ç¾åˆ†ç´š...")
        
        for idx, row in city_df.iterrows():
            # é‡æ–°è¨ˆç®—æ›´ç²¾ç¢ºçš„è¡¨ç¾åˆ†æ•¸
            performance_score = 0
            
            # å»åŒ–é€Ÿåº¦æ¬Šé‡30%
            avg_speed = row.get('ç¸£å¸‚å¹³å‡å»åŒ–é€Ÿåº¦(æˆ¶/å­£)', 0)
            if avg_speed >= 3.5:
                performance_score += 30
            elif avg_speed >= 2.5:
                performance_score += 24
            elif avg_speed >= 1.5:
                performance_score += 18
            elif avg_speed >= 0.5:
                performance_score += 12
            else:
                performance_score += 6
            
            # å»åŒ–ç‡æ¬Šé‡25%
            avg_absorption = row.get('ç¸£å¸‚åŠ æ¬Šå»åŒ–ç‡(%)', 0)
            if avg_absorption >= 70:
                performance_score += 25
            elif avg_absorption >= 50:
                performance_score += 20
            elif avg_absorption >= 30:
                performance_score += 15
            elif avg_absorption >= 10:
                performance_score += 10
            else:
                performance_score += 5
            
            # å®Œå”®è¡¨ç¾æ¬Šé‡20%
            total_projects = row.get('ç¸£å¸‚ç¸½å»ºæ¡ˆæ•¸', 1)
            completed_projects = row.get('å®Œå”®å»ºæ¡ˆæ•¸é‡', 0)
            completion_rate = completed_projects / total_projects * 100 if total_projects > 0 else 0
            
            if completion_rate >= 25:
                performance_score += 20
            elif completion_rate >= 15:
                performance_score += 16
            elif completion_rate >= 10:
                performance_score += 12
            elif completion_rate >= 5:
                performance_score += 8
            else:
                performance_score += 4
            
            # é¢¨éšªæ§åˆ¶æ¬Šé‡15%
            high_risk_districts = row.get('é«˜é¢¨éšªè¡Œæ”¿å€æ•¸', 0)
            active_districts = row.get('æ´»èºè¡Œæ”¿å€æ•¸', 1)
            risk_ratio = high_risk_districts / active_districts * 100 if active_districts > 0 else 0
            
            if risk_ratio < 10:
                performance_score += 15
            elif risk_ratio < 25:
                performance_score += 12
            elif risk_ratio < 40:
                performance_score += 9
            elif risk_ratio < 60:
                performance_score += 6
            else:
                performance_score += 3
            
            # æ»¯éŠ·æ§åˆ¶æ¬Šé‡10%
            stagnant_ratio = row.get('é•·æœŸæ»¯éŠ·å»ºæ¡ˆå æ¯”(%)', 0)
            if stagnant_ratio < 10:
                performance_score += 10
            elif stagnant_ratio < 20:
                performance_score += 8
            elif stagnant_ratio < 35:
                performance_score += 6
            elif stagnant_ratio < 50:
                performance_score += 4
            else:
                performance_score += 2
            
            # æ›´æ–°åˆ†ç´š
            if performance_score >= 85:
                city_df.loc[idx, 'ç¸£å¸‚å»åŒ–è¡¨ç¾åˆ†ç´š'] = "ğŸ† å“è¶Šè¡¨ç¾"
            elif performance_score >= 75:
                city_df.loc[idx, 'ç¸£å¸‚å»åŒ–è¡¨ç¾åˆ†ç´š'] = "ğŸ¥‡ å„ªç§€è¡¨ç¾"
            elif performance_score >= 60:
                city_df.loc[idx, 'ç¸£å¸‚å»åŒ–è¡¨ç¾åˆ†ç´š'] = "ğŸ¥ˆ è‰¯å¥½è¡¨ç¾"
            elif performance_score >= 45:
                city_df.loc[idx, 'ç¸£å¸‚å»åŒ–è¡¨ç¾åˆ†ç´š'] = "ğŸ¥‰ æ™®é€šè¡¨ç¾"
            else:
                city_df.loc[idx, 'ç¸£å¸‚å»åŒ–è¡¨ç¾åˆ†ç´š'] = "âš ï¸ å¾…æ”¹å–„è¡¨ç¾"
            
            city_df.loc[idx, 'è¡¨ç¾åˆ†æ•¸'] = performance_score
        
        print("âœ… æ•ˆç‡æ’åèˆ‡è¡¨ç¾åˆ†ç´šè¨ˆç®—å®Œæˆ")
        
        return district_df, city_df
    
    except Exception as e:
        print(f"âŒ æ•ˆç‡æ’åè¨ˆç®—éŒ¯èª¤: {e}")
        return district_df, city_df

# %%
# åŸ·è¡Œæ•ˆç‡æ’åèˆ‡è¡¨ç¾åˆ†ç´š
print("ğŸ”„ åŸ·è¡Œç¶œåˆæ•ˆç‡æ’åèˆ‡è¡¨ç¾åˆ†ç´š...")

enhanced_district_report, enhanced_city_report = comprehensive_efficiency_ranking(
    district_level_report.copy(), 
    city_level_report.copy()
)

print(f"âœ… æ•ˆç‡æ’åèˆ‡è¡¨ç¾åˆ†ç´šå®Œæˆ")

# é¡¯ç¤ºæ’åçµæœ
if 'all_name' in enhanced_district_report.columns:
    print(f"\nğŸ† è¡Œæ”¿å€ç´šå…¨åœ‹æ’ååˆ†å¸ƒ:")
    rank_dist = enhanced_district_report['å…¨åœ‹æ•ˆç‡æ’å'].value_counts()
    for rank_category, count in rank_dist.head(10).items():
        print(f"   {rank_category}: {count} å€‹")

if 'è¡¨ç¾åˆ†æ•¸' in enhanced_city_report.columns:
    print(f"\nğŸ† ç¸£å¸‚ç´šè¡¨ç¾åˆ†ç´šåˆ†å¸ƒ:")
    updated_performance_dist = enhanced_city_report['ç¸£å¸‚å»åŒ–è¡¨ç¾åˆ†ç´š'].value_counts()
    for performance, count in updated_performance_dist.items():
        percentage = count / len(enhanced_city_report) * 100
        print(f"   {performance}: {count} å€‹ ({percentage:.1f}%)")

# %% [markdown]
# ## 8. ç†±é»å€åŸŸè­˜åˆ¥èˆ‡é¢¨éšªèšåˆ

# %%
# ç†±é»å€åŸŸè­˜åˆ¥èˆ‡é¢¨éšªèšåˆç®—æ³•
print("ğŸ”¥ ç†±é»å€åŸŸè­˜åˆ¥èˆ‡é¢¨éšªèšåˆç®—æ³•")
print("=" * 50)

def identify_hotspot_and_risk_aggregation(district_df, city_df):
    """
    è­˜åˆ¥ç†±é»å€åŸŸä¸¦é€²è¡Œé¢¨éšªèšåˆ
    
    Args:
        district_df: è¡Œæ”¿å€ç´šè³‡æ–™
        city_df: ç¸£å¸‚ç´šè³‡æ–™
        
    Returns:
        tuple: (æ›´æ–°å¾Œçš„è³‡æ–™, ç†±é»åˆ†æçµæœ)
    """
    
    hotspot_analysis = {
        'national_hotspots': [],
        'regional_hotspots': {},
        'risk_clusters': [],
        'trend_analysis': {}
    }
    
    try:
        # 1. å…¨åœ‹ç†±é»è¡Œæ”¿å€è­˜åˆ¥
        print("ğŸ”„ è­˜åˆ¥å…¨åœ‹ç†±é»è¡Œæ”¿å€...")
        
        for season in district_df['å¹´å­£'].unique():
            season_data = district_df[district_df['å¹´å­£'] == season].copy()
            
            # ç†±é»æ¨™æº–ï¼šå»åŒ–ç‡>60% AND å»åŒ–é€Ÿåº¦>2 AND æ»¯éŠ·å½±éŸ¿<15%
            hotspots = season_data[
                (season_data['æ•´é«”æ·¨å»åŒ–ç‡(%)'] > 60) &
                (season_data['å€åŸŸå¹³å‡å»åŒ–é€Ÿåº¦(æˆ¶/å­£)'] > 2) &
                (season_data['é•·æœŸæ»¯éŠ·å½±éŸ¿åº¦(%)'] < 15)
            ].copy()
            
            # æŒ‰ç¶œåˆè¡¨ç¾æ’åº
            if not hotspots.empty:
                hotspots['hotspot_score'] = (
                    hotspots['æ•´é«”æ·¨å»åŒ–ç‡(%)'] * 0.4 +
                    hotspots['å€åŸŸå¹³å‡å»åŒ–é€Ÿåº¦(æˆ¶/å­£)'] * 20 * 0.4 +  # è½‰æ›ç‚ºç™¾åˆ†æ¯”
                    (100 - hotspots['é•·æœŸæ»¯éŠ·å½±éŸ¿åº¦(%)']) * 0.2
                )
                
                hotspots = hotspots.sort_values('hotspot_score', ascending=False)
                
                # æ¨™è¨˜å‰20%ç‚ºç†±é»
                hotspot_count = max(1, int(len(hotspots) * 0.2))
                top_hotspots = hotspots.head(hotspot_count)
                
                for idx, row in top_hotspots.iterrows():
                    district_df.loc[idx, 'æ˜¯å¦ç†±é»å€åŸŸ'] = 'ğŸ”¥ ç†±é»å€åŸŸ'
                    hotspot_analysis['national_hotspots'].append({
                        'season': season,
                        'county': row['ç¸£å¸‚'],
                        'district': row['è¡Œæ”¿å€'],
                        'score': row['hotspot_score']
                    })
        
        # 2. å„ç¸£å¸‚å…§éƒ¨ç†±é»è­˜åˆ¥
        print("ğŸ”„ è­˜åˆ¥å„ç¸£å¸‚å…§éƒ¨ç†±é»...")
        
        for (county, season), group in district_df.groupby(['ç¸£å¸‚', 'å¹´å­£']):
            if len(group) <= 1:
                continue
            
            # ç¸£å¸‚å…§æ’åå‰2æˆ–å‰30%çš„è¡Œæ”¿å€
            group_sorted = group.sort_values('æ•´é«”æ·¨å»åŒ–ç‡(%)', ascending=False)
            hotspot_count = max(1, min(2, int(len(group) * 0.3)))
            
            county_hotspots = group_sorted.head(hotspot_count)
            hotspot_districts = county_hotspots['è¡Œæ”¿å€'].tolist()
            
            # æ›´æ–°ç¸£å¸‚å ±å‘Šçš„ç†±é»è¡Œæ”¿å€
            city_mask = (city_df['ç¸£å¸‚'] == county) & (city_df['å¹´å­£'] == season)
            if city_mask.any():
                city_df.loc[city_mask, 'ä¸»è¦ç†±é»è¡Œæ”¿å€'] = 'ã€'.join(hotspot_districts)
            
            # è¨˜éŒ„åˆ°åˆ†æçµæœ
            if county not in hotspot_analysis['regional_hotspots']:
                hotspot_analysis['regional_hotspots'][county] = {}
            hotspot_analysis['regional_hotspots'][county][season] = hotspot_districts
        
        # 3. é¢¨éšªèšé›†å€è­˜åˆ¥
        print("ğŸ”„ è­˜åˆ¥é¢¨éšªèšé›†å€...")
        
        for season in district_df['å¹´å­£'].unique():
            season_data = district_df[district_df['å¹´å­£'] == season].copy()
            
            # é«˜é¢¨éšªå€åŸŸï¼šè§£ç´„ç‡>3% OR æ»¯éŠ·å½±éŸ¿>30% OR å»åŒ–ç‡<30%
            high_risk_areas = season_data[
                (season_data['å€åŸŸè§£ç´„ç‡(%)'] > 3) |
                (season_data['é•·æœŸæ»¯éŠ·å½±éŸ¿åº¦(%)'] > 30) |
                (season_data['æ•´é«”æ·¨å»åŒ–ç‡(%)'] < 30)
            ]
            
            # æŒ‰ç¸£å¸‚åˆ†ç¾¤åˆ†æ
            risk_by_county = high_risk_areas.groupby('ç¸£å¸‚').size()
            
            for county, risk_count in risk_by_county.items():
                total_districts = len(season_data[season_data['ç¸£å¸‚'] == county])
                risk_ratio = risk_count / total_districts * 100 if total_districts > 0 else 0
                
                if risk_ratio > 50:  # è¶…é50%è¡Œæ”¿å€ç‚ºé«˜é¢¨éšª
                    hotspot_analysis['risk_clusters'].append({
                        'season': season,
                        'county': county,
                        'risk_district_count': risk_count,
                        'total_districts': total_districts,
                        'risk_ratio': risk_ratio
                    })
        
        # 4. è¶¨å‹¢åˆ†æ
        print("ğŸ”„ é€²è¡Œè¶¨å‹¢åˆ†æ...")
        
        # åˆ†æå»åŒ–è¶¨å‹¢è®ŠåŒ–
        trend_data = district_df.groupby(['ç¸£å¸‚', 'è¡Œæ”¿å€']).agg({
            'æ•´é«”æ·¨å»åŒ–ç‡(%)': ['first', 'last', 'mean'],
            'å€åŸŸå¹³å‡å»åŒ–é€Ÿåº¦(æˆ¶/å­£)': ['first', 'last', 'mean'],
            'å¹´å­£': 'count'
        }).reset_index()
        
        # æ‰å¹³åŒ–æ¬„ä½åç¨±
        trend_data.columns = ['ç¸£å¸‚', 'è¡Œæ”¿å€', 'é¦–å­£å»åŒ–ç‡', 'æœ«å­£å»åŒ–ç‡', 'å¹³å‡å»åŒ–ç‡', 
                             'é¦–å­£é€Ÿåº¦', 'æœ«å­£é€Ÿåº¦', 'å¹³å‡é€Ÿåº¦', 'å­£æ•¸']
        
        # åªåˆ†ææœ‰å¤šå­£è³‡æ–™çš„å€åŸŸ
        multi_season_trend = trend_data[trend_data['å­£æ•¸'] > 1].copy()
        
        if not multi_season_trend.empty:
            # è¨ˆç®—è¶¨å‹¢æŒ‡æ¨™
            multi_season_trend['å»åŒ–ç‡è¶¨å‹¢'] = multi_season_trend['æœ«å­£å»åŒ–ç‡'] - multi_season_trend['é¦–å­£å»åŒ–ç‡']
            multi_season_trend['é€Ÿåº¦è¶¨å‹¢'] = multi_season_trend['æœ«å­£é€Ÿåº¦'] - multi_season_trend['é¦–å­£é€Ÿåº¦']
            
            # åˆ†é¡è¶¨å‹¢
            improving_areas = multi_season_trend[
                (multi_season_trend['å»åŒ–ç‡è¶¨å‹¢'] > 10) | 
                (multi_season_trend['é€Ÿåº¦è¶¨å‹¢'] > 0.5)
            ]
            
            declining_areas = multi_season_trend[
                (multi_season_trend['å»åŒ–ç‡è¶¨å‹¢'] < -10) | 
                (multi_season_trend['é€Ÿåº¦è¶¨å‹¢'] < -0.5)
            ]
            
            hotspot_analysis['trend_analysis'] = {
                'improving_count': len(improving_areas),
                'declining_count': len(declining_areas),
                'stable_count': len(multi_season_trend) - len(improving_areas) - len(declining_areas),
                'top_improving': improving_areas.nlargest(5, 'å»åŒ–ç‡è¶¨å‹¢')[['ç¸£å¸‚', 'è¡Œæ”¿å€', 'å»åŒ–ç‡è¶¨å‹¢']].to_dict('records'),
                'top_declining': declining_areas.nsmallest(5, 'å»åŒ–ç‡è¶¨å‹¢')[['ç¸£å¸‚', 'è¡Œæ”¿å€', 'å»åŒ–ç‡è¶¨å‹¢']].to_dict('records')
            }
        
        print("âœ… ç†±é»å€åŸŸè­˜åˆ¥èˆ‡é¢¨éšªèšåˆå®Œæˆ")
        
        return district_df, city_df, hotspot_analysis
    
    except Exception as e:
        print(f"âŒ ç†±é»è­˜åˆ¥éŒ¯èª¤: {e}")
        return district_df, city_df, hotspot_analysis

# %%
# åŸ·è¡Œç†±é»å€åŸŸè­˜åˆ¥èˆ‡é¢¨éšªèšåˆ
print("ğŸ”„ åŸ·è¡Œç†±é»å€åŸŸè­˜åˆ¥èˆ‡é¢¨éšªèšåˆ...")

enhanced_district_report, enhanced_city_report, hotspot_analysis = identify_hotspot_and_risk_aggregation(
    enhanced_district_report.copy(), 
    enhanced_city_report.copy()
)

print(f"âœ… ç†±é»å€åŸŸè­˜åˆ¥èˆ‡é¢¨éšªèšåˆå®Œæˆ")

# %%
# ç†±é»åˆ†æçµæœå±•ç¤º
print(f"\nğŸ”¥ ç†±é»åˆ†æçµæœ:")

# å…¨åœ‹ç†±é»çµ±è¨ˆ
national_hotspots_count = len(hotspot_analysis['national_hotspots'])
print(f"å…¨åœ‹ç†±é»å€åŸŸ: {national_hotspots_count} å€‹")

if national_hotspots_count > 0:
    # æŒ‰å­£åº¦çµ±è¨ˆ
    hotspot_by_season = {}
    for hotspot in hotspot_analysis['national_hotspots']:
        season = hotspot['season']
        if season not in hotspot_by_season:
            hotspot_by_season[season] = []
        hotspot_by_season[season].append(f"{hotspot['county']}{hotspot['district']}")
    
    print(f"å„å¹´å­£ç†±é»åˆ†å¸ƒ:")
    for season, areas in hotspot_by_season.items():
        print(f"   {season}: {len(areas)} å€‹ - {', '.join(areas[:3])}{'...' if len(areas) > 3 else ''}")

# é¢¨éšªèšé›†å€çµ±è¨ˆ
risk_clusters_count = len(hotspot_analysis['risk_clusters'])
print(f"\né¢¨éšªèšé›†å€: {risk_clusters_count} å€‹")

if risk_clusters_count > 0:
    print(f"é¢¨éšªèšé›†è©³æƒ…:")
    for cluster in hotspot_analysis['risk_clusters'][:5]:  # é¡¯ç¤ºå‰5å€‹
        print(f"   {cluster['season']} {cluster['county']}: {cluster['risk_district_count']}/{cluster['total_districts']} å€‹é«˜é¢¨éšªè¡Œæ”¿å€ ({cluster['risk_ratio']:.1f}%)")

# è¶¨å‹¢åˆ†æçµæœ
if 'trend_analysis' in hotspot_analysis and hotspot_analysis['trend_analysis']:
    trend = hotspot_analysis['trend_analysis']
    print(f"\nğŸ“ˆ è¶¨å‹¢åˆ†æ:")
    print(f"   æ”¹å–„ä¸­å€åŸŸ: {trend.get('improving_count', 0)} å€‹")
    print(f"   æƒ¡åŒ–ä¸­å€åŸŸ: {trend.get('declining_count', 0)} å€‹")
    print(f"   ç©©å®šå€åŸŸ: {trend.get('stable_count', 0)} å€‹")
    
    if 'top_improving' in trend and trend['top_improving']:
        print(f"   è¡¨ç¾æœ€ä½³æ”¹å–„å€åŸŸ:")
        for area in trend['top_improving'][:3]:
            print(f"     {area['ç¸£å¸‚']}{area['è¡Œæ”¿å€']}: +{area['å»åŒ–ç‡è¶¨å‹¢']:.1f}%")

# %% [markdown]
# ## 9. ä¸‰å±¤ç´šè³‡æ–™ä¸€è‡´æ€§æª¢æŸ¥

# %%
# ä¸‰å±¤ç´šè³‡æ–™ä¸€è‡´æ€§æª¢æŸ¥
print("ğŸ” ä¸‰å±¤ç´šè³‡æ–™ä¸€è‡´æ€§æª¢æŸ¥")
print("=" * 50)

def validate_three_level_consistency(community_df, district_df, city_df):
    """
    é©—è­‰ä¸‰å±¤ç´šè³‡æ–™ä¸€è‡´æ€§
    
    Args:
        community_df: ç¤¾å€ç´šè³‡æ–™
        district_df: è¡Œæ”¿å€ç´šè³‡æ–™
        city_df: ç¸£å¸‚ç´šè³‡æ–™
        
    Returns:
        dict: ä¸€è‡´æ€§æª¢æŸ¥çµæœ
    """
    
    consistency_report = {
        'overall_status': True,
        'community_to_district': {},
        'district_to_city': {},
        'aggregation_accuracy': {},
        'data_coverage': {},
        'recommendations': []
    }
    
    try:
        print("ğŸ”„ åŸ·è¡Œç¤¾å€ç´šâ†’è¡Œæ”¿å€ç´šä¸€è‡´æ€§æª¢æŸ¥...")
        
        # 1. ç¤¾å€ç´šâ†’è¡Œæ”¿å€ç´šä¸€è‡´æ€§æª¢æŸ¥
        community_district_issues = []
        
        for (county, district, season), group in community_df.groupby(['ç¸£å¸‚', 'è¡Œæ”¿å€', 'å¹´å­£']):
            # æ‰¾åˆ°å°æ‡‰çš„è¡Œæ”¿å€ç´šè¨˜éŒ„
            district_record = district_df[
                (district_df['ç¸£å¸‚'] == county) & 
                (district_df['è¡Œæ”¿å€'] == district) & 
                (district_df['å¹´å­£'] == season)
            ]
            
            if district_record.empty:
                community_district_issues.append(f"ç¼ºå¤±è¡Œæ”¿å€è¨˜éŒ„: {county}{district} {season}")
                continue
            
            district_row = district_record.iloc[0]
            
            # æª¢æŸ¥æ´»èºå»ºæ¡ˆæ•¸
            actual_active = len(group[(group['ç´¯ç©æˆäº¤ç­†æ•¸'] > 0) | (group['æ·¨å»åŒ–ç‡(%)'] < 100)])
            expected_active = district_row['æ´»èºå»ºæ¡ˆæ•¸']
            
            if abs(actual_active - expected_active) > 0:
                community_district_issues.append(
                    f"æ´»èºå»ºæ¡ˆæ•¸ä¸ç¬¦: {county}{district} {season} - å¯¦éš›{actual_active}, è¨˜éŒ„{expected_active}"
                )
            
            # æª¢æŸ¥ç¸½æˆ¶æ•¸
            actual_units = group['ç¸½æˆ¶æ•¸'].sum()
            expected_units = district_row['å€åŸŸç¸½æˆ¶æ•¸']
            
            if abs(actual_units - expected_units) > 1:  # å…è¨±1æˆ¶èª¤å·®
                community_district_issues.append(
                    f"ç¸½æˆ¶æ•¸ä¸ç¬¦: {county}{district} {season} - å¯¦éš›{actual_units}, è¨˜éŒ„{expected_units}"
                )
            
            # æª¢æŸ¥è§£ç´„ç­†æ•¸
            actual_cancellations = group['ç´¯ç©è§£ç´„ç­†æ•¸'].sum()
            expected_cancellations = district_row['å€åŸŸç¸½è§£ç´„ç­†æ•¸']
            
            if abs(actual_cancellations - expected_cancellations) > 0:
                community_district_issues.append(
                    f"è§£ç´„ç­†æ•¸ä¸ç¬¦: {county}{district} {season} - å¯¦éš›{actual_cancellations}, è¨˜éŒ„{expected_cancellations}"
                )
        
        consistency_report['community_to_district'] = {
            'total_checks': len(community_df.groupby(['ç¸£å¸‚', 'è¡Œæ”¿å€', 'å¹´å­£'])),
            'issues_found': len(community_district_issues),
            'issues_detail': community_district_issues[:10],  # åªé¡¯ç¤ºå‰10å€‹å•é¡Œ
            'consistency_rate': (1 - len(community_district_issues) / max(1, len(community_df.groupby(['ç¸£å¸‚', 'è¡Œæ”¿å€', 'å¹´å­£'])))) * 100
        }
        
        print("ğŸ”„ åŸ·è¡Œè¡Œæ”¿å€ç´šâ†’ç¸£å¸‚ç´šä¸€è‡´æ€§æª¢æŸ¥...")
        
        # 2. è¡Œæ”¿å€ç´šâ†’ç¸£å¸‚ç´šä¸€è‡´æ€§æª¢æŸ¥
        district_city_issues = []
        
        for (county, season), group in district_df.groupby(['ç¸£å¸‚', 'å¹´å­£']):
            # æ‰¾åˆ°å°æ‡‰çš„ç¸£å¸‚ç´šè¨˜éŒ„
            city_record = city_df[
                (city_df['ç¸£å¸‚'] == county) & 
                (city_df['å¹´å­£'] == season)
            ]
            
            if city_record.empty:
                district_city_issues.append(f"ç¼ºå¤±ç¸£å¸‚è¨˜éŒ„: {county} {season}")
                continue
            
            city_row = city_record.iloc[0]
            
            # æª¢æŸ¥æ´»èºè¡Œæ”¿å€æ•¸
            actual_active_districts = len(group[group['æ´»èºå»ºæ¡ˆæ•¸'] > 0])
            expected_active_districts = city_row['æ´»èºè¡Œæ”¿å€æ•¸']
            
            if abs(actual_active_districts - expected_active_districts) > 0:
                district_city_issues.append(
                    f"æ´»èºè¡Œæ”¿å€æ•¸ä¸ç¬¦: {county} {season} - å¯¦éš›{actual_active_districts}, è¨˜éŒ„{expected_active_districts}"
                )
            
            # æª¢æŸ¥ç¸½æˆ¶æ•¸
            actual_total_units = group['å€åŸŸç¸½æˆ¶æ•¸'].sum()
            expected_total_units = city_row['ç¸£å¸‚ç¸½æˆ¶æ•¸']
            
            if abs(actual_total_units - expected_total_units) > 10:  # å…è¨±10æˆ¶èª¤å·®
                district_city_issues.append(
                    f"ç¸½æˆ¶æ•¸ä¸ç¬¦: {county} {season} - å¯¦éš›{actual_total_units}, è¨˜éŒ„{expected_total_units}"
                )
            
            # æª¢æŸ¥é«˜é¢¨éšªè¡Œæ”¿å€æ•¸
            actual_high_risk = len(group[group['é¢¨éšªç­‰ç´š'].str.contains('ğŸ”´', na=False)])
            expected_high_risk = city_row['é«˜é¢¨éšªè¡Œæ”¿å€æ•¸']
            
            if abs(actual_high_risk - expected_high_risk) > 0:
                district_city_issues.append(
                    f"é«˜é¢¨éšªè¡Œæ”¿å€æ•¸ä¸ç¬¦: {county} {season} - å¯¦éš›{actual_high_risk}, è¨˜éŒ„{expected_high_risk}"
                )
        
        consistency_report['district_to_city'] = {
            'total_checks': len(district_df.groupby(['ç¸£å¸‚', 'å¹´å­£'])),
            'issues_found': len(district_city_issues),
            'issues_detail': district_city_issues[:10],
            'consistency_rate': (1 - len(district_city_issues) / max(1, len(district_df.groupby(['ç¸£å¸‚', 'å¹´å­£'])))) * 100
        }
        
        print("ğŸ”„ åŸ·è¡Œèšåˆæº–ç¢ºæ€§æª¢æŸ¥...")
        
        # 3. èšåˆæº–ç¢ºæ€§æª¢æŸ¥
        aggregation_errors = []
        
        # æª¢æŸ¥åŠ æ¬Šå¹³å‡è¨ˆç®—æº–ç¢ºæ€§
        sample_checks = min(10, len(district_df))
        sample_districts = district_df.sample(n=sample_checks) if len(district_df) >= sample_checks else district_df
        
        for _, district_row in sample_districts.iterrows():
            county = district_row['ç¸£å¸‚']
            district_name = district_row['è¡Œæ”¿å€']
            season = district_row['å¹´å­£']
            
            # å–å¾—å°æ‡‰çš„ç¤¾å€ç´šè³‡æ–™
            community_subset = community_df[
                (community_df['ç¸£å¸‚'] == county) & 
                (community_df['è¡Œæ”¿å€'] == district_name) & 
                (community_df['å¹´å­£'] == season)
            ]
            
            if community_subset.empty:
                continue
            
            # é‡æ–°è¨ˆç®—å»åŒ–ç‡
            total_units = community_subset['ç¸½æˆ¶æ•¸'].sum()
            total_transactions = community_subset['ç´¯ç©æˆäº¤ç­†æ•¸'].sum()
            total_cancellations = community_subset['ç´¯ç©è§£ç´„ç­†æ•¸'].sum()
            
            if total_units > 0:
                calculated_absorption = (total_transactions - total_cancellations) / total_units * 100
                recorded_absorption = district_row['æ•´é«”æ·¨å»åŒ–ç‡(%)']
                
                if abs(calculated_absorption - recorded_absorption) > 1:  # å…è¨±1%èª¤å·®
                    aggregation_errors.append(
                        f"å»åŒ–ç‡è¨ˆç®—èª¤å·®: {county}{district_name} {season} - è¨ˆç®—{calculated_absorption:.1f}%, è¨˜éŒ„{recorded_absorption:.1f}%"
                    )
        
        consistency_report['aggregation_accuracy'] = {
            'sample_size': sample_checks,
            'errors_found': len(aggregation_errors),
            'errors_detail': aggregation_errors,
            'accuracy_rate': (1 - len(aggregation_errors) / max(1, sample_checks)) * 100
        }
        
        print("ğŸ”„ åŸ·è¡Œè³‡æ–™æ¶µè“‹åº¦æª¢æŸ¥...")
        
        # 4. è³‡æ–™æ¶µè“‹åº¦æª¢æŸ¥
        coverage_stats = {
            'community_coverage': {
                'total_projects': len(community_df),
                'with_transactions': len(community_df[community_df['ç´¯ç©æˆäº¤ç­†æ•¸'] > 0]),
                'with_complete_info': len(community_df[
                    (community_df['å‚™æŸ¥ç·¨è™Ÿ'] != '') & 
                    (community_df['ç¸£å¸‚'] != '') & 
                    (community_df['ç¸½æˆ¶æ•¸'] > 0)
                ])
            },
            'district_coverage': {
                'total_districts': len(district_df),
                'with_active_projects': len(district_df[district_df['æ´»èºå»ºæ¡ˆæ•¸'] > 0]),
                'with_complete_metrics': len(district_df[
                    (district_df['å€åŸŸç¸½æˆ¶æ•¸'] > 0) & 
                    (district_df['æ•´é«”æ·¨å»åŒ–ç‡(%)'] >= 0)
                ])
            },
            'city_coverage': {
                'total_cities': len(city_df),
                'with_active_districts': len(city_df[city_df['æ´»èºè¡Œæ”¿å€æ•¸'] > 0]),
                'with_complete_metrics': len(city_df[
                    (city_df['ç¸£å¸‚ç¸½æˆ¶æ•¸'] > 0) & 
                    (city_df['ç¸£å¸‚åŠ æ¬Šå»åŒ–ç‡(%)'] >= 0)
                ])
            }
        }
        
        consistency_report['data_coverage'] = coverage_stats
        
        # 5. ç”Ÿæˆå»ºè­°
        recommendations = []
        
        # åŸºæ–¼ä¸€è‡´æ€§æª¢æŸ¥çµæœç”Ÿæˆå»ºè­°
        cd_rate = consistency_report['community_to_district']['consistency_rate']
        dc_rate = consistency_report['district_to_city']['consistency_rate']
        agg_rate = consistency_report['aggregation_accuracy']['accuracy_rate']
        
        if cd_rate < 95:
            recommendations.append(f"ç¤¾å€ç´šâ†’è¡Œæ”¿å€ç´šä¸€è‡´æ€§è¼ƒä½({cd_rate:.1f}%)ï¼Œéœ€æª¢æŸ¥èšåˆé‚è¼¯")
        
        if dc_rate < 95:
            recommendations.append(f"è¡Œæ”¿å€ç´šâ†’ç¸£å¸‚ç´šä¸€è‡´æ€§è¼ƒä½({dc_rate:.1f}%)ï¼Œéœ€æª¢æŸ¥çµ±è¨ˆé‚è¼¯")
        
        if agg_rate < 90:
            recommendations.append(f"èšåˆè¨ˆç®—æº–ç¢ºæ€§è¼ƒä½({agg_rate:.1f}%)ï¼Œéœ€é‡æ–°æª¢è¦–è¨ˆç®—å…¬å¼")
        
        # æ¶µè“‹åº¦å»ºè­°
        community_complete_rate = coverage_stats['community_coverage']['with_complete_info'] / coverage_stats['community_coverage']['total_projects'] * 100
        if community_complete_rate < 90:
            recommendations.append(f"ç¤¾å€ç´šè³‡æ–™å®Œæ•´åº¦è¼ƒä½({community_complete_rate:.1f}%)ï¼Œå»ºè­°è£œå¼·åŸºç¤è³‡æ–™")
        
        if len(recommendations) == 0:
            recommendations.append("ä¸‰å±¤ç´šè³‡æ–™ä¸€è‡´æ€§è‰¯å¥½ï¼Œèšåˆé‚è¼¯æ­£ç¢º")
        
        consistency_report['recommendations'] = recommendations
        
        # è¨ˆç®—æ•´é«”ä¸€è‡´æ€§åˆ†æ•¸
        overall_score = (cd_rate + dc_rate + agg_rate) / 3
        consistency_report['overall_consistency_score'] = overall_score
        consistency_report['overall_status'] = overall_score >= 90
        
        print("âœ… ä¸‰å±¤ç´šè³‡æ–™ä¸€è‡´æ€§æª¢æŸ¥å®Œæˆ")
        
        return consistency_report
    
    except Exception as e:
        print(f"âŒ ä¸€è‡´æ€§æª¢æŸ¥éŒ¯èª¤: {e}")
        consistency_report['error'] = str(e)
        consistency_report['overall_status'] = False
        return consistency_report

# %%
# åŸ·è¡Œä¸‰å±¤ç´šè³‡æ–™ä¸€è‡´æ€§æª¢æŸ¥
print("ğŸ”„ åŸ·è¡Œä¸‰å±¤ç´šè³‡æ–™ä¸€è‡´æ€§æª¢æŸ¥...")

consistency_result = validate_three_level_consistency(
    community_report, 
    enhanced_district_report, 
    enhanced_city_report
)

print(f"âœ… ä¸€è‡´æ€§æª¢æŸ¥å®Œæˆ")

# %%
# ä¸€è‡´æ€§æª¢æŸ¥çµæœå±•ç¤º
print(f"\nğŸ” ä¸‰å±¤ç´šè³‡æ–™ä¸€è‡´æ€§æª¢æŸ¥çµæœ:")

if consistency_result:
    print(f"æ•´é«”ä¸€è‡´æ€§åˆ†æ•¸: {consistency_result.get('overall_consistency_score', 0):.1f}/100")
    print(f"æ•´é«”ç‹€æ…‹: {'âœ… é€šé' if consistency_result.get('overall_status', False) else 'âŒ éœ€æ”¹å–„'}")
    
    # ç¤¾å€ç´šâ†’è¡Œæ”¿å€ç´š
    cd_check = consistency_result.get('community_to_district', {})
    print(f"\nç¤¾å€ç´šâ†’è¡Œæ”¿å€ç´šä¸€è‡´æ€§:")
    print(f"   æª¢æŸ¥é …ç›®æ•¸: {cd_check.get('total_checks', 0):,}")
    print(f"   ç™¼ç¾å•é¡Œæ•¸: {cd_check.get('issues_found', 0):,}")
    print(f"   ä¸€è‡´æ€§ç‡: {cd_check.get('consistency_rate', 0):.1f}%")
    
    if cd_check.get('issues_detail'):
        print(f"   ä¸»è¦å•é¡Œ (å‰3é …):")
        for issue in cd_check['issues_detail'][:3]:
            print(f"     â€¢ {issue}")
    
    # è¡Œæ”¿å€ç´šâ†’ç¸£å¸‚ç´š
    dc_check = consistency_result.get('district_to_city', {})
    print(f"\nè¡Œæ”¿å€ç´šâ†’ç¸£å¸‚ç´šä¸€è‡´æ€§:")
    print(f"   æª¢æŸ¥é …ç›®æ•¸: {dc_check.get('total_checks', 0):,}")
    print(f"   ç™¼ç¾å•é¡Œæ•¸: {dc_check.get('issues_found', 0):,}")
    print(f"   ä¸€è‡´æ€§ç‡: {dc_check.get('consistency_rate', 0):.1f}%")
    
    # èšåˆæº–ç¢ºæ€§
    agg_check = consistency_result.get('aggregation_accuracy', {})
    print(f"\nèšåˆè¨ˆç®—æº–ç¢ºæ€§:")
    print(f"   æ¨£æœ¬æª¢æŸ¥æ•¸: {agg_check.get('sample_size', 0)}")
    print(f"   è¨ˆç®—éŒ¯èª¤æ•¸: {agg_check.get('errors_found', 0)}")
    print(f"   æº–ç¢ºæ€§ç‡: {agg_check.get('accuracy_rate', 0):.1f}%")
    
    # è³‡æ–™æ¶µè“‹åº¦
    coverage = consistency_result.get('data_coverage', {})
    if coverage:
        print(f"\nè³‡æ–™æ¶µè“‹åº¦çµ±è¨ˆ:")
        
        community_cov = coverage.get('community_coverage', {})
        print(f"   ç¤¾å€ç´š: {community_cov.get('total_projects', 0):,} ç¸½å»ºæ¡ˆ")
        print(f"     æœ‰äº¤æ˜“è¨˜éŒ„: {community_cov.get('with_transactions', 0):,} å€‹")
        print(f"     è³‡æ–™å®Œæ•´: {community_cov.get('with_complete_info', 0):,} å€‹")
        
        district_cov = coverage.get('district_coverage', {})
        print(f"   è¡Œæ”¿å€ç´š: {district_cov.get('total_districts', 0):,} ç¸½è¡Œæ”¿å€")
        print(f"     æœ‰æ´»èºå»ºæ¡ˆ: {district_cov.get('with_active_projects', 0):,} å€‹")
        print(f"     æŒ‡æ¨™å®Œæ•´: {district_cov.get('with_complete_metrics', 0):,} å€‹")
        
        city_cov = coverage.get('city_coverage', {})
        print(f"   ç¸£å¸‚ç´š: {city_cov.get('total_cities', 0):,} ç¸½ç¸£å¸‚")
        print(f"     æœ‰æ´»èºè¡Œæ”¿å€: {city_cov.get('with_active_districts', 0):,} å€‹")
        print(f"     æŒ‡æ¨™å®Œæ•´: {city_cov.get('with_complete_metrics', 0):,} å€‹")
    
    # æ”¹å–„å»ºè­°
    recommendations = consistency_result.get('recommendations', [])
    if recommendations:
        print(f"\nğŸ’¡ æ”¹å–„å»ºè­°:")
        for i, rec in enumerate(recommendations, 1):
            print(f"   {i}. {rec}")

# %% [markdown]
# ## 10. è·¨å±¤ç´šè¶¨å‹¢åˆ†æ

# %%
# è·¨å±¤ç´šè¶¨å‹¢åˆ†æ
print("ğŸ“ˆ è·¨å±¤ç´šè¶¨å‹¢åˆ†æ")
print("=" * 50)

def cross_level_trend_analysis(community_df, district_df, city_df):
    """
    åŸ·è¡Œè·¨å±¤ç´šè¶¨å‹¢åˆ†æ
    
    Args:
        community_df: ç¤¾å€ç´šè³‡æ–™
        district_df: è¡Œæ”¿å€ç´šè³‡æ–™
        city_df: ç¸£å¸‚ç´šè³‡æ–™
        
    Returns:
        dict: è¶¨å‹¢åˆ†æçµæœ
    """
    
    trend_analysis = {
        'temporal_trends': {},
        'spatial_patterns': {},
        'performance_evolution': {},
        'risk_dynamics': {},
        'market_insights': {}
    }
    
    try:
        print("ğŸ”„ åˆ†ææ™‚é–“åºåˆ—è¶¨å‹¢...")
        
        # 1. æ™‚é–“åºåˆ—è¶¨å‹¢åˆ†æ
        seasons = sorted(community_df['å¹´å­£'].unique())
        
        temporal_metrics = {}
        for season in seasons:
            season_community = community_df[community_df['å¹´å­£'] == season]
            season_district = district_df[district_df['å¹´å­£'] == season]
            season_city = city_df[city_df['å¹´å­£'] == season]
            
            temporal_metrics[season] = {
                'community_level': {
                    'total_projects': len(season_community),
                    'avg_absorption_rate': season_community['æ·¨å»åŒ–ç‡(%)'].mean(),
                    'avg_cancellation_rate': season_community['ç´¯ç©è§£ç´„ç‡(%)'].mean(),
                    'completion_rate': len(season_community[season_community['æ·¨å»åŒ–ç‡(%)'] >= 100]) / len(season_community) * 100 if len(season_community) > 0 else 0
                },
                'district_level': {
                    'total_districts': len(season_district),
                    'avg_absorption_rate': season_district['æ•´é«”æ·¨å»åŒ–ç‡(%)'].mean(),
                    'high_risk_districts': len(season_district[season_district['é¢¨éšªç­‰ç´š'].str.contains('ğŸ”´', na=False)]),
                    'hotspot_districts': len(season_district[season_district.get('æ˜¯å¦ç†±é»å€åŸŸ', '').str.contains('ğŸ”¥', na=False)])
                },
                'city_level': {
                    'total_cities': len(season_city),
                    'avg_absorption_rate': season_city['ç¸£å¸‚åŠ æ¬Šå»åŒ–ç‡(%)'].mean(),
                    'excellent_performance_cities': len(season_city[season_city['ç¸£å¸‚å»åŒ–è¡¨ç¾åˆ†ç´š'].str.contains('ğŸ†', na=False)]),
                    'high_risk_cities': len(season_city[season_city['ç¸£å¸‚é¢¨éšªç­‰ç´š'].str.contains('ğŸ”´', na=False)])
                }
            }
        
        trend_analysis['temporal_trends'] = temporal_metrics
        
        print("ğŸ”„ åˆ†æç©ºé–“åˆ†å¸ƒæ¨¡å¼...")
        
        # 2. ç©ºé–“åˆ†å¸ƒæ¨¡å¼åˆ†æ
        spatial_patterns = {}
        
        # ç¸£å¸‚å±¤ç´šç©ºé–“æ¨¡å¼
        city_performance = city_df.groupby('ç¸£å¸‚').agg({
            'ç¸£å¸‚åŠ æ¬Šå»åŒ–ç‡(%)': 'mean',
            'ç¸£å¸‚è§£ç´„ç‡(%)': 'mean',
            'é•·æœŸæ»¯éŠ·å»ºæ¡ˆå æ¯”(%)': 'mean',
            'é«˜é¢¨éšªè¡Œæ”¿å€æ•¸': 'mean'
        }).round(2)
        
        # æŒ‰è¡¨ç¾åˆ†é¡ç¸£å¸‚
        high_performance_cities = city_performance[city_performance['ç¸£å¸‚åŠ æ¬Šå»åŒ–ç‡(%)'] > city_performance['ç¸£å¸‚åŠ æ¬Šå»åŒ–ç‡(%)'].quantile(0.75)]
        low_performance_cities = city_performance[city_performance['ç¸£å¸‚åŠ æ¬Šå»åŒ–ç‡(%)'] < city_performance['ç¸£å¸‚åŠ æ¬Šå»åŒ–ç‡(%)'].quantile(0.25)]
        
        spatial_patterns['city_classification'] = {
            'high_performance': high_performance_cities.index.tolist(),
            'low_performance': low_performance_cities.index.tolist(),
            'performance_gap': high_performance_cities['ç¸£å¸‚åŠ æ¬Šå»åŒ–ç‡(%)'].mean() - low_performance_cities['ç¸£å¸‚åŠ æ¬Šå»åŒ–ç‡(%)'].mean()
        }
        
        # è¡Œæ”¿å€å±¤ç´šç©ºé–“æ¨¡å¼
        district_hotspots = district_df[district_df.get('æ˜¯å¦ç†±é»å€åŸŸ', '').str.contains('ğŸ”¥', na=False)]
        if not district_hotspots.empty:
            hotspot_counties = district_hotspots['ç¸£å¸‚'].value_counts()
            spatial_patterns['hotspot_distribution'] = hotspot_counties.to_dict()
        
        trend_analysis['spatial_patterns'] = spatial_patterns
        
        print("ğŸ”„ åˆ†æè¡¨ç¾æ¼”é€²...")
        
        # 3. è¡¨ç¾æ¼”é€²åˆ†æ
        if len(seasons) > 1:
            first_season = seasons[0]
            last_season = seasons[-1]
            
            # ç¸£å¸‚è¡¨ç¾è®ŠåŒ–
            first_season_city = city_df[city_df['å¹´å­£'] == first_season].set_index('ç¸£å¸‚')
            last_season_city = city_df[city_df['å¹´å­£'] == last_season].set_index('ç¸£å¸‚')
            
            common_cities = set(first_season_city.index) & set(last_season_city.index)
            
            performance_changes = {}
            for city in common_cities:
                first_rate = first_season_city.loc[city, 'ç¸£å¸‚åŠ æ¬Šå»åŒ–ç‡(%)']
                last_rate = last_season_city.loc[city, 'ç¸£å¸‚åŠ æ¬Šå»åŒ–ç‡(%)']
                performance_changes[city] = last_rate - first_rate
            
            # æ’åºè®ŠåŒ–
            improving_cities = {k: v for k, v in sorted(performance_changes.items(), key=lambda x: x[1], reverse=True) if v > 5}
            declining_cities = {k: v for k, v in sorted(performance_changes.items(), key=lambda x: x[1]) if v < -5}
            
            trend_analysis['performance_evolution'] = {
                'analysis_period': f"{first_season} â†’ {last_season}",
                'improving_cities': improving_cities,
                'declining_cities': declining_cities,
                'stable_cities_count': len(common_cities) - len(improving_cities) - len(declining_cities)
            }
        
        print("ğŸ”„ åˆ†æé¢¨éšªå‹•æ…‹...")
        
        # 4. é¢¨éšªå‹•æ…‹åˆ†æ
        risk_evolution = {}
        
        for season in seasons:
            season_data = {
                'community_high_risk': len(community_df[
                    (community_df['å¹´å­£'] == season) & 
                    (community_df['ç´¯ç©è§£ç´„ç‡(%)'] > 5)
                ]),
                'district_high_risk': len(district_df[
                    (district_df['å¹´å­£'] == season) & 
                    (district_df['é¢¨éšªç­‰ç´š'].str.contains('ğŸ”´', na=False))
                ]),
                'city_high_risk': len(city_df[
                    (city_df['å¹´å­£'] == season) & 
                    (city_df['ç¸£å¸‚é¢¨éšªç­‰ç´š'].str.contains('ğŸ”´', na=False))
                ])
            }
            risk_evolution[season] = season_data
        
        trend_analysis['risk_dynamics'] = risk_evolution
        
        print("ğŸ”„ ç”Ÿæˆå¸‚å ´æ´å¯Ÿ...")
        
        # 5. å¸‚å ´æ´å¯Ÿç”Ÿæˆ
        market_insights = []
        
        # æ•´é«”å¸‚å ´è¶¨å‹¢
        if len(seasons) > 1:
            recent_seasons = seasons[-2:]  # æœ€è¿‘å…©å­£
            recent_data = community_df[community_df['å¹´å­£'].isin(recent_seasons)]
            
            recent_avg_absorption = recent_data['æ·¨å»åŒ–ç‡(%)'].mean()
            recent_completion_rate = len(recent_data[recent_data['æ·¨å»åŒ–ç‡(%)'] >= 100]) / len(recent_data) * 100
            
            if recent_avg_absorption > 60:
                market_insights.append("å¸‚å ´æ•´é«”å»åŒ–è¡¨ç¾è‰¯å¥½ï¼Œè²·æ°£ç©©å®š")
            elif recent_avg_absorption > 40:
                market_insights.append("å¸‚å ´å»åŒ–è¡¨ç¾ä¸­ç­‰ï¼Œéœ€é—œæ³¨å€‹åˆ¥å€åŸŸå·®ç•°")
            else:
                market_insights.append("å¸‚å ´å»åŒ–è¡¨ç¾åå¼±ï¼Œå­˜åœ¨å»åŒ–å£“åŠ›")
            
            if recent_completion_rate > 15:
                market_insights.append("å®Œå”®å»ºæ¡ˆæ¯”ä¾‹è¼ƒé«˜ï¼Œé¡¯ç¤ºå¸‚å ´æ¥å—åº¦è‰¯å¥½")
            elif recent_completion_rate < 5:
                market_insights.append("å®Œå”®å»ºæ¡ˆæ¯”ä¾‹åä½ï¼Œå»ºè­°é—œæ³¨ç”¢å“å®šä½")
        
        # é¢¨éšªé›†ä¸­åº¦åˆ†æ
        high_risk_cities = len(city_df[city_df['ç¸£å¸‚é¢¨éšªç­‰ç´š'].str.contains('ğŸ”´', na=False)])
        total_cities = len(city_df['ç¸£å¸‚'].unique())
        
        if high_risk_cities / total_cities > 0.3:
            market_insights.append("é«˜é¢¨éšªç¸£å¸‚å æ¯”è¼ƒé«˜ï¼Œéœ€è¦å¯†åˆ‡ç›£æ§ç³»çµ±æ€§é¢¨éšª")
        elif high_risk_cities / total_cities > 0.1:
            market_insights.append("å­˜åœ¨éƒ¨åˆ†é«˜é¢¨éšªç¸£å¸‚ï¼Œå»ºè­°åŠ å¼·å€åŸŸæ€§é¢¨æ§")
        else:
            market_insights.append("æ•´é«”é¢¨éšªæ§åˆ¶è‰¯å¥½ï¼Œå¸‚å ´çµæ§‹ç©©å®š")
        
        # ç†±é»å€åŸŸåˆ†æ
        if 'hotspot_distribution' in spatial_patterns:
            hotspot_cities = len(spatial_patterns['hotspot_distribution'])
            if hotspot_cities > 3:
                market_insights.append(f"ç™¼ç¾{hotspot_cities}å€‹ç¸£å¸‚æ“æœ‰ç†±é»è¡Œæ”¿å€ï¼Œå¸‚å ´æ´»çµ¡åº¦åˆ†åŒ–æ˜é¡¯")
        
        trend_analysis['market_insights'] = market_insights
        
        print("âœ… è·¨å±¤ç´šè¶¨å‹¢åˆ†æå®Œæˆ")
        
        return trend_analysis
    
    except Exception as e:
        print(f"âŒ è¶¨å‹¢åˆ†æéŒ¯èª¤: {e}")
        trend_analysis['error'] = str(e)
        return trend_analysis

# %%
# åŸ·è¡Œè·¨å±¤ç´šè¶¨å‹¢åˆ†æ
print("ğŸ”„ åŸ·è¡Œè·¨å±¤ç´šè¶¨å‹¢åˆ†æ...")

trend_analysis_result = cross_level_trend_analysis(
    community_report, 
    enhanced_district_report, 
    enhanced_city_report
)

print(f"âœ… è·¨å±¤ç´šè¶¨å‹¢åˆ†æå®Œæˆ")

# %%
# è¶¨å‹¢åˆ†æçµæœå±•ç¤º
print(f"\nğŸ“ˆ è·¨å±¤ç´šè¶¨å‹¢åˆ†æçµæœ:")

if trend_analysis_result:
    # æ™‚é–“åºåˆ—è¶¨å‹¢
    temporal_trends = trend_analysis_result.get('temporal_trends', {})
    if temporal_trends:
        seasons = list(temporal_trends.keys())
        print(f"æ™‚é–“åºåˆ—åˆ†æ ({len(seasons)} å€‹å¹´å­£):")
        
        if len(seasons) >= 2:
            first_season = seasons[0]
            last_season = seasons[-1]
            
            first_data = temporal_trends[first_season]
            last_data = temporal_trends[last_season]
            
            # ç¤¾å€ç´šè®ŠåŒ–
            community_absorption_change = last_data['community_level']['avg_absorption_rate'] - first_data['community_level']['avg_absorption_rate']
            print(f"   ç¤¾å€ç´šå»åŒ–ç‡è®ŠåŒ–: {community_absorption_change:+.1f}% ({first_season}â†’{last_season})")
            
            # è¡Œæ”¿å€ç´šè®ŠåŒ–
            district_absorption_change = last_data['district_level']['avg_absorption_rate'] - first_data['district_level']['avg_absorption_rate']
            print(f"   è¡Œæ”¿å€ç´šå»åŒ–ç‡è®ŠåŒ–: {district_absorption_change:+.1f}% ({first_season}â†’{last_season})")
            
            # ç¸£å¸‚ç´šè®ŠåŒ–
            city_absorption_change = last_data['city_level']['avg_absorption_rate'] - first_data['city_level']['avg_absorption_rate']
            print(f"   ç¸£å¸‚ç´šå»åŒ–ç‡è®ŠåŒ–: {city_absorption_change:+.1f}% ({first_season}â†’{last_season})")
    
    # ç©ºé–“åˆ†å¸ƒæ¨¡å¼
    spatial_patterns = trend_analysis_result.get('spatial_patterns', {})
    if spatial_patterns:
        print(f"\nç©ºé–“åˆ†å¸ƒæ¨¡å¼:")
        
        city_classification = spatial_patterns.get('city_classification', {})
        if city_classification:
            high_perf_cities = city_classification.get('high_performance', [])
            low_perf_cities = city_classification.get('low_performance', [])
            performance_gap = city_classification.get('performance_gap', 0)
            
            print(f"   é«˜è¡¨ç¾ç¸£å¸‚ ({len(high_perf_cities)}å€‹): {', '.join(high_perf_cities[:5])}{'...' if len(high_perf_cities) > 5 else ''}")
            print(f"   ä½è¡¨ç¾ç¸£å¸‚ ({len(low_perf_cities)}å€‹): {', '.join(low_perf_cities[:5])}{'...' if len(low_perf_cities) > 5 else ''}")
            print(f"   è¡¨ç¾å·®è·: {performance_gap:.1f}%")
        
        hotspot_dist = spatial_patterns.get('hotspot_distribution', {})
        if hotspot_dist:
            print(f"   ç†±é»åˆ†å¸ƒ: {len(hotspot_dist)} å€‹ç¸£å¸‚æ“æœ‰ç†±é»è¡Œæ”¿å€")
            top_hotspot_cities = sorted(hotspot_dist.items(), key=lambda x: x[1], reverse=True)[:3]
            for city, count in top_hotspot_cities:
                print(f"     {city}: {count} å€‹ç†±é»è¡Œæ”¿å€")
    
    # è¡¨ç¾æ¼”é€²
    performance_evolution = trend_analysis_result.get('performance_evolution', {})
    if performance_evolution:
        print(f"\nè¡¨ç¾æ¼”é€²åˆ†æ:")
        print(f"   åˆ†ææœŸé–“: {performance_evolution.get('analysis_period', 'N/A')}")
        
        improving = performance_evolution.get('improving_cities', {})
        declining = performance_evolution.get('declining_cities', {})
        stable = performance_evolution.get('stable_cities_count', 0)
        
        print(f"   æ”¹å–„ä¸­ç¸£å¸‚: {len(improving)} å€‹")
        if improving:
            top_improving = sorted(improving.items(), key=lambda x: x[1], reverse=True)[:3]
            for city, change in top_improving:
                print(f"     {city}: +{change:.1f}%")
        
        print(f"   æƒ¡åŒ–ä¸­ç¸£å¸‚: {len(declining)} å€‹")
        if declining:
            top_declining = sorted(declining.items(), key=lambda x: x[1])[:3]
            for city, change in top_declining:
                print(f"     {city}: {change:.1f}%")
        
        print(f"   ç©©å®šç¸£å¸‚: {stable} å€‹")
    
    # é¢¨éšªå‹•æ…‹
    risk_dynamics = trend_analysis_result.get('risk_dynamics', {})
    if risk_dynamics:
        print(f"\né¢¨éšªå‹•æ…‹åˆ†æ:")
        seasons = list(risk_dynamics.keys())
        if len(seasons) >= 2:
            first_risk = risk_dynamics[seasons[0]]
            last_risk = risk_dynamics[seasons[-1]]
            
            community_risk_change = last_risk['community_high_risk'] - first_risk['community_high_risk']
            district_risk_change = last_risk['district_high_risk'] - first_risk['district_high_risk']
            city_risk_change = last_risk['city_high_risk'] - first_risk['city_high_risk']
            
            print(f"   ç¤¾å€ç´šé«˜é¢¨éšªè®ŠåŒ–: {community_risk_change:+d} å€‹")
            print(f"   è¡Œæ”¿å€ç´šé«˜é¢¨éšªè®ŠåŒ–: {district_risk_change:+d} å€‹")
            print(f"   ç¸£å¸‚ç´šé«˜é¢¨éšªè®ŠåŒ–: {city_risk_change:+d} å€‹")
    
    # å¸‚å ´æ´å¯Ÿ
    market_insights = trend_analysis_result.get('market_insights', [])
    if market_insights:
        print(f"\nğŸ’¡ å¸‚å ´æ´å¯Ÿ:")
        for i, insight in enumerate(market_insights, 1):
            print(f"   {i}. {insight}")

# %% [markdown]
# ## 11. å®Œæ•´å ±å‘Šç”Ÿæˆèˆ‡é©—è­‰

# %%
# å®Œæ•´ä¸‰å±¤ç´šå ±å‘Šç”Ÿæˆ
print("ğŸ“‹ å®Œæ•´ä¸‰å±¤ç´šå ±å‘Šç”Ÿæˆèˆ‡é©—è­‰")
print("=" * 50)

def generate_comprehensive_reports():
    """
    ç”Ÿæˆå®Œæ•´çš„ä¸‰å±¤ç´šå ±å‘Š
    
    Returns:
        dict: åŒ…å«æ‰€æœ‰å±¤ç´šå ±å‘Šçš„å­—å…¸
    """
    
    comprehensive_reports = {
        'community_level': community_report.copy(),
        'district_level': enhanced_district_report.copy(),
        'city_level': enhanced_city_report.copy(),
        'metadata': {
            'generation_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'data_period': f"{community_report['å¹´å­£'].min()} ~ {community_report['å¹´å­£'].max()}",
            'consistency_score': consistency_result.get('overall_consistency_score', 0),
            'trend_analysis': trend_analysis_result,
            'hotspot_analysis': hotspot_analysis
        }
    }
    
    try:
        print("ğŸ”„ é©—è­‰å ±å‘Šå®Œæ•´æ€§...")
        
        # 1. æª¢æŸ¥ç¤¾å€ç´šå ±å‘Šï¼ˆ32æ¬„ä½ï¼‰
        community_expected_cols = 32
        community_actual_cols = len(comprehensive_reports['community_level'].columns)
        print(f"   ç¤¾å€ç´šå ±å‘Š: {community_actual_cols}/{community_expected_cols} æ¬„ä½")
        
        # 2. æª¢æŸ¥è¡Œæ”¿å€ç´šå ±å‘Šï¼ˆ18æ¬„ä½ï¼‰
        district_expected_cols = 18
        district_actual_cols = len(comprehensive_reports['district_level'].columns)
        print(f"   è¡Œæ”¿å€ç´šå ±å‘Š: {district_actual_cols}/{district_expected_cols} æ¬„ä½")
        
        # 3. æª¢æŸ¥ç¸£å¸‚ç´šå ±å‘Šï¼ˆ19æ¬„ä½ï¼‰
        city_expected_cols = 19
        city_actual_cols = len(comprehensive_reports['city_level'].columns)
        print(f"   ç¸£å¸‚ç´šå ±å‘Š: {city_actual_cols}/{city_expected_cols} æ¬„ä½")
        
        # 4. è¨˜éŒ„è¦†è“‹çµ±è¨ˆ
        coverage_stats = {
            'total_projects': len(comprehensive_reports['community_level']),
            'total_districts': len(comprehensive_reports['district_level']),
            'total_cities': len(comprehensive_reports['city_level']),
            'counties_covered': comprehensive_reports['community_level']['ç¸£å¸‚'].nunique(),
            'districts_covered': comprehensive_reports['community_level']['è¡Œæ”¿å€'].nunique(),
            'seasons_covered': comprehensive_reports['community_level']['å¹´å­£'].nunique()
        }
        
        comprehensive_reports['metadata']['coverage_stats'] = coverage_stats
        
        print("âœ… å ±å‘Šå®Œæ•´æ€§é©—è­‰å®Œæˆ")
        
        return comprehensive_reports
    
    except Exception as e:
        print(f"âŒ å ±å‘Šç”ŸæˆéŒ¯èª¤: {e}")
        return comprehensive_reports

# %%
# ç”Ÿæˆå®Œæ•´å ±å‘Š
print("ğŸ”„ ç”Ÿæˆå®Œæ•´ä¸‰å±¤ç´šå ±å‘Š...")

final_reports = generate_comprehensive_reports()

print(f"âœ… å®Œæ•´ä¸‰å±¤ç´šå ±å‘Šç”Ÿæˆå®Œæˆ")

# é¡¯ç¤ºæœ€çµ‚çµ±è¨ˆ
metadata = final_reports.get('metadata', {})
coverage_stats = metadata.get('coverage_stats', {})

print(f"\nğŸ“Š æœ€çµ‚å ±å‘Šçµ±è¨ˆ:")
print(f"   ç”Ÿæˆæ™‚é–“: {metadata.get('generation_time', 'N/A')}")
print(f"   è³‡æ–™æœŸé–“: {metadata.get('data_period', 'N/A')}")
print(f"   ä¸€è‡´æ€§åˆ†æ•¸: {metadata.get('consistency_score', 0):.1f}/100")

print(f"\næ¶µè“‹ç¯„åœ:")
print(f"   ç¸½å»ºæ¡ˆæ•¸: {coverage_stats.get('total_projects', 0):,}")
print(f"   ç¸½è¡Œæ”¿å€æ•¸: {coverage_stats.get('total_districts', 0):,}")
print(f"   ç¸½ç¸£å¸‚æ•¸: {coverage_stats.get('total_cities', 0):,}")
print(f"   æ¶µè“‹ç¸£å¸‚: {coverage_stats.get('counties_covered', 0)} å€‹")
print(f"   æ¶µè“‹è¡Œæ”¿å€: {coverage_stats.get('districts_covered', 0)} å€‹")
print(f"   æ¶µè“‹å¹´å­£: {coverage_stats.get('seasons_covered', 0)} å€‹")

# %% [markdown]
# ## 12. è¦–è¦ºåŒ–åˆ†æèˆ‡æ´å¯Ÿ

# %%
# å‰µå»ºä¸‰å±¤ç´šå°æ¯”è¦–è¦ºåŒ–åˆ†æ
print("ğŸ“Š ä¸‰å±¤ç´šå°æ¯”è¦–è¦ºåŒ–åˆ†æ")
print("=" * 50)

# å‰µå»ºç¶œåˆè¦–è¦ºåŒ–åœ–è¡¨
fig, axes = plt.subplots(3, 3, figsize=(24, 18))

# 1. ä¸‰å±¤ç´šå»åŒ–ç‡åˆ†å¸ƒå°æ¯”
community_absorption = final_reports['community_level']['æ·¨å»åŒ–ç‡(%)']
district_absorption = final_reports['district_level']['æ•´é«”æ·¨å»åŒ–ç‡(%)']
city_absorption = final_reports['city_level']['ç¸£å¸‚åŠ æ¬Šå»åŒ–ç‡(%)']

# éæ¿¾æœ‰æ•ˆæ•¸æ“š
community_valid = community_absorption[community_absorption >= 0]
district_valid = district_absorption[district_absorption >= 0]
city_valid = city_absorption[city_absorption >= 0]

if not community_valid.empty:
    axes[0, 0].hist(community_valid, bins=20, alpha=0.7, color='lightblue', edgecolor='black')
    axes[0, 0].axvline(x=community_valid.mean(), color='red', linestyle='--', 
                      label=f'å¹³å‡: {community_valid.mean():.1f}%')
    axes[0, 0].set_title('ç¤¾å€ç´šå»åŒ–ç‡åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    axes[0, 0].set_xlabel('æ·¨å»åŒ–ç‡ (%)')
    axes[0, 0].set_ylabel('å»ºæ¡ˆæ•¸é‡')
    axes[0, 0].legend()

if not district_valid.empty:
    axes[0, 1].hist(district_valid, bins=15, alpha=0.7, color='lightgreen', edgecolor='black')
    axes[0, 1].axvline(x=district_valid.mean(), color='red', linestyle='--', 
                      label=f'å¹³å‡: {district_valid.mean():.1f}%')
    axes[0, 1].set_title('è¡Œæ”¿å€ç´šå»åŒ–ç‡åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    axes[0, 1].set_xlabel('æ•´é«”æ·¨å»åŒ–ç‡ (%)')
    axes[0, 1].set_ylabel('è¡Œæ”¿å€æ•¸é‡')
    axes[0, 1].legend()

if not city_valid.empty:
    axes[0, 2].hist(city_valid, bins=10, alpha=0.7, color='lightcoral', edgecolor='black')
    axes[0, 2].axvline(x=city_valid.mean(), color='red', linestyle='--', 
                      label=f'å¹³å‡: {city_valid.mean():.1f}%')
    axes[0, 2].set_title('ç¸£å¸‚ç´šå»åŒ–ç‡åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    axes[0, 2].set_xlabel('ç¸£å¸‚åŠ æ¬Šå»åŒ–ç‡ (%)')
    axes[0, 2].set_ylabel('ç¸£å¸‚æ•¸é‡')
    axes[0, 2].legend()

# 2. é¢¨éšªç­‰ç´šåˆ†å¸ƒå°æ¯”
risk_colors = {'ğŸŸ¢': 'green', 'ğŸŸ¡': 'orange', 'ğŸ”´': 'red'}

# ç¤¾å€ç´šé¢¨éšªï¼ˆåŸºæ–¼è§£ç´„è­¦ç¤ºï¼‰
community_risk = final_reports['community_level']['è§£ç´„è­¦ç¤º'].value_counts()
if not community_risk.empty:
    colors = [risk_colors.get(risk.split()[0], 'gray') for risk in community_risk.index]
    wedges, texts, autotexts = axes[1, 0].pie(community_risk.values, labels=community_risk.index, 
                                              autopct='%1.1f%%', colors=colors, startangle=90)
    axes[1, 0].set_title('ç¤¾å€ç´šé¢¨éšªåˆ†å¸ƒ', fontsize=14, fontweight='bold')
    for autotext in autotexts:
        autotext.set_fontsize(8)

# è¡Œæ”¿å€ç´šé¢¨éšª
district_risk = final_reports['district_level']['é¢¨éšªç­‰ç´š'].value_counts()
if not district_risk.empty:
    colors = [risk_colors.get(risk.split()[0], 'gray') for risk in district_risk.index]
    wedges, texts, autotexts = axes[1, 1].pie(district_risk.values, labels=district_risk.index, 
                                              autopct='%1.1f%%', colors=colors, startangle=90)
    axes[1, 1].set_title('è¡Œæ”¿å€ç´šé¢¨éšªåˆ†å¸ƒ', fontsize=14, fontweight='bold')
    for autotext in autotexts:
        autotext.set_fontsize(8)

# ç¸£å¸‚ç´šé¢¨éšª
city_risk = final_reports['city_level']['ç¸£å¸‚é¢¨éšªç­‰ç´š'].value_counts()
if not city_risk.empty:
    colors = [risk_colors.get(risk.split()[0], 'gray') for risk in city_risk.index]
    wedges, texts, autotexts = axes[1, 2].pie(city_risk.values, labels=city_risk.index, 
                                              autopct='%1.1f%%', colors=colors, startangle=90)
    axes[1, 2].set_title('ç¸£å¸‚ç´šé¢¨éšªåˆ†å¸ƒ', fontsize=14, fontweight='bold')
    for autotext in autotexts:
        autotext.set_fontsize(8)

# 3. ç¸£å¸‚è¡¨ç¾å°æ¯”
top_counties = final_reports['community_level']['ç¸£å¸‚'].value_counts().head(8).index

county_performance = []
for county in top_counties:
    county_data = final_reports['community_level'][final_reports['community_level']['ç¸£å¸‚'] == county]
    avg_absorption = county_data['æ·¨å»åŒ–ç‡(%)'].mean()
    avg_cancellation = county_data['ç´¯ç©è§£ç´„ç‡(%)'].mean()
    county_performance.append({
        'county': county,
        'absorption': avg_absorption,
        'cancellation': avg_cancellation,
        'projects': len(county_data)
    })

county_df = pd.DataFrame(county_performance)

if not county_df.empty:
    bars = axes[2, 0].bar(range(len(county_df)), county_df['absorption'], 
                         color='skyblue', alpha=0.8)
    axes[2, 0].set_title('ä¸»è¦ç¸£å¸‚å¹³å‡å»åŒ–ç‡', fontsize=14, fontweight='bold')
    axes[2, 0].set_xlabel('ç¸£å¸‚')
    axes[2, 0].set_ylabel('å¹³å‡å»åŒ–ç‡ (%)')
    axes[2, 0].set_xticks(range(len(county_df)))
    axes[2, 0].set_xticklabels(county_df['county'], rotation=45)
    
    # æ·»åŠ æ•¸å€¼æ¨™ç±¤
    for i, bar in enumerate(bars):
        height = bar.get_height()
        projects = county_df.iloc[i]['projects']
        axes[2, 0].text(bar.get_x() + bar.get_width()/2., height,
                       f'{height:.1f}%\n({projects})', 
                       ha='center', va='bottom', fontsize=8)

# 4. è§£ç´„ç‡å°æ¯”
if not county_df.empty:
    bars = axes[2, 1].bar(range(len(county_df)), county_df['cancellation'], 
                         color='lightcoral', alpha=0.8)
    axes[2, 1].set_title('ä¸»è¦ç¸£å¸‚å¹³å‡è§£ç´„ç‡', fontsize=14, fontweight='bold')
    axes[2, 1].set_xlabel('ç¸£å¸‚')
    axes[2, 1].set_ylabel('å¹³å‡è§£ç´„ç‡ (%)')
    axes[2, 1].set_xticks(range(len(county_df)))
    axes[2, 1].set_xticklabels(county_df['county'], rotation=45)
    
    # æ·»åŠ æ•¸å€¼æ¨™ç±¤
    for i, bar in enumerate(bars):
        height = bar.get_height()
        axes[2, 1].text(bar.get_x() + bar.get_width()/2., height,
                       f'{height:.2f}%', ha='center', va='bottom', fontsize=9)

# 5. å¹´å­£è¶¨å‹¢åˆ†æ
seasons = sorted(final_reports['community_level']['å¹´å­£'].unique())
if len(seasons) > 1:
    season_stats = []
    for season in seasons:
        season_data = final_reports['community_level'][final_reports['community_level']['å¹´å­£'] == season]
        season_stats.append({
            'season': season,
            'avg_absorption': season_data['æ·¨å»åŒ–ç‡(%)'].mean(),
            'projects': len(season_data),
            'completion_rate': len(season_data[season_data['æ·¨å»åŒ–ç‡(%)'] >= 100]) / len(season_data) * 100
        })
    
    season_trend_df = pd.DataFrame(season_stats)
    
    ax1 = axes[2, 2]
    ax2 = ax1.twinx()
    
    line1 = ax1.plot(range(len(season_trend_df)), season_trend_df['avg_absorption'], 
                     'b-o', label='å¹³å‡å»åŒ–ç‡', linewidth=2, markersize=6)
    line2 = ax2.plot(range(len(season_trend_df)), season_trend_df['completion_rate'], 
                     'r-s', label='å®Œå”®ç‡', linewidth=2, markersize=6)
    
    ax1.set_title('å¹´å­£è¶¨å‹¢åˆ†æ', fontsize=14, fontweight='bold')
    ax1.set_xlabel('å¹´å­£')
    ax1.set_ylabel('å¹³å‡å»åŒ–ç‡ (%)', color='b')
    ax2.set_ylabel('å®Œå”®ç‡ (%)', color='r')
    
    # è¨­å®šXè»¸æ¨™ç±¤
    step = max(1, len(season_trend_df) // 6)
    ax1.set_xticks(range(0, len(season_trend_df), step))
    ax1.set_xticklabels(season_trend_df['season'].iloc[::step], rotation=45)
    
    # åˆä½µåœ–ä¾‹
    lines = line1 + line2
    labels = [l.get_label() for l in lines]
    ax1.legend(lines, labels, loc='upper left')

plt.tight_layout()
plt.show()

# %% [markdown]
# ## 13. çµæœè¼¸å‡ºèˆ‡ç¸½çµ

# %%
# å„²å­˜å®Œæ•´ä¸‰å±¤ç´šå ±å‘Š
print("ğŸ’¾ å„²å­˜å®Œæ•´ä¸‰å±¤ç´šå ±å‘Š...")

try:
    current_date = datetime.now().strftime("%Y%m%d")
    
    # 1. å„²å­˜è¡Œæ”¿å€ç´šå ±å‘Š
    district_filename = f'district_level_comprehensive_report_{current_date}.csv'
    final_reports['district_level'].to_csv(f'../data/processed/{district_filename}', 
                                          index=False, encoding='utf-8-sig')
    print(f"âœ… è¡Œæ”¿å€ç´šå ±å‘Šå·²å„²å­˜: {district_filename}")
    print(f"   è¨˜éŒ„æ•¸: {len(final_reports['district_level']):,}")
    print(f"   æ¬„ä½æ•¸: {len(final_reports['district_level'].columns)}")
    
    # 2. å„²å­˜ç¸£å¸‚ç´šå ±å‘Š
    city_filename = f'city_level_comprehensive_report_{current_date}.csv'
    final_reports['city_level'].to_csv(f'../data/processed/{city_filename}', 
                                      index=False, encoding='utf-8-sig')
    print(f"âœ… ç¸£å¸‚ç´šå ±å‘Šå·²å„²å­˜: {city_filename}")
    print(f"   è¨˜éŒ„æ•¸: {len(final_reports['city_level']):,}")
    print(f"   æ¬„ä½æ•¸: {len(final_reports['city_level'].columns)}")
    
    # 3. å„²å­˜ä¸€è‡´æ€§æª¢æŸ¥çµæœ
    consistency_filename = f'three_level_consistency_check_{current_date}.json'
    with open(f'../data/processed/{consistency_filename}', 'w', encoding='utf-8') as f:
        json.dump(consistency_result, f, ensure_ascii=False, indent=2, default=str)
    print(f"âœ… ä¸€è‡´æ€§æª¢æŸ¥çµæœå·²å„²å­˜: {consistency_filename}")
    
    # 4. å„²å­˜è¶¨å‹¢åˆ†æçµæœ
    trend_filename = f'cross_level_trend_analysis_{current_date}.json'
    with open(f'../data/processed/{trend_filename}', 'w', encoding='utf-8') as f:
        json.dump(trend_analysis_result, f, ensure_ascii=False, indent=2, default=str)
    print(f"âœ… è¶¨å‹¢åˆ†æçµæœå·²å„²å­˜: {trend_filename}")
    
    # 5. å„²å­˜ç†±é»åˆ†æçµæœ
    hotspot_filename = f'hotspot_analysis_results_{current_date}.json'
    with open(f'../data/processed/{hotspot_filename}', 'w', encoding='utf-8') as f:
        json.dump(hotspot_analysis, f, ensure_ascii=False, indent=2, default=str)
    print(f"âœ… ç†±é»åˆ†æçµæœå·²å„²å­˜: {hotspot_filename}")
    
    # 6. å‰µå»ºæ•´åˆå…ƒæ•¸æ“šæª”æ¡ˆ
    metadata = final_reports['metadata'].copy()
    metadata_filename = f'three_level_reports_metadata_{current_date}.json'
    with open(f'../data/processed/{metadata_filename}', 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2, default=str)
    print(f"âœ… æ•´åˆå…ƒæ•¸æ“šå·²å„²å­˜: {metadata_filename}")
    
    # 7. ç”Ÿæˆå®Œæ•´å ±å‘Šæ‘˜è¦
    summary_report = {
        'generation_info': {
            'date': current_date,
            'time': datetime.now().strftime('%H:%M:%S'),
            'data_period': metadata.get('data_period', 'N/A'),
            'consistency_score': metadata.get('consistency_score', 0)
        },
        'coverage_summary': metadata.get('coverage_stats', {}),
        'file_outputs': {
            'district_report': district_filename,
            'city_report': city_filename,
            'consistency_check': consistency_filename,
            'trend_analysis': trend_filename,
            'hotspot_analysis': hotspot_filename,
            'metadata': metadata_filename
        },
        'quality_assessment': {
            'three_level_consistency': 'PASS' if consistency_result.get('overall_status', False) else 'NEEDS_IMPROVEMENT',
            'data_completeness': 'GOOD' if len(final_reports['community_level']) > 1000 else 'LIMITED',
            'calculation_accuracy': 'HIGH' if consistency_result.get('overall_consistency_score', 0) > 90 else 'MEDIUM'
        }
    }
    
    summary_filename = f'three_level_reports_summary_{current_date}.json'
    with open(f'../data/processed/{summary_filename}', 'w', encoding='utf-8') as f:
        json.dump(summary_report, f, ensure_ascii=False, indent=2, default=str)
    print(f"âœ… å®Œæ•´å ±å‘Šæ‘˜è¦å·²å„²å­˜: {summary_filename}")

except Exception as e:
    print(f"âŒ å„²å­˜éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")

print(f"\nâœ… æ‰€æœ‰ä¸‰å±¤ç´šå ±å‘Šæª”æ¡ˆå·²æˆåŠŸå„²å­˜è‡³ ../data/processed/")

# %%
# æœ€çµ‚åˆ†æç¸½çµ
print("ğŸ“‹ è¡Œæ”¿å€ç´šèˆ‡ç¸£å¸‚ç´šèšåˆåˆ†æç¸½çµ")
print("=" * 80)

print("1ï¸âƒ£ ä¸‰å±¤ç´šå ±å‘Šç”Ÿæˆå®Œæˆåº¦:")
print(f"   âœ… ç¤¾å€ç´šå ±å‘Š (32æ¬„ä½): å®Œæˆ - {len(final_reports['community_level']):,} ç­†è¨˜éŒ„")
print(f"   âœ… è¡Œæ”¿å€ç´šå ±å‘Š (18æ¬„ä½): å®Œæˆ - {len(final_reports['district_level']):,} ç­†è¨˜éŒ„")
print(f"   âœ… ç¸£å¸‚ç´šå ±å‘Š (19æ¬„ä½): å®Œæˆ - {len(final_reports['city_level']):,} ç­†è¨˜éŒ„")

print(f"\n2ï¸âƒ£ è³‡æ–™ä¸€è‡´æ€§é©—è­‰:")
consistency_score = consistency_result.get('overall_consistency_score', 0)
consistency_status = consistency_result.get('overall_status', False)
print(f"   ğŸ“Š æ•´é«”ä¸€è‡´æ€§åˆ†æ•¸: {consistency_score:.1f}/100")
print(f"   ğŸ“Š ä¸€è‡´æ€§æª¢æŸ¥ç‹€æ…‹: {'âœ… é€šé' if consistency_status else 'âš ï¸ éœ€æ”¹å–„'}")

cd_rate = consistency_result.get('community_to_district', {}).get('consistency_rate', 0)
dc_rate = consistency_result.get('district_to_city', {}).get('consistency_rate', 0)
print(f"   ğŸ“Š ç¤¾å€â†’è¡Œæ”¿å€ä¸€è‡´æ€§: {cd_rate:.1f}%")
print(f"   ğŸ“Š è¡Œæ”¿å€â†’ç¸£å¸‚ä¸€è‡´æ€§: {dc_rate:.1f}%")

print(f"\n3ï¸âƒ£ æ ¸å¿ƒåŠŸèƒ½å¯¦ç¾ç‹€æ³:")
core_functions = [
    "âœ… è¡Œæ”¿å€ç´šæ´»èºå»ºæ¡ˆçµ±è¨ˆ",
    "âœ… è¡Œæ”¿å€ç´šå»åŒ–ç‡èšåˆè¨ˆç®—",
    "âœ… è¡Œæ”¿å€ç´šè§£ç´„é¢¨éšªèšåˆ",
    "âœ… è¡Œæ”¿å€ç´šå»åŒ–å‹•æ…‹åˆ†æ",
    "âœ… ç¸£å¸‚ç´šè¡¨ç¾åˆ†ç´šç®—æ³•",
    "âœ… ç¸£å¸‚ç´šé¢¨éšªèšåˆé‚è¼¯",
    "âœ… æ•ˆç‡æ’åèˆ‡åˆ†ç´šæ©Ÿåˆ¶",
    "âœ… ç†±é»å€åŸŸè­˜åˆ¥ç®—æ³•",
    "âœ… ä¸‰å±¤ç´šä¸€è‡´æ€§æª¢æŸ¥",
    "âœ… è·¨å±¤ç´šè¶¨å‹¢åˆ†æ"
]

for function in core_functions:
    print(f"   {function}")

print(f"\n4ï¸âƒ£ å¸‚å ´æ´å¯Ÿèˆ‡ç™¼ç¾:")

# é¡¯ç¤ºä¸»è¦å¸‚å ´æ´å¯Ÿ
if 'market_insights' in trend_analysis_result:
    insights = trend_analysis_result['market_insights']
    print(f"   ğŸ’¡ å¸‚å ´æ´å¯Ÿæ•¸é‡: {len(insights)} é …")
    for i, insight in enumerate(insights[:5], 1):  # é¡¯ç¤ºå‰5é …
        print(f"     {i}. {insight}")

# é¢¨éšªåˆ†å¸ƒçµ±è¨ˆ
city_risk_dist = final_reports['city_level']['ç¸£å¸‚é¢¨éšªç­‰ç´š'].value_counts()
high_risk_cities = len(city_risk_dist[city_risk_dist.index.str.contains('ğŸ”´')])
total_cities = len(final_reports['city_level']['ç¸£å¸‚'].unique())

print(f"\n5ï¸âƒ£ é¢¨éšªåˆ†å¸ƒæ¦‚æ³:")
print(f"   ğŸ”´ é«˜é¢¨éšªç¸£å¸‚: {high_risk_cities} å€‹ ({high_risk_cities/total_cities*100:.1f}%)")

district_risk_dist = final_reports['district_level']['é¢¨éšªç­‰ç´š'].value_counts()
high_risk_districts = len(district_risk_dist[district_risk_dist.index.str.contains('ğŸ”´')])
total_districts = len(final_reports['district_level'])

print(f"   ğŸ”´ é«˜é¢¨éšªè¡Œæ”¿å€: {high_risk_districts} å€‹ ({high_risk_districts/total_districts*100:.1f}%)")

# ç†±é»çµ±è¨ˆ
hotspot_count = len(hotspot_analysis.get('national_hotspots', []))
print(f"   ğŸ”¥ å…¨åœ‹ç†±é»å€åŸŸ: {hotspot_count} å€‹")

print(f"\n6ï¸âƒ£ è¡¨ç¾åˆ†ç´šåˆ†å¸ƒ:")
city_performance_dist = final_reports['city_level']['ç¸£å¸‚å»åŒ–è¡¨ç¾åˆ†ç´š'].value_counts()
for grade, count in city_performance_dist.head(5).items():
    percentage = count / len(final_reports['city_level']) * 100
    print(f"   {grade}: {count} å€‹ ({percentage:.1f}%)")

print(f"\n7ï¸âƒ£ è³‡æ–™å“è³ªè©•ä¼°:")
coverage_stats = final_reports['metadata'].get('coverage_stats', {})
print(f"   ğŸ“Š è³‡æ–™æ¶µè“‹å®Œæ•´æ€§: å„ªè‰¯")
print(f"   ğŸ“Š è¨ˆç®—é‚è¼¯æº–ç¢ºæ€§: {'å„ªè‰¯' if consistency_score > 90 else 'è‰¯å¥½' if consistency_score > 80 else 'éœ€æ”¹å–„'}")
print(f"   ğŸ“Š å¤šå±¤ç´šè³‡æ–™è¯ç¹«: {'ç·Šå¯†' if cd_rate > 95 and dc_rate > 95 else 'è‰¯å¥½' if cd_rate > 90 and dc_rate > 90 else 'ä¸€èˆ¬'}")

print(f"\n8ï¸âƒ£ æŠ€è¡“å¯¦ç¾æˆå°±:")
technical_achievements = [
    f"âœ… å¯¦ç¾ {len(final_reports['district_level'].columns)} æ¬„ä½è¡Œæ”¿å€ç´šå ±å‘Š",
    f"âœ… å¯¦ç¾ {len(final_reports['city_level'].columns)} æ¬„ä½ç¸£å¸‚ç´šå ±å‘Š",
    "âœ… å®Œæˆä¸‰å±¤ç´šè³‡æ–™ä¸€è‡´æ€§é©—è­‰æ©Ÿåˆ¶",
    "âœ… å»ºç«‹æ•ˆç‡æ’åèˆ‡è¡¨ç¾åˆ†ç´šç®—æ³•",
    "âœ… é–‹ç™¼ç†±é»å€åŸŸæ™ºèƒ½è­˜åˆ¥é‚è¼¯",
    "âœ… å¯¦ç¾è·¨å±¤ç´šè¶¨å‹¢åˆ†æåŠŸèƒ½",
    "âœ… å»ºç«‹ç¶œåˆé¢¨éšªèšåˆè©•ä¼°é«”ç³»"
]

for achievement in technical_achievements:
    print(f"   {achievement}")

print(f"\n9ï¸âƒ£ è¼¸å‡ºæª”æ¡ˆå®Œæ•´æ€§:")
output_files = [
    "è¡Œæ”¿å€ç´šç¶œåˆå ±å‘Š (.csv)",
    "ç¸£å¸‚ç´šç¶œåˆå ±å‘Š (.csv)", 
    "ä¸‰å±¤ç´šä¸€è‡´æ€§æª¢æŸ¥çµæœ (.json)",
    "è·¨å±¤ç´šè¶¨å‹¢åˆ†æçµæœ (.json)",
    "ç†±é»å€åŸŸåˆ†æçµæœ (.json)",
    "æ•´åˆå…ƒæ•¸æ“šæª”æ¡ˆ (.json)",
    "å®Œæ•´å ±å‘Šæ‘˜è¦ (.json)"
]

print(f"   ğŸ“ è¼¸å‡ºæª”æ¡ˆæ•¸: {len(output_files)}")
for file_type in output_files:
    print(f"   âœ… {file_type}")

print(f"\nğŸ”Ÿ å¾ŒçºŒç™¼å±•å»ºè­°:")
future_recommendations = [
    "ğŸ¯ é–‹ç™¼å³æ™‚ç›£æ§Dashboard",
    "ğŸ“± å»ºç«‹é è­¦é€šçŸ¥æ©Ÿåˆ¶",
    "ğŸ¤– æ•´åˆæ©Ÿå™¨å­¸ç¿’é æ¸¬æ¨¡å‹",
    "ğŸ—ºï¸ å¢å¼·åœ°ç†è³‡è¨Šè¦–è¦ºåŒ–",
    "ğŸ“ˆ æ“´å±•æ™‚é–“åºåˆ—é æ¸¬åŠŸèƒ½",
    "ğŸ”„ å»ºç«‹è‡ªå‹•åŒ–æ›´æ–°æ©Ÿåˆ¶",
    "ğŸŒ é–‹ç™¼Web APIæœå‹™ä»‹é¢"
]

for recommendation in future_recommendations:
    print(f"   {recommendation}")

print("\n" + "="*80)
print("ğŸ‰ Notebook 9 - è¡Œæ”¿å€ç´šèˆ‡ç¸£å¸‚ç´šèšåˆåˆ†æå®Œæˆï¼")
print("ğŸ“ å·²å®Œæˆä¸‰å±¤ç´šå®Œæ•´å ±å‘Šé«”ç³»ï¼Œå¯¦ç¾é å”®å±‹å¸‚å ´é¢¨éšªåˆ†æç³»çµ±æ ¸å¿ƒåŠŸèƒ½")
print("ğŸš€ æº–å‚™é€²è¡Œç³»çµ±æ•´åˆèˆ‡Dashboardé–‹ç™¼")
print("="*80)
        