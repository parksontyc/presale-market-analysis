# é å”®å±‹å¸‚å ´åˆ†æç³»çµ± - 08_ç¤¾å€ç´šå ±å‘Šç”Ÿæˆ
# åŸºæ–¼ PRD v2.3 è¦æ ¼é€²è¡Œ32æ¬„ä½ç¤¾å€ç´šå®Œæ•´å ±å‘Šç”Ÿæˆ
# ================================================================================

# %% [markdown]
# # é å”®å±‹å¸‚å ´åˆ†æç³»çµ± - ç¤¾å€ç´šå ±å‘Šç”Ÿæˆ
# 
# ## ğŸ“‹ ç›®æ¨™
# - âœ… å¯¦ä½œ32æ¬„ä½ç¤¾å€ç´šå ±å‘Š
# - âœ… æ•´åˆæ‰€æœ‰å‰åºåˆ†æçµæœ
# - âœ… é©—è­‰æ‰€æœ‰è¨ˆç®—é‚è¼¯
# - âœ… ç”¢ç”Ÿæ¨™æº–åŒ–è¼¸å‡ºæ ¼å¼
# - âœ… å»ºç«‹è³‡æ–™å“è³ªæª¢æŸ¥æ©Ÿåˆ¶
# - âœ… è™•ç†ç•°å¸¸æ¡ˆä¾‹èˆ‡é‚Šç•Œæƒ…æ³
# - âœ… ç”Ÿæˆå®Œæ•´å ±å‘Šæ–‡æª”
# 
# ## ğŸ¯ å…§å®¹å¤§ç¶±
# 1. ç’°å¢ƒè¨­å®šèˆ‡è³‡æ–™è¼‰å…¥
# 2. 32æ¬„ä½å ±å‘Šæ ¼å¼å®šç¾©
# 3. åŸºæœ¬è³‡è¨Šæ•´åˆ (7æ¬„ä½)
# 4. æ™‚é–“èˆ‡æ•¸é‡è¨ˆç®— (5æ¬„ä½)
# 5. è§£ç´„è³‡è¨Šçµ±è¨ˆ (6æ¬„ä½)
# 6. å»åŒ–åˆ†ææ•´åˆ (3æ¬„ä½)
# 7. å»åŒ–å‹•æ…‹æ•´åˆ (4æ¬„ä½)
# 8. åƒ¹æ ¼åˆ†æè¨ˆç®— (3æ¬„ä½)
# 9. éšæ®µåˆ†ææ•´åˆ (3æ¬„ä½)
# 10. å“è³ªæ§åˆ¶è©•ä¼° (1æ¬„ä½)
# 11. ç¤¾å€ç´šå ±å‘Šç”Ÿæˆèˆ‡é©—è­‰
# 12. è³‡æ–™å“è³ªæª¢æŸ¥èˆ‡ç•°å¸¸è™•ç†
# 13. å ±å‘Šè¼¸å‡ºèˆ‡æ–‡æª”ç”Ÿæˆ
# 
# ## ğŸ“Š 32æ¬„ä½ç¤¾å€ç´šå ±å‘Šè¦æ ¼
# ä¾æ“šPRD v2.3è¦æ ¼ï¼Œç”ŸæˆåŒ…å«åŸºæœ¬è³‡è¨Šã€å»åŒ–åˆ†æã€é¢¨éšªè©•ä¼°ã€åƒ¹æ ¼è¶¨å‹¢ç­‰å®Œæ•´è³‡è¨Šçš„ç¤¾å€ç´šå ±å‘Š

# %% [markdown]
# ## 1. ç’°å¢ƒè¨­å®šèˆ‡è³‡æ–™è¼‰å…¥

# %%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import re
import warnings
from collections import Counter, defaultdict
import math
from scipy import stats
import json
warnings.filterwarnings('ignore')

# è¨­å®šé¡¯ç¤ºé¸é …
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)
pd.set_option('display.width', None)
pd.set_option('display.max_colwidth', 80)

# è¨­å®šä¸­æ–‡å­—å‹
plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'Arial Unicode MS', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False

# è¨­å®šåœ–è¡¨æ¨£å¼
sns.set_style("whitegrid")
plt.style.use('default')

print("âœ… ç’°å¢ƒè¨­å®šå®Œæˆ")
print(f"ğŸ“… åˆ†ææ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

# %%
# è¼‰å…¥æ‰€æœ‰å‰åºåˆ†æçµæœ
print("ğŸ”„ è¼‰å…¥å‰åºåˆ†æçµæœ...")

try:
    # Notebook 1-2: åŸºç¤è³‡æ–™èˆ‡è§£ç´„åˆ†æ
    cancellation_analysis = pd.read_csv('../data/processed/02_cancellation_analysis.csv', encoding='utf-8')
    print(f"âœ… è§£ç´„åˆ†æè³‡æ–™: {cancellation_analysis.shape}")
    
    # Notebook 3: é‡è¤‡äº¤æ˜“è™•ç†çµæœ
    clean_transactions = pd.read_csv('../data/processed/03_clean_transactions.csv', encoding='utf-8')
    print(f"âœ… ä¹¾æ·¨äº¤æ˜“è³‡æ–™: {clean_transactions.shape}")
    
    deduplication_results = pd.read_csv('../data/processed/03_deduplication_results.csv', encoding='utf-8')
    print(f"âœ… å»é‡è™•ç†çµæœ: {deduplication_results.shape}")
    
    # Notebook 4: å»ºæ¡ˆæ•´åˆèˆ‡æ´»èºå»ºæ¡ˆ
    project_integration = pd.read_csv('../data/processed/04_project_integration_results.csv', encoding='utf-8')
    print(f"âœ… å»ºæ¡ˆæ•´åˆçµæœ: {project_integration.shape}")
    
    active_projects = pd.read_csv('../data/processed/04_active_projects_analysis.csv', encoding='utf-8')
    print(f"âœ… æ´»èºå»ºæ¡ˆåˆ†æ: {active_projects.shape}")
    
    # Notebook 5: å»åŒ–ç‡åˆ†æ
    absorption_analysis = pd.read_csv('../data/processed/05_absorption_rate_analysis.csv', encoding='utf-8')
    print(f"âœ… å»åŒ–ç‡åˆ†æ: {absorption_analysis.shape}")
    
    # Notebook 6: å»åŒ–å‹•æ…‹åˆ†æ
    quarterly_speed = pd.read_csv('../data/processed/06_quarterly_absorption_speed.csv', encoding='utf-8')
    print(f"âœ… å­£åº¦å»åŒ–é€Ÿåº¦: {quarterly_speed.shape}")
    
    absorption_acceleration = pd.read_csv('../data/processed/06_absorption_acceleration.csv', encoding='utf-8')
    print(f"âœ… å»åŒ–åŠ é€Ÿåº¦: {absorption_acceleration.shape}")
    
    completion_prediction = pd.read_csv('../data/processed/06_completion_prediction.csv', encoding='utf-8')
    print(f"âœ… å®Œå”®é æ¸¬: {completion_prediction.shape}")
    
    absorption_efficiency = pd.read_csv('../data/processed/06_absorption_efficiency.csv', encoding='utf-8')
    print(f"âœ… å»åŒ–æ•ˆç‡è©•ç´š: {absorption_efficiency.shape}")
    
    # Notebook 7: éšæ®µåˆ¤æ–·èˆ‡é¢¨éšªè©•ä¼°
    sales_stage_analysis = pd.read_csv('../data/processed/07_sales_stage_analysis.csv', encoding='utf-8')
    print(f"âœ… éŠ·å”®éšæ®µåˆ†æ: {sales_stage_analysis.shape}")
    
    stage_performance = pd.read_csv('../data/processed/07_stage_performance_evaluation.csv', encoding='utf-8')
    print(f"âœ… éšæ®µè¡¨ç¾è©•ä¼°: {stage_performance.shape}")
    
    cancellation_risk = pd.read_csv('../data/processed/07_cancellation_risk_assessment.csv', encoding='utf-8')
    print(f"âœ… è§£ç´„é¢¨éšªè©•ä¼°: {cancellation_risk.shape}")
    
    comprehensive_risk = pd.read_csv('../data/processed/07_comprehensive_risk_assessment.csv', encoding='utf-8')
    print(f"âœ… ç¶œåˆé¢¨éšªè©•ä¼°: {comprehensive_risk.shape}")
    
    risk_warning = pd.read_csv('../data/processed/07_risk_warning_classification.csv', encoding='utf-8')
    print(f"âœ… é¢¨éšªé è­¦åˆ†é¡: {risk_warning.shape}")
    
    # è¼‰å…¥åŸå§‹è³‡æ–™ä»¥è£œå……è¨ˆç®—
    original_transactions = pd.read_csv('../data/lvr_pre_sale_test.csv', encoding='utf-8')
    print(f"âœ… åŸå§‹äº¤æ˜“è³‡æ–™: {original_transactions.shape}")
    
    original_projects = pd.read_csv('../data/lvr_sale_data_test.csv', encoding='utf-8')
    print(f"âœ… åŸå§‹å»ºæ¡ˆè³‡æ–™: {original_projects.shape}")

except FileNotFoundError as e:
    print(f"âŒ æª”æ¡ˆè¼‰å…¥å¤±æ•—: {e}")
    print("ğŸ“ è«‹ç¢ºèªæ˜¯å¦å·²åŸ·è¡Œ Notebook 1-7")
except Exception as e:
    print(f"âŒ è¼‰å…¥éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")

# %% [markdown]
# ## 2. 32æ¬„ä½å ±å‘Šæ ¼å¼å®šç¾©

# %%
# å®šç¾©32æ¬„ä½ç¤¾å€ç´šå ±å‘Šæ ¼å¼
print("ğŸ“‹ å®šç¾©32æ¬„ä½ç¤¾å€ç´šå ±å‘Šæ ¼å¼")
print("=" * 60)

# PRD v2.3 è¦æ ¼çš„32æ¬„ä½å®šç¾©
COMMUNITY_REPORT_SCHEMA = {
    # A. åŸºæœ¬è³‡è¨Š (7æ¬„)
    'basic_info': {
        'å‚™æŸ¥ç·¨è™Ÿ': 'project_code',
        'ç¤¾å€åç¨±': 'project_name', 
        'ç¸£å¸‚': 'county',
        'è¡Œæ”¿å€': 'district',
        'åè½è¡—é“': 'street_address',
        'ç¸½æˆ¶æ•¸': 'total_units',
        'éŠ·å”®èµ·å§‹å¹´å­£': 'sales_start_season'
    },
    
    # B. æ™‚é–“èˆ‡æ•¸é‡ (5æ¬„)
    'time_quantity': {
        'å¹´å­£': 'target_season',
        'éŠ·å”®å­£æ•¸': 'sales_seasons',
        'ç´¯ç©æˆäº¤ç­†æ•¸': 'cumulative_transactions',
        'è©²å­£æˆäº¤ç­†æ•¸': 'quarterly_transactions',
        'è©²å­£éŠ·å”®å¤©æ•¸': 'quarterly_sales_days'
    },
    
    # C. è§£ç´„è³‡è¨Š (6æ¬„)
    'cancellation_info': {
        'ç´¯ç©è§£ç´„ç­†æ•¸': 'cumulative_cancellations',
        'è©²å­£è§£ç´„ç­†æ•¸': 'quarterly_cancellations',
        'å­£åº¦è§£ç´„ç‡(%)': 'quarterly_cancellation_rate',
        'ç´¯ç©è§£ç´„ç‡(%)': 'cumulative_cancellation_rate',
        'æœ€è¿‘è§£ç´„å¹´å­£': 'latest_cancellation_season',
        'é€£çºŒç„¡è§£ç´„å­£æ•¸': 'consecutive_no_cancellation_seasons'
    },
    
    # D. å»åŒ–åˆ†æ (3æ¬„)
    'absorption_analysis': {
        'æ¯›å»åŒ–ç‡(%)': 'gross_absorption_rate',
        'æ·¨å»åŒ–ç‡(%)': 'net_absorption_rate',
        'èª¿æ•´å»åŒ–ç‡(%)': 'adjusted_absorption_rate'
    },
    
    # E. å»åŒ–å‹•æ…‹åˆ†æ (4æ¬„)
    'absorption_dynamics': {
        'å­£åº¦å»åŒ–é€Ÿåº¦(æˆ¶/å­£)': 'quarterly_absorption_speed',
        'å»åŒ–åŠ é€Ÿåº¦(%)': 'absorption_acceleration',
        'é ä¼°å®Œå”®å­£æ•¸': 'estimated_completion_seasons',
        'å»åŒ–æ•ˆç‡è©•ç´š': 'absorption_efficiency_grade'
    },
    
    # F. åƒ¹æ ¼åˆ†æ (3æ¬„)
    'price_analysis': {
        'å¹³å‡äº¤æ˜“å–®åƒ¹(è¬/åª)': 'avg_unit_price_per_ping',
        'å¹³å‡ç¸½é¢ç©(åª)': 'avg_total_area_ping',
        'å¹³å‡äº¤æ˜“ç¸½åƒ¹(è¬)': 'avg_total_price_wan'
    },
    
    # G. éšæ®µåˆ†æ (3æ¬„)
    'stage_analysis': {
        'éŠ·å”®éšæ®µ': 'sales_stage',
        'éšæ®µè¡¨ç¾': 'stage_performance',
        'è§£ç´„è­¦ç¤º': 'cancellation_warning'
    },
    
    # H. å“è³ªæ§åˆ¶ (1æ¬„)
    'quality_control': {
        'æ˜¯å¦å®Œæ•´å­£': 'is_complete_quarter'
    }
}

# å‰µå»ºæ¬„ä½å°æ‡‰è¡¨
COLUMN_MAPPING = {}
for category, fields in COMMUNITY_REPORT_SCHEMA.items():
    COLUMN_MAPPING.update(fields)

# åå‘å°æ‡‰è¡¨ï¼ˆè‹±æ–‡->ä¸­æ–‡ï¼‰
REVERSE_COLUMN_MAPPING = {v: k for k, v in COLUMN_MAPPING.items()}

print(f"âœ… å·²å®šç¾©32æ¬„ä½å ±å‘Šæ ¼å¼")
print(f"   åŸºæœ¬è³‡è¨Š: {len(COMMUNITY_REPORT_SCHEMA['basic_info'])} æ¬„")
print(f"   æ™‚é–“èˆ‡æ•¸é‡: {len(COMMUNITY_REPORT_SCHEMA['time_quantity'])} æ¬„")
print(f"   è§£ç´„è³‡è¨Š: {len(COMMUNITY_REPORT_SCHEMA['cancellation_info'])} æ¬„")
print(f"   å»åŒ–åˆ†æ: {len(COMMUNITY_REPORT_SCHEMA['absorption_analysis'])} æ¬„")
print(f"   å»åŒ–å‹•æ…‹: {len(COMMUNITY_REPORT_SCHEMA['absorption_dynamics'])} æ¬„")
print(f"   åƒ¹æ ¼åˆ†æ: {len(COMMUNITY_REPORT_SCHEMA['price_analysis'])} æ¬„")
print(f"   éšæ®µåˆ†æ: {len(COMMUNITY_REPORT_SCHEMA['stage_analysis'])} æ¬„")
print(f"   å“è³ªæ§åˆ¶: {len(COMMUNITY_REPORT_SCHEMA['quality_control'])} æ¬„")

total_columns = sum(len(fields) for fields in COMMUNITY_REPORT_SCHEMA.values())
print(f"   ç¸½è¨ˆ: {total_columns} æ¬„ä½")

# %% [markdown]
# ## 3. å·¥å…·å‡½æ•¸å®šç¾©

# %%
# å¹´å­£è™•ç†å·¥å…·å‡½æ•¸
def season_to_number(season_str):
    """å°‡å¹´å­£å­—ä¸²è½‰æ›ç‚ºå¯æ¯”è¼ƒçš„æ•¸å­—"""
    try:
        if not season_str or pd.isna(season_str):
            return 0
        season_str = str(season_str).strip()
        if 'Y' not in season_str or 'S' not in season_str:
            return 0
        year_part = season_str.split('Y')[0]
        season_part = season_str.split('Y')[1].replace('S', '')
        return int(year_part) * 10 + int(season_part)
    except:
        return 0

def number_to_season(season_num):
    """å°‡æ•¸å­—è½‰æ›å›å¹´å­£å­—ä¸²"""
    try:
        if season_num <= 0:
            return ""
        year = season_num // 10
        season = season_num % 10
        return f"{year:03d}Y{season}S"
    except:
        return ""

def get_season_sequence(start_season, end_season):
    """ç²å–å¾é–‹å§‹åˆ°çµæŸçš„æ‰€æœ‰å¹´å­£åºåˆ—"""
    seasons = []
    current = start_season
    start_num = season_to_number(start_season)
    end_num = season_to_number(end_season)
    
    current_num = start_num
    while current_num <= end_num:
        seasons.append(number_to_season(current_num))
        year = current_num // 10
        season = current_num % 10
        if season == 4:
            current_num = (year + 1) * 10 + 1
        else:
            current_num = year * 10 + (season + 1)
        if len(seasons) > 100:  # é˜²æ­¢ç„¡é™è¿´åœˆ
            break
    
    return seasons

def calculate_quarter_days(season_str):
    """è¨ˆç®—è©²å­£åº¦çš„å¤©æ•¸"""
    try:
        if not season_str or pd.isna(season_str):
            return 90  # é è¨­å€¼
        
        year_part = season_str.split('Y')[0]
        season_part = season_str.split('Y')[1].replace('S', '')
        
        year = int(year_part) + 1911  # è½‰æ›ç‚ºè¥¿å…ƒå¹´
        season = int(season_part)
        
        # è¨ˆç®—å„å­£åº¦å¤©æ•¸
        quarter_days = {
            1: 90,  # Q1: 1-3æœˆ
            2: 91,  # Q2: 4-6æœˆ  
            3: 92,  # Q3: 7-9æœˆ
            4: 92   # Q4: 10-12æœˆ
        }
        
        # é–å¹´èª¿æ•´
        if season == 1 and year % 4 == 0 and (year % 100 != 0 or year % 400 == 0):
            return 91  # é–å¹´Q1å¤šä¸€å¤©
        
        return quarter_days.get(season, 90)
    except:
        return 90

print("âœ… å·¥å…·å‡½æ•¸æº–å‚™å®Œæˆ")

# %% [markdown]
# ## 4. åŸºæœ¬è³‡è¨Šæ•´åˆ (7æ¬„ä½)

# %%
# åŸºæœ¬è³‡è¨Šæ•´åˆé‚è¼¯
print("ğŸ¢ åŸºæœ¬è³‡è¨Šæ•´åˆè™•ç†")
print("=" * 50)

def integrate_basic_info(project_code, target_season, integration_data, original_project_data):
    """
    æ•´åˆåŸºæœ¬è³‡è¨Š (7æ¬„ä½)
    
    Args:
        project_code: å»ºæ¡ˆç·¨è™Ÿ
        target_season: ç›®æ¨™å¹´å­£
        integration_data: å»ºæ¡ˆæ•´åˆè³‡æ–™
        original_project_data: åŸå§‹å»ºæ¡ˆè³‡æ–™
        
    Returns:
        dict: åŸºæœ¬è³‡è¨Š
    """
    
    basic_info = {
        'project_code': project_code,
        'project_name': '',
        'county': '',
        'district': '',
        'street_address': '',
        'total_units': 0,
        'sales_start_season': '',
        'target_season': target_season
    }
    
    try:
        # å¾å»ºæ¡ˆæ•´åˆè³‡æ–™å–å¾—è³‡è¨Š
        project_row = integration_data[integration_data['project_code'] == project_code]
        
        if not project_row.empty:
            project_info = project_row.iloc[0]
            
            basic_info.update({
                'project_name': project_info.get('ç¤¾å€åç¨±', project_info.get('project_name', '')),
                'county': project_info.get('ç¸£å¸‚', project_info.get('county', '')),
                'district': project_info.get('è¡Œæ”¿å€', project_info.get('district', '')),
                'street_address': project_info.get('åè½è¡—é“', project_info.get('street_address', '')),
                'total_units': int(project_info.get('æˆ¶æ•¸', project_info.get('total_units', 0))),
                'sales_start_season': project_info.get('éŠ·å”®èµ·å§‹å¹´å­£', project_info.get('sales_start_season', ''))
            })
        
        # å¦‚æœæ•´åˆè³‡æ–™ä¸å®Œæ•´ï¼Œå¾åŸå§‹å»ºæ¡ˆè³‡æ–™è£œå……
        if not basic_info['project_name'] or not basic_info['total_units']:
            original_row = original_project_data[original_project_data['ç·¨è™Ÿ'] == project_code]
            
            if not original_row.empty:
                orig_info = original_row.iloc[0]
                
                if not basic_info['project_name']:
                    basic_info['project_name'] = orig_info.get('ç¤¾å€åç¨±', '')
                
                if not basic_info['total_units']:
                    basic_info['total_units'] = int(orig_info.get('æˆ¶æ•¸', 0))
                
                if not basic_info['street_address']:
                    basic_info['street_address'] = orig_info.get('åè½è¡—é“', '')
                
                if not basic_info['sales_start_season']:
                    start_time = orig_info.get('éŠ·å”®èµ·å§‹æ™‚é–“', '')
                    if start_time:
                        # è½‰æ›éŠ·å”®èµ·å§‹æ™‚é–“ç‚ºå¹´å­£
                        try:
                            if len(str(start_time)) == 7:  # æ°‘åœ‹å¹´æ ¼å¼
                                year = int(str(start_time)[:3])
                                month = int(str(start_time)[3:5])
                                season = (month - 1) // 3 + 1
                                basic_info['sales_start_season'] = f"{year:03d}Y{season}S"
                        except:
                            pass
        
        # è³‡æ–™å“è³ªæª¢æŸ¥èˆ‡ä¿®æ­£
        if basic_info['total_units'] <= 0:
            basic_info['total_units'] = 50  # é è¨­å€¼
        
        # ç¢ºä¿å­—ä¸²æ¬„ä½ä¸ç‚ºNone
        for field in ['project_name', 'county', 'district', 'street_address', 'sales_start_season']:
            if pd.isna(basic_info[field]) or basic_info[field] is None:
                basic_info[field] = ''
    
    except Exception as e:
        print(f"âŒ åŸºæœ¬è³‡è¨Šæ•´åˆéŒ¯èª¤ {project_code}: {e}")
    
    return basic_info

# %%
# æ‰¹é‡è™•ç†åŸºæœ¬è³‡è¨Šæ•´åˆ
print("ğŸ”„ æ‰¹é‡è™•ç†åŸºæœ¬è³‡è¨Šæ•´åˆ...")

# ç²å–æ‰€æœ‰éœ€è¦è™•ç†çš„å»ºæ¡ˆ-å¹´å­£çµ„åˆ
project_seasons = []

# å¾å»åŒ–ç‡åˆ†æçµæœå–å¾—ä¸»è¦æ¸…å–®
if not absorption_analysis.empty:
    valid_absorption = absorption_analysis[absorption_analysis['calculation_status'] == 'success']
    for _, row in valid_absorption.iterrows():
        project_seasons.append({
            'project_code': row['project_code'],
            'target_season': row['target_season']
        })

# å»é‡
project_seasons_df = pd.DataFrame(project_seasons).drop_duplicates()

print(f"âœ… æ‰¾åˆ° {len(project_seasons_df)} å€‹å»ºæ¡ˆ-å¹´å­£çµ„åˆéœ€è¦è™•ç†")

# æ‰¹é‡æ•´åˆåŸºæœ¬è³‡è¨Š
basic_info_results = []

for _, row in project_seasons_df.iterrows():
    basic_info = integrate_basic_info(
        row['project_code'],
        row['target_season'],
        project_integration,
        original_projects
    )
    basic_info_results.append(basic_info)

basic_info_df = pd.DataFrame(basic_info_results)

print(f"âœ… å®Œæˆ {len(basic_info_df)} ç­†åŸºæœ¬è³‡è¨Šæ•´åˆ")

# %%
# åŸºæœ¬è³‡è¨Šå“è³ªæª¢æŸ¥
print(f"\nğŸ“Š åŸºæœ¬è³‡è¨Šå“è³ªæª¢æŸ¥:")

if not basic_info_df.empty:
    print(f"è³‡æ–™å®Œæ•´æ€§çµ±è¨ˆ:")
    print(f"   ç¸½è¨˜éŒ„æ•¸: {len(basic_info_df):,}")
    print(f"   æœ‰å»ºæ¡ˆåç¨±: {len(basic_info_df[basic_info_df['project_name'] != '']):,}")
    print(f"   æœ‰ç¸£å¸‚è³‡è¨Š: {len(basic_info_df[basic_info_df['county'] != '']):,}")
    print(f"   æœ‰è¡Œæ”¿å€è³‡è¨Š: {len(basic_info_df[basic_info_df['district'] != '']):,}")
    print(f"   æœ‰è¡—é“åœ°å€: {len(basic_info_df[basic_info_df['street_address'] != '']):,}")
    print(f"   æœ‰ç¸½æˆ¶æ•¸: {len(basic_info_df[basic_info_df['total_units'] > 0]):,}")
    print(f"   æœ‰éŠ·å”®èµ·å§‹å­£: {len(basic_info_df[basic_info_df['sales_start_season'] != '']):,}")
    
    # ç¸£å¸‚åˆ†å¸ƒ
    if 'county' in basic_info_df.columns:
        county_dist = basic_info_df[basic_info_df['county'] != '']['county'].value_counts()
        print(f"\nç¸£å¸‚åˆ†å¸ƒ (å‰8å):")
        for county, count in county_dist.head(8).items():
            percentage = count / len(basic_info_df) * 100
            print(f"   {county}: {count:,} å€‹ ({percentage:.1f}%)")
    
    # ç¸½æˆ¶æ•¸çµ±è¨ˆ
    valid_units = basic_info_df[basic_info_df['total_units'] > 0]
    if not valid_units.empty:
        print(f"\nç¸½æˆ¶æ•¸çµ±è¨ˆ:")
        print(f"   å¹³å‡æˆ¶æ•¸: {valid_units['total_units'].mean():.1f}")
        print(f"   ä¸­ä½æ•¸æˆ¶æ•¸: {valid_units['total_units'].median():.1f}")
        print(f"   æœ€å¤§æˆ¶æ•¸: {valid_units['total_units'].max()}")
        print(f"   æœ€å°æˆ¶æ•¸: {valid_units['total_units'].min()}")

# %% [markdown]
# ## 5. æ™‚é–“èˆ‡æ•¸é‡è¨ˆç®— (5æ¬„ä½)

# %%
# æ™‚é–“èˆ‡æ•¸é‡è¨ˆç®—é‚è¼¯
print("â° æ™‚é–“èˆ‡æ•¸é‡è¨ˆç®—è™•ç†")
print("=" * 50)

def calculate_time_quantity_metrics(project_code, target_season, basic_info, clean_transaction_data, 
                                  cancellation_data, dedup_data):
    """
    è¨ˆç®—æ™‚é–“èˆ‡æ•¸é‡æŒ‡æ¨™ (5æ¬„ä½)
    
    Args:
        project_code: å»ºæ¡ˆç·¨è™Ÿ
        target_season: ç›®æ¨™å¹´å­£
        basic_info: åŸºæœ¬è³‡è¨Š
        clean_transaction_data: ä¹¾æ·¨äº¤æ˜“è³‡æ–™
        cancellation_data: è§£ç´„è³‡æ–™
        dedup_data: å»é‡è³‡æ–™
        
    Returns:
        dict: æ™‚é–“èˆ‡æ•¸é‡æŒ‡æ¨™
    """
    
    metrics = {
        'project_code': project_code,
        'target_season': target_season,
        'sales_seasons': 0,
        'cumulative_transactions': 0,
        'quarterly_transactions': 0,
        'quarterly_sales_days': 0
    }
    
    try:
        # è¨ˆç®—éŠ·å”®å­£æ•¸
        sales_start_season = basic_info.get('sales_start_season', '')
        if sales_start_season:
            start_num = season_to_number(sales_start_season)
            target_num = season_to_number(target_season)
            if target_num >= start_num:
                seasons_list = get_season_sequence(sales_start_season, target_season)
                metrics['sales_seasons'] = len(seasons_list)
            else:
                metrics['sales_seasons'] = 1  # è‡³å°‘1å­£
        else:
            metrics['sales_seasons'] = 1  # é è¨­å€¼
        
        # å¾ä¹¾æ·¨äº¤æ˜“è³‡æ–™è¨ˆç®—æˆäº¤ç­†æ•¸
        project_transactions = clean_transaction_data[
            clean_transaction_data['å‚™æŸ¥ç·¨è™Ÿ'] == project_code
        ]
        
        if not project_transactions.empty:
            # ç´¯ç©æˆäº¤ç­†æ•¸ï¼ˆåˆ°ç›®æ¨™å¹´å­£ç‚ºæ­¢ï¼‰
            cumulative_transactions = project_transactions[
                project_transactions['äº¤æ˜“å¹´å­£'] <= target_season
            ]
            metrics['cumulative_transactions'] = len(cumulative_transactions)
            
            # è©²å­£æˆäº¤ç­†æ•¸
            quarterly_transactions = project_transactions[
                project_transactions['äº¤æ˜“å¹´å­£'] == target_season
            ]
            metrics['quarterly_transactions'] = len(quarterly_transactions)
        
        # è¨ˆç®—è©²å­£éŠ·å”®å¤©æ•¸
        metrics['quarterly_sales_days'] = calculate_quarter_days(target_season)
        
        # å¦‚æœæ˜¯éŠ·å”®èµ·å§‹å­£ï¼Œå¯èƒ½ä¸æ˜¯å®Œæ•´å­£åº¦
        if sales_start_season == target_season:
            # é€™è£¡å¯ä»¥é€²ä¸€æ­¥å„ªåŒ–ï¼Œè¨ˆç®—å¯¦éš›éŠ·å”®å¤©æ•¸
            # æš«æ™‚ä½¿ç”¨å®Œæ•´å­£åº¦å¤©æ•¸
            pass
    
    except Exception as e:
        print(f"âŒ æ™‚é–“æ•¸é‡è¨ˆç®—éŒ¯èª¤ {project_code}: {e}")
    
    return metrics

# %%
# æ‰¹é‡è¨ˆç®—æ™‚é–“èˆ‡æ•¸é‡æŒ‡æ¨™
print("ğŸ”„ æ‰¹é‡è¨ˆç®—æ™‚é–“èˆ‡æ•¸é‡æŒ‡æ¨™...")

time_quantity_results = []

for _, basic_row in basic_info_df.iterrows():
    metrics = calculate_time_quantity_metrics(
        basic_row['project_code'],
        basic_row['target_season'],
        basic_row.to_dict(),
        clean_transactions,
        cancellation_analysis,
        deduplication_results
    )
    time_quantity_results.append(metrics)

time_quantity_df = pd.DataFrame(time_quantity_results)

print(f"âœ… å®Œæˆ {len(time_quantity_df)} ç­†æ™‚é–“èˆ‡æ•¸é‡æŒ‡æ¨™è¨ˆç®—")

# %%
# æ™‚é–“èˆ‡æ•¸é‡æŒ‡æ¨™çµ±è¨ˆ
print(f"\nğŸ“Š æ™‚é–“èˆ‡æ•¸é‡æŒ‡æ¨™çµ±è¨ˆ:")

if not time_quantity_df.empty:
    print(f"éŠ·å”®å­£æ•¸çµ±è¨ˆ:")
    print(f"   å¹³å‡éŠ·å”®å­£æ•¸: {time_quantity_df['sales_seasons'].mean():.1f}")
    print(f"   ä¸­ä½æ•¸éŠ·å”®å­£æ•¸: {time_quantity_df['sales_seasons'].median():.1f}")
    print(f"   æœ€é•·éŠ·å”®å­£æ•¸: {time_quantity_df['sales_seasons'].max()}")
    print(f"   æœ€çŸ­éŠ·å”®å­£æ•¸: {time_quantity_df['sales_seasons'].min()}")
    
    print(f"\næˆäº¤ç­†æ•¸çµ±è¨ˆ:")
    print(f"   å¹³å‡ç´¯ç©æˆäº¤: {time_quantity_df['cumulative_transactions'].mean():.1f}")
    print(f"   å¹³å‡å­£åº¦æˆäº¤: {time_quantity_df['quarterly_transactions'].mean():.1f}")
    print(f"   æœ€å¤§ç´¯ç©æˆäº¤: {time_quantity_df['cumulative_transactions'].max()}")
    
    # éŠ·å”®å­£æ•¸åˆ†å¸ƒ
    seasons_dist = time_quantity_df['sales_seasons'].value_counts().sort_index()
    print(f"\néŠ·å”®å­£æ•¸åˆ†å¸ƒ (å‰10å):")
    for seasons, count in seasons_dist.head(10).items():
        percentage = count / len(time_quantity_df) * 100
        print(f"   {seasons}å­£: {count:,} å€‹ ({percentage:.1f}%)")
    
    # æœ‰æˆäº¤è¨˜éŒ„çš„æ¯”ä¾‹
    with_transactions = len(time_quantity_df[time_quantity_df['cumulative_transactions'] > 0])
    print(f"\næœ‰æˆäº¤è¨˜éŒ„: {with_transactions:,} å€‹ ({with_transactions/len(time_quantity_df)*100:.1f}%)")

# %% [markdown]
# ## 6. è§£ç´„è³‡è¨Šçµ±è¨ˆ (6æ¬„ä½)

# %%
# è§£ç´„è³‡è¨Šçµ±è¨ˆé‚è¼¯
print("âš ï¸ è§£ç´„è³‡è¨Šçµ±è¨ˆè™•ç†")
print("=" * 50)

def calculate_cancellation_metrics(project_code, target_season, cancellation_risk_data, 
                                 cancellation_analysis_data, time_quantity_data):
    """
    è¨ˆç®—è§£ç´„è³‡è¨ŠæŒ‡æ¨™ (6æ¬„ä½)
    
    Args:
        project_code: å»ºæ¡ˆç·¨è™Ÿ
        target_season: ç›®æ¨™å¹´å­£
        cancellation_risk_data: è§£ç´„é¢¨éšªè©•ä¼°è³‡æ–™
        cancellation_analysis_data: è§£ç´„åˆ†æè³‡æ–™
        time_quantity_data: æ™‚é–“æ•¸é‡è³‡æ–™
        
    Returns:
        dict: è§£ç´„è³‡è¨ŠæŒ‡æ¨™
    """
    
    metrics = {
        'project_code': project_code,
        'target_season': target_season,
        'cumulative_cancellations': 0,
        'quarterly_cancellations': 0,
        'quarterly_cancellation_rate': 0.0,
        'cumulative_cancellation_rate': 0.0,
        'latest_cancellation_season': '',
        'consecutive_no_cancellation_seasons': 0
    }
    
    try:
        # å¾è§£ç´„é¢¨éšªè©•ä¼°è³‡æ–™å–å¾—æŒ‡æ¨™
        risk_row = cancellation_risk_data[
            (cancellation_risk_data['project_code'] == project_code) &
            (cancellation_risk_data['target_season'] == target_season) &
            (cancellation_risk_data['calculation_status'] == 'success')
        ]
        
        if not risk_row.empty:
            risk_info = risk_row.iloc[0]
            
            metrics.update({
                'cumulative_cancellations': int(risk_info.get('cumulative_cancellation_count', 0)),
                'quarterly_cancellations': int(risk_info.get('quarterly_cancellation_count', 0)),
                'quarterly_cancellation_rate': float(risk_info.get('quarterly_cancellation_rate', 0)),
                'cumulative_cancellation_rate': float(risk_info.get('cumulative_cancellation_rate', 0)),
                'latest_cancellation_season': str(risk_info.get('latest_cancellation_season', '')),
                'consecutive_no_cancellation_seasons': int(risk_info.get('consecutive_no_cancellation_seasons', 0))
            })
        
        else:
            # å¦‚æœæ²’æœ‰é¢¨éšªè©•ä¼°è³‡æ–™ï¼Œå¾åŸå§‹è§£ç´„åˆ†æè¨ˆç®—
            project_cancellations = cancellation_analysis_data[
                cancellation_analysis_data['å‚™æŸ¥ç·¨è™Ÿ'] == project_code
            ]
            
            if not project_cancellations.empty:
                # è¨ˆç®—ç´¯ç©è§£ç´„æ•¸
                if 'æ˜¯å¦è§£ç´„' in project_cancellations.columns:
                    total_cancellations = len(project_cancellations[project_cancellations['æ˜¯å¦è§£ç´„'] == True])
                else:
                    # æ ¹æ“šè§£ç´„æƒ…å½¢æ¬„ä½åˆ¤æ–·
                    total_cancellations = len(project_cancellations[
                        project_cancellations['è§£ç´„æƒ…å½¢'].notna() & 
                        project_cancellations['è§£ç´„æƒ…å½¢'].str.contains('è§£ç´„', na=False)
                    ])
                
                metrics['cumulative_cancellations'] = total_cancellations
                
                # è¨ˆç®—ç´¯ç©è§£ç´„ç‡
                total_transactions = len(project_cancellations)
                if total_transactions > 0:
                    metrics['cumulative_cancellation_rate'] = (total_cancellations / total_transactions) * 100
                
                # è¨ˆç®—è©²å­£è§£ç´„æ•¸
                if 'äº¤æ˜“å¹´å­£' in project_cancellations.columns:
                    season_cancellations = project_cancellations[
                        (project_cancellations['äº¤æ˜“å¹´å­£'] == target_season) &
                        (project_cancellations['æ˜¯å¦è§£ç´„'] == True)
                    ]
                    metrics['quarterly_cancellations'] = len(season_cancellations)
                    
                    # è¨ˆç®—è©²å­£è§£ç´„ç‡
                    season_total = len(project_cancellations[
                        project_cancellations['äº¤æ˜“å¹´å­£'] == target_season
                    ])
                    if season_total > 0:
                        metrics['quarterly_cancellation_rate'] = (len(season_cancellations) / season_total) * 100
        
        # ç¢ºä¿æ•¸å€¼åœ¨åˆç†ç¯„åœå…§
        metrics['quarterly_cancellation_rate'] = min(100, max(0, metrics['quarterly_cancellation_rate']))
        metrics['cumulative_cancellation_rate'] = min(100, max(0, metrics['cumulative_cancellation_rate']))
        
        # è™•ç†å­—ä¸²æ¬„ä½
        if pd.isna(metrics['latest_cancellation_season']):
            metrics['latest_cancellation_season'] = ''
    
    except Exception as e:
        print(f"âŒ è§£ç´„æŒ‡æ¨™è¨ˆç®—éŒ¯èª¤ {project_code}: {e}")
    
    return metrics

# %%
# æ‰¹é‡è¨ˆç®—è§£ç´„è³‡è¨ŠæŒ‡æ¨™
print("ğŸ”„ æ‰¹é‡è¨ˆç®—è§£ç´„è³‡è¨ŠæŒ‡æ¨™...")

cancellation_metrics_results = []

for _, basic_row in basic_info_df.iterrows():
    metrics = calculate_cancellation_metrics(
        basic_row['project_code'],
        basic_row['target_season'],
        cancellation_risk,
        cancellation_analysis,
        time_quantity_df
    )
    cancellation_metrics_results.append(metrics)

cancellation_metrics_df = pd.DataFrame(cancellation_metrics_results)

print(f"âœ… å®Œæˆ {len(cancellation_metrics_df)} ç­†è§£ç´„è³‡è¨ŠæŒ‡æ¨™è¨ˆç®—")

# %%
# è§£ç´„è³‡è¨Šçµ±è¨ˆ
print(f"\nğŸ“Š è§£ç´„è³‡è¨Šçµ±è¨ˆ:")

if not cancellation_metrics_df.empty:
    print(f"è§£ç´„ç­†æ•¸çµ±è¨ˆ:")
    total_projects = len(cancellation_metrics_df)
    projects_with_cancellations = len(cancellation_metrics_df[cancellation_metrics_df['cumulative_cancellations'] > 0])
    
    print(f"   ç¸½å»ºæ¡ˆæ•¸: {total_projects:,}")
    print(f"   æœ‰è§£ç´„è¨˜éŒ„: {projects_with_cancellations:,} å€‹ ({projects_with_cancellations/total_projects*100:.1f}%)")
    print(f"   å¹³å‡ç´¯ç©è§£ç´„æ•¸: {cancellation_metrics_df['cumulative_cancellations'].mean():.1f}")
    print(f"   å¹³å‡ç´¯ç©è§£ç´„ç‡: {cancellation_metrics_df['cumulative_cancellation_rate'].mean():.2f}%")
    
    if projects_with_cancellations > 0:
        with_cancellations = cancellation_metrics_df[cancellation_metrics_df['cumulative_cancellations'] > 0]
        print(f"\næœ‰è§£ç´„å»ºæ¡ˆçµ±è¨ˆ:")
        print(f"   å¹³å‡è§£ç´„æ•¸: {with_cancellations['cumulative_cancellations'].mean():.1f}")
        print(f"   å¹³å‡è§£ç´„ç‡: {with_cancellations['cumulative_cancellation_rate'].mean():.2f}%")
        print(f"   æœ€é«˜è§£ç´„æ•¸: {with_cancellations['cumulative_cancellations'].max()}")
        print(f"   æœ€é«˜è§£ç´„ç‡: {with_cancellations['cumulative_cancellation_rate'].max():.2f}%")
    
    # è§£ç´„ç‡åˆ†å¸ƒ
    rate_ranges = [
        (0, 1, "0-1%"),
        (1, 3, "1-3%"), 
        (3, 5, "3-5%"),
        (5, 10, "5-10%"),
        (10, float('inf'), ">10%")
    ]
    
    print(f"\nç´¯ç©è§£ç´„ç‡åˆ†å¸ƒ:")
    for min_rate, max_rate, label in rate_ranges:
        count = len(cancellation_metrics_df[
            (cancellation_metrics_df['cumulative_cancellation_rate'] >= min_rate) &
            (cancellation_metrics_df['cumulative_cancellation_rate'] < max_rate)
        ])
        percentage = count / total_projects * 100
        print(f"   {label}: {count:,} å€‹ ({percentage:.1f}%)")

# %% [markdown]
# ## 7. å»åŒ–åˆ†ææ•´åˆ (3æ¬„ä½)

# %%
# å»åŒ–åˆ†ææ•´åˆé‚è¼¯
print("ğŸ“ˆ å»åŒ–åˆ†ææ•´åˆè™•ç†")
print("=" * 50)

def integrate_absorption_analysis(project_code, target_season, absorption_data):
    """
    æ•´åˆå»åŒ–åˆ†ææŒ‡æ¨™ (3æ¬„ä½)
    
    Args:
        project_code: å»ºæ¡ˆç·¨è™Ÿ
        target_season: ç›®æ¨™å¹´å­£
        absorption_data: å»åŒ–ç‡åˆ†æè³‡æ–™
        
    Returns:
        dict: å»åŒ–åˆ†ææŒ‡æ¨™
    """
    
    metrics = {
        'project_code': project_code,
        'target_season': target_season,
        'gross_absorption_rate': 0.0,
        'net_absorption_rate': 0.0,
        'adjusted_absorption_rate': 0.0
    }
    
    try:
        # å¾å»åŒ–ç‡åˆ†æè³‡æ–™å–å¾—æŒ‡æ¨™
        absorption_row = absorption_data[
            (absorption_data['project_code'] == project_code) &
            (absorption_data['target_season'] == target_season) &
            (absorption_data['calculation_status'] == 'success')
        ]
        
        if not absorption_row.empty:
            absorption_info = absorption_row.iloc[0]
            
            metrics.update({
                'gross_absorption_rate': float(absorption_info.get('gross_absorption_rate', 0)),
                'net_absorption_rate': float(absorption_info.get('net_absorption_rate', 0)),
                'adjusted_absorption_rate': float(absorption_info.get('adjusted_absorption_rate', 0))
            })
        
        # ç¢ºä¿æ•¸å€¼åœ¨åˆç†ç¯„åœå…§ (0-100%)
        for key in ['gross_absorption_rate', 'net_absorption_rate', 'adjusted_absorption_rate']:
            metrics[key] = min(100, max(0, metrics[key]))
    
    except Exception as e:
        print(f"âŒ å»åŒ–åˆ†ææ•´åˆéŒ¯èª¤ {project_code}: {e}")
    
    return metrics

# %%
# æ‰¹é‡æ•´åˆå»åŒ–åˆ†ææŒ‡æ¨™
print("ğŸ”„ æ‰¹é‡æ•´åˆå»åŒ–åˆ†ææŒ‡æ¨™...")

absorption_metrics_results = []

for _, basic_row in basic_info_df.iterrows():
    metrics = integrate_absorption_analysis(
        basic_row['project_code'],
        basic_row['target_season'],
        absorption_analysis
    )
    absorption_metrics_results.append(metrics)

absorption_metrics_df = pd.DataFrame(absorption_metrics_results)

print(f"âœ… å®Œæˆ {len(absorption_metrics_df)} ç­†å»åŒ–åˆ†ææŒ‡æ¨™æ•´åˆ")

# %%
# å»åŒ–åˆ†æçµ±è¨ˆ
print(f"\nğŸ“Š å»åŒ–åˆ†æçµ±è¨ˆ:")

if not absorption_metrics_df.empty:
    print(f"å»åŒ–ç‡çµ±è¨ˆ:")
    print(f"   å¹³å‡æ¯›å»åŒ–ç‡: {absorption_metrics_df['gross_absorption_rate'].mean():.1f}%")
    print(f"   å¹³å‡æ·¨å»åŒ–ç‡: {absorption_metrics_df['net_absorption_rate'].mean():.1f}%")
    print(f"   å¹³å‡èª¿æ•´å»åŒ–ç‡: {absorption_metrics_df['adjusted_absorption_rate'].mean():.1f}%")
    
    print(f"\nå»åŒ–ç‡åˆ†å¸ƒ (æ·¨å»åŒ–ç‡):")
    rate_ranges = [
        (0, 30, "0-30%"),
        (30, 50, "30-50%"),
        (50, 70, "50-70%"),
        (70, 90, "70-90%"),
        (90, 100, "90-100%"),
        (100, float('inf'), ">100%")
    ]
    
    total_projects = len(absorption_metrics_df)
    for min_rate, max_rate, label in rate_ranges:
        count = len(absorption_metrics_df[
            (absorption_metrics_df['net_absorption_rate'] >= min_rate) &
            (absorption_metrics_df['net_absorption_rate'] < max_rate)
        ])
        percentage = count / total_projects * 100
        print(f"   {label}: {count:,} å€‹ ({percentage:.1f}%)")
    
    # å®Œå”®å»ºæ¡ˆçµ±è¨ˆ
    completed_projects = len(absorption_metrics_df[absorption_metrics_df['net_absorption_rate'] >= 100])
    print(f"\nå®Œå”®å»ºæ¡ˆ: {completed_projects:,} å€‹ ({completed_projects/total_projects*100:.1f}%)")
    
    # å»åŒ–ç‡å·®ç•°åˆ†æ
    absorption_metrics_df['gross_net_diff'] = absorption_metrics_df['gross_absorption_rate'] - absorption_metrics_df['net_absorption_rate']
    print(f"\næ¯›æ·¨å»åŒ–ç‡å·®ç•°:")
    print(f"   å¹³å‡å·®ç•°: {absorption_metrics_df['gross_net_diff'].mean():.2f}%")
    print(f"   æœ€å¤§å·®ç•°: {absorption_metrics_df['gross_net_diff'].max():.2f}%")

# %% [markdown]
# ## 8. å»åŒ–å‹•æ…‹æ•´åˆ (4æ¬„ä½)

# %%
# å»åŒ–å‹•æ…‹æ•´åˆé‚è¼¯
print("ğŸš€ å»åŒ–å‹•æ…‹æ•´åˆè™•ç†")
print("=" * 50)

def integrate_absorption_dynamics(project_code, target_season, speed_data, acceleration_data, 
                                prediction_data, efficiency_data):
    """
    æ•´åˆå»åŒ–å‹•æ…‹æŒ‡æ¨™ (4æ¬„ä½)
    
    Args:
        project_code: å»ºæ¡ˆç·¨è™Ÿ
        target_season: ç›®æ¨™å¹´å­£
        speed_data: å»åŒ–é€Ÿåº¦è³‡æ–™
        acceleration_data: å»åŒ–åŠ é€Ÿåº¦è³‡æ–™
        prediction_data: å®Œå”®é æ¸¬è³‡æ–™
        efficiency_data: æ•ˆç‡è©•ç´šè³‡æ–™
        
    Returns:
        dict: å»åŒ–å‹•æ…‹æŒ‡æ¨™
    """
    
    metrics = {
        'project_code': project_code,
        'target_season': target_season,
        'quarterly_absorption_speed': 0.0,
        'absorption_acceleration': 0.0,
        'estimated_completion_seasons': 0,
        'absorption_efficiency_grade': ''
    }
    
    try:
        # 1. å­£åº¦å»åŒ–é€Ÿåº¦
        speed_row = speed_data[
            (speed_data['project_code'] == project_code) &
            (speed_data['target_season'] == target_season) &
            (speed_data['calculation_status'] == 'success')
        ]
        
        if not speed_row.empty:
            speed_info = speed_row.iloc[0]
            metrics['quarterly_absorption_speed'] = float(speed_info.get('quarterly_absorption_speed', 0))
        
        # 2. å»åŒ–åŠ é€Ÿåº¦
        acceleration_row = acceleration_data[
            (acceleration_data['project_code'] == project_code) &
            (acceleration_data['target_season'] == target_season) &
            (acceleration_data['calculation_status'] == 'success')
        ]
        
        if not acceleration_row.empty:
            accel_info = acceleration_row.iloc[0]
            metrics['absorption_acceleration'] = float(accel_info.get('absorption_acceleration', 0))
        
        # 3. é ä¼°å®Œå”®å­£æ•¸
        prediction_row = prediction_data[
            (prediction_data['project_code'] == project_code) &
            (prediction_data['target_season'] == target_season) &
            (prediction_data['calculation_status'] == 'success')
        ]
        
        if not prediction_row.empty:
            pred_info = prediction_row.iloc[0]
            estimated_seasons = pred_info.get('estimated_seasons_to_completion', 0)
            # è™•ç†ç‰¹æ®Šå€¼
            if estimated_seasons == 999 or estimated_seasons < 0:
                metrics['estimated_completion_seasons'] = 999  # ç„¡æ³•é ä¼°
            else:
                metrics['estimated_completion_seasons'] = int(estimated_seasons)
        
        # 4. å»åŒ–æ•ˆç‡è©•ç´š
        efficiency_row = efficiency_data[
            (efficiency_data['project_code'] == project_code) &
            (efficiency_data['target_season'] == target_season) &
            (efficiency_data['calculation_status'] == 'success')
        ]
        
        if not efficiency_row.empty:
            eff_info = efficiency_row.iloc[0]
            grade_emoji = eff_info.get('grade_emoji', '')
            grade_desc = eff_info.get('grade_description', '')
            metrics['absorption_efficiency_grade'] = f"{grade_emoji} {grade_desc}".strip()
        
        # ç¢ºä¿æ•¸å€¼ç¯„åœåˆç†
        metrics['quarterly_absorption_speed'] = max(0, metrics['quarterly_absorption_speed'])
        metrics['absorption_acceleration'] = max(-100, min(500, metrics['absorption_acceleration']))  # é™åˆ¶åœ¨åˆç†ç¯„åœ
        
        # è™•ç†å­—ä¸²æ¬„ä½
        if pd.isna(metrics['absorption_efficiency_grade']):
            metrics['absorption_efficiency_grade'] = ''
    
    except Exception as e:
        print(f"âŒ å»åŒ–å‹•æ…‹æ•´åˆéŒ¯èª¤ {project_code}: {e}")
    
    return metrics

# %%
# æ‰¹é‡æ•´åˆå»åŒ–å‹•æ…‹æŒ‡æ¨™
print("ğŸ”„ æ‰¹é‡æ•´åˆå»åŒ–å‹•æ…‹æŒ‡æ¨™...")

absorption_dynamics_results = []

for _, basic_row in basic_info_df.iterrows():
    metrics = integrate_absorption_dynamics(
        basic_row['project_code'],
        basic_row['target_season'],
        quarterly_speed,
        absorption_acceleration,
        completion_prediction,
        absorption_efficiency
    )
    absorption_dynamics_results.append(metrics)

absorption_dynamics_df = pd.DataFrame(absorption_dynamics_results)

print(f"âœ… å®Œæˆ {len(absorption_dynamics_df)} ç­†å»åŒ–å‹•æ…‹æŒ‡æ¨™æ•´åˆ")

# %%
# å»åŒ–å‹•æ…‹çµ±è¨ˆ
print(f"\nğŸ“Š å»åŒ–å‹•æ…‹çµ±è¨ˆ:")

if not absorption_dynamics_df.empty:
    print(f"å»åŒ–é€Ÿåº¦çµ±è¨ˆ:")
    valid_speeds = absorption_dynamics_df[absorption_dynamics_df['quarterly_absorption_speed'] > 0]
    if not valid_speeds.empty:
        print(f"   å¹³å‡å»åŒ–é€Ÿåº¦: {valid_speeds['quarterly_absorption_speed'].mean():.2f} æˆ¶/å­£")
        print(f"   ä¸­ä½æ•¸å»åŒ–é€Ÿåº¦: {valid_speeds['quarterly_absorption_speed'].median():.2f} æˆ¶/å­£")
        print(f"   æœ€é«˜å»åŒ–é€Ÿåº¦: {valid_speeds['quarterly_absorption_speed'].max():.2f} æˆ¶/å­£")
    
    print(f"\nå»åŒ–åŠ é€Ÿåº¦çµ±è¨ˆ:")
    valid_accel = absorption_dynamics_df[
        (absorption_dynamics_df['absorption_acceleration'] != 0) &
        (absorption_dynamics_df['absorption_acceleration'].between(-100, 500))
    ]
    if not valid_accel.empty:
        print(f"   å¹³å‡åŠ é€Ÿåº¦: {valid_accel['absorption_acceleration'].mean():.1f}%")
        print(f"   åŠ é€Ÿå»ºæ¡ˆæ•¸: {len(valid_accel[valid_accel['absorption_acceleration'] > 0]):,}")
        print(f"   æ¸›é€Ÿå»ºæ¡ˆæ•¸: {len(valid_accel[valid_accel['absorption_acceleration'] < 0]):,}")
    
    print(f"\nå®Œå”®é æ¸¬çµ±è¨ˆ:")
    predictable_projects = absorption_dynamics_df[
        (absorption_dynamics_df['estimated_completion_seasons'] > 0) &
        (absorption_dynamics_df['estimated_completion_seasons'] < 999)
    ]
    if not predictable_projects.empty:
        print(f"   å¯é æ¸¬å®Œå”®: {len(predictable_projects):,} å€‹")
        print(f"   å¹³å‡é ä¼°å­£æ•¸: {predictable_projects['estimated_completion_seasons'].mean():.1f} å­£")
        print(f"   æœ€å¿«å®Œå”®é ä¼°: {predictable_projects['estimated_completion_seasons'].min()} å­£")
    
    unpredictable = len(absorption_dynamics_df[absorption_dynamics_df['estimated_completion_seasons'] >= 999])
    print(f"   ç„¡æ³•é ä¼°: {unpredictable:,} å€‹")
    
    # æ•ˆç‡è©•ç´šåˆ†å¸ƒ
    efficiency_dist = absorption_dynamics_df[
        absorption_dynamics_df['absorption_efficiency_grade'] != ''
    ]['absorption_efficiency_grade'].value_counts()
    
    if not efficiency_dist.empty:
        print(f"\næ•ˆç‡è©•ç´šåˆ†å¸ƒ:")
        for grade, count in efficiency_dist.head(8).items():
            percentage = count / len(absorption_dynamics_df) * 100
            print(f"   {grade}: {count:,} å€‹ ({percentage:.1f}%)")

# %% [markdown]
# ## 9. åƒ¹æ ¼åˆ†æè¨ˆç®— (3æ¬„ä½)

# %%
# åƒ¹æ ¼åˆ†æè¨ˆç®—é‚è¼¯
print("ğŸ’° åƒ¹æ ¼åˆ†æè¨ˆç®—è™•ç†")
print("=" * 50)

def calculate_price_analysis(project_code, target_season, clean_transaction_data):
    """
    è¨ˆç®—åƒ¹æ ¼åˆ†ææŒ‡æ¨™ (3æ¬„ä½)
    
    Args:
        project_code: å»ºæ¡ˆç·¨è™Ÿ
        target_season: ç›®æ¨™å¹´å­£
        clean_transaction_data: ä¹¾æ·¨äº¤æ˜“è³‡æ–™
        
    Returns:
        dict: åƒ¹æ ¼åˆ†ææŒ‡æ¨™
    """
    
    metrics = {
        'project_code': project_code,
        'target_season': target_season,
        'avg_unit_price_per_ping': 0.0,
        'avg_total_area_ping': 0.0,
        'avg_total_price_wan': 0.0
    }
    
    try:
        # ç¯©é¸è©²å»ºæ¡ˆè©²å­£çš„äº¤æ˜“è¨˜éŒ„
        project_transactions = clean_transaction_data[
            (clean_transaction_data['å‚™æŸ¥ç·¨è™Ÿ'] == project_code) &
            (clean_transaction_data['äº¤æ˜“å¹´å­£'] == target_season)
        ]
        
        if not project_transactions.empty:
            # 1. å¹³å‡äº¤æ˜“å–®åƒ¹ (è¬/åª)
            if 'å»ºç‰©å–®åƒ¹' in project_transactions.columns:
                valid_unit_prices = project_transactions['å»ºç‰©å–®åƒ¹'].dropna()
                valid_unit_prices = valid_unit_prices[valid_unit_prices > 0]
                
                if not valid_unit_prices.empty:
                    # è½‰æ›ç‚ºè¬/åª (å‡è¨­åŸå§‹å–®ä½ç‚ºå…ƒ/åª)
                    metrics['avg_unit_price_per_ping'] = valid_unit_prices.mean() / 10000
            
            # 2. å¹³å‡ç¸½é¢ç© (åª)
            area_columns = ['ç¸½é¢ç©_æ•¸å€¼', 'å»ºç‰©é¢ç©', 'ç¸½é¢ç©']
            for col in area_columns:
                if col in project_transactions.columns:
                    valid_areas = project_transactions[col].dropna()
                    valid_areas = valid_areas[valid_areas > 0]
                    
                    if not valid_areas.empty:
                        metrics['avg_total_area_ping'] = valid_areas.mean()
                        break
            
            # 3. å¹³å‡äº¤æ˜“ç¸½åƒ¹ (è¬)
            if 'äº¤æ˜“ç¸½åƒ¹' in project_transactions.columns:
                valid_total_prices = project_transactions['äº¤æ˜“ç¸½åƒ¹'].dropna()
                valid_total_prices = valid_total_prices[valid_total_prices > 0]
                
                if not valid_total_prices.empty:
                    # è½‰æ›ç‚ºè¬å…ƒ (å‡è¨­åŸå§‹å–®ä½ç‚ºå…ƒ)
                    metrics['avg_total_price_wan'] = valid_total_prices.mean() / 10000
        
        # å¦‚æœè©²å­£æ²’æœ‰äº¤æ˜“ï¼Œå˜—è©¦ä½¿ç”¨ç´¯ç©è³‡æ–™
        if (metrics['avg_unit_price_per_ping'] == 0 or 
            metrics['avg_total_area_ping'] == 0 or 
            metrics['avg_total_price_wan'] == 0):
            
            cumulative_transactions = clean_transaction_data[
                (clean_transaction_data['å‚™æŸ¥ç·¨è™Ÿ'] == project_code) &
                (clean_transaction_data['äº¤æ˜“å¹´å­£'] <= target_season)
            ]
            
            if not cumulative_transactions.empty:
                # è£œå……è¨ˆç®—
                if metrics['avg_unit_price_per_ping'] == 0:
                    if 'å»ºç‰©å–®åƒ¹' in cumulative_transactions.columns:
                        valid_prices = cumulative_transactions['å»ºç‰©å–®åƒ¹'].dropna()
                        valid_prices = valid_prices[valid_prices > 0]
                        if not valid_prices.empty:
                            metrics['avg_unit_price_per_ping'] = valid_prices.mean() / 10000
                
                if metrics['avg_total_area_ping'] == 0:
                    for col in area_columns:
                        if col in cumulative_transactions.columns:
                            valid_areas = cumulative_transactions[col].dropna()
                            valid_areas = valid_areas[valid_areas > 0]
                            if not valid_areas.empty:
                                metrics['avg_total_area_ping'] = valid_areas.mean()
                                break
                
                if metrics['avg_total_price_wan'] == 0:
                    if 'äº¤æ˜“ç¸½åƒ¹' in cumulative_transactions.columns:
                        valid_total_prices = cumulative_transactions['äº¤æ˜“ç¸½åƒ¹'].dropna()
                        valid_total_prices = valid_total_prices[valid_total_prices > 0]
                        if not valid_total_prices.empty:
                            metrics['avg_total_price_wan'] = valid_total_prices.mean() / 10000
        
        # æ•¸å€¼åˆç†æ€§æª¢æŸ¥
        if metrics['avg_unit_price_per_ping'] < 0 or metrics['avg_unit_price_per_ping'] > 300:
            metrics['avg_unit_price_per_ping'] = 0.0  # è¶…å‡ºåˆç†ç¯„åœï¼Œè¨­ç‚º0
        
        if metrics['avg_total_area_ping'] < 0 or metrics['avg_total_area_ping'] > 200:
            metrics['avg_total_area_ping'] = 0.0  # è¶…å‡ºåˆç†ç¯„åœï¼Œè¨­ç‚º0
        
        if metrics['avg_total_price_wan'] < 0 or metrics['avg_total_price_wan'] > 50000:
            metrics['avg_total_price_wan'] = 0.0  # è¶…å‡ºåˆç†ç¯„åœï¼Œè¨­ç‚º0
    
    except Exception as e:
        print(f"âŒ åƒ¹æ ¼åˆ†æè¨ˆç®—éŒ¯èª¤ {project_code}: {e}")
    
    return metrics

# %%
# æ‰¹é‡è¨ˆç®—åƒ¹æ ¼åˆ†ææŒ‡æ¨™
print("ğŸ”„ æ‰¹é‡è¨ˆç®—åƒ¹æ ¼åˆ†ææŒ‡æ¨™...")

price_analysis_results = []

for _, basic_row in basic_info_df.iterrows():
    metrics = calculate_price_analysis(
        basic_row['project_code'],
        basic_row['target_season'],
        clean_transactions
    )
    price_analysis_results.append(metrics)

price_analysis_df = pd.DataFrame(price_analysis_results)

print(f"âœ… å®Œæˆ {len(price_analysis_df)} ç­†åƒ¹æ ¼åˆ†ææŒ‡æ¨™è¨ˆç®—")

# %%
# åƒ¹æ ¼åˆ†æçµ±è¨ˆ
print(f"\nğŸ“Š åƒ¹æ ¼åˆ†æçµ±è¨ˆ:")

if not price_analysis_df.empty:
    # å–®åƒ¹çµ±è¨ˆ
    valid_unit_prices = price_analysis_df[price_analysis_df['avg_unit_price_per_ping'] > 0]
    if not valid_unit_prices.empty:
        print(f"å–®åƒ¹çµ±è¨ˆ (è¬/åª):")
        print(f"   æœ‰å–®åƒ¹è³‡æ–™: {len(valid_unit_prices):,} å€‹")
        print(f"   å¹³å‡å–®åƒ¹: {valid_unit_prices['avg_unit_price_per_ping'].mean():.1f} è¬/åª")
        print(f"   ä¸­ä½æ•¸å–®åƒ¹: {valid_unit_prices['avg_unit_price_per_ping'].median():.1f} è¬/åª")
        print(f"   æœ€é«˜å–®åƒ¹: {valid_unit_prices['avg_unit_price_per_ping'].max():.1f} è¬/åª")
        print(f"   æœ€ä½å–®åƒ¹: {valid_unit_prices['avg_unit_price_per_ping'].min():.1f} è¬/åª")
    
    # é¢ç©çµ±è¨ˆ
    valid_areas = price_analysis_df[price_analysis_df['avg_total_area_ping'] > 0]
    if not valid_areas.empty:
        print(f"\né¢ç©çµ±è¨ˆ (åª):")
        print(f"   æœ‰é¢ç©è³‡æ–™: {len(valid_areas):,} å€‹")
        print(f"   å¹³å‡é¢ç©: {valid_areas['avg_total_area_ping'].mean():.1f} åª")
        print(f"   ä¸­ä½æ•¸é¢ç©: {valid_areas['avg_total_area_ping'].median():.1f} åª")
        print(f"   æœ€å¤§é¢ç©: {valid_areas['avg_total_area_ping'].max():.1f} åª")
        print(f"   æœ€å°é¢ç©: {valid_areas['avg_total_area_ping'].min():.1f} åª")
    
    # ç¸½åƒ¹çµ±è¨ˆ
    valid_total_prices = price_analysis_df[price_analysis_df['avg_total_price_wan'] > 0]
    if not valid_total_prices.empty:
        print(f"\nç¸½åƒ¹çµ±è¨ˆ (è¬):")
        print(f"   æœ‰ç¸½åƒ¹è³‡æ–™: {len(valid_total_prices):,} å€‹")
        print(f"   å¹³å‡ç¸½åƒ¹: {valid_total_prices['avg_total_price_wan'].mean():.0f} è¬")
        print(f"   ä¸­ä½æ•¸ç¸½åƒ¹: {valid_total_prices['avg_total_price_wan'].median():.0f} è¬")
        print(f"   æœ€é«˜ç¸½åƒ¹: {valid_total_prices['avg_total_price_wan'].max():.0f} è¬")
        print(f"   æœ€ä½ç¸½åƒ¹: {valid_total_prices['avg_total_price_wan'].min():.0f} è¬")
    
    # åƒ¹æ ¼å€é–“åˆ†å¸ƒ
    if not valid_unit_prices.empty:
        print(f"\nå–®åƒ¹å€é–“åˆ†å¸ƒ:")
        price_ranges = [
            (0, 30, "0-30è¬/åª"),
            (30, 50, "30-50è¬/åª"),
            (50, 70, "50-70è¬/åª"),
            (70, 100, "70-100è¬/åª"),
            (100, float('inf'), ">100è¬/åª")
        ]
        
        for min_price, max_price, label in price_ranges:
            count = len(valid_unit_prices[
                (valid_unit_prices['avg_unit_price_per_ping'] >= min_price) &
                (valid_unit_prices['avg_unit_price_per_ping'] < max_price)
            ])
            percentage = count / len(valid_unit_prices) * 100
            print(f"   {label}: {count:,} å€‹ ({percentage:.1f}%)")

# %% [markdown]
# ## 10. éšæ®µåˆ†ææ•´åˆ (3æ¬„ä½)

# %%
# éšæ®µåˆ†ææ•´åˆé‚è¼¯
print("ğŸ­ éšæ®µåˆ†ææ•´åˆè™•ç†")
print("=" * 50)

def integrate_stage_analysis(project_code, target_season, stage_analysis_data, 
                           stage_performance_data, cancellation_risk_data):
    """
    æ•´åˆéšæ®µåˆ†ææŒ‡æ¨™ (3æ¬„ä½)
    
    Args:
        project_code: å»ºæ¡ˆç·¨è™Ÿ
        target_season: ç›®æ¨™å¹´å­£
        stage_analysis_data: éšæ®µåˆ†æè³‡æ–™
        stage_performance_data: éšæ®µè¡¨ç¾è³‡æ–™
        cancellation_risk_data: è§£ç´„é¢¨éšªè³‡æ–™
        
    Returns:
        dict: éšæ®µåˆ†ææŒ‡æ¨™
    """
    
    metrics = {
        'project_code': project_code,
        'target_season': target_season,
        'sales_stage': '',
        'stage_performance': '',
        'cancellation_warning': ''
    }
    
    try:
        # 1. éŠ·å”®éšæ®µ
        stage_row = stage_analysis_data[
            (stage_analysis_data['project_code'] == project_code) &
            (stage_analysis_data['target_season'] == target_season) &
            (stage_analysis_data['calculation_status'] == 'success')
        ]
        
        if not stage_row.empty:
            stage_info = stage_row.iloc[0]
            metrics['sales_stage'] = str(stage_info.get('sales_stage', ''))
        
        # 2. éšæ®µè¡¨ç¾
        performance_row = stage_performance_data[
            (stage_performance_data['project_code'] == project_code) &
            (stage_performance_data['target_season'] == target_season)
        ]
        
        if not performance_row.empty:
            perf_info = performance_row.iloc[0]
            performance_emoji = perf_info.get('performance_emoji', '')
            performance_level = perf_info.get('stage_performance', '')
            metrics['stage_performance'] = f"{performance_emoji} {performance_level}".strip()
        
        # 3. è§£ç´„è­¦ç¤º
        risk_row = cancellation_risk_data[
            (cancellation_risk_data['project_code'] == project_code) &
            (cancellation_risk_data['target_season'] == target_season) &
            (cancellation_risk_data['calculation_status'] == 'success')
        ]
        
        if not risk_row.empty:
            risk_info = risk_row.iloc[0]
            risk_emoji = risk_info.get('risk_emoji', '')
            risk_level = risk_info.get('cancellation_risk_level', '')
            metrics['cancellation_warning'] = f"{risk_emoji} {risk_level}".strip()
        
        # è™•ç†ç©ºå€¼
        for field in ['sales_stage', 'stage_performance', 'cancellation_warning']:
            if pd.isna(metrics[field]) or metrics[field] is None:
                metrics[field] = ''
    
    except Exception as e:
        print(f"âŒ éšæ®µåˆ†ææ•´åˆéŒ¯èª¤ {project_code}: {e}")
    
    return metrics

# %%
# æ‰¹é‡æ•´åˆéšæ®µåˆ†ææŒ‡æ¨™
print("ğŸ”„ æ‰¹é‡æ•´åˆéšæ®µåˆ†ææŒ‡æ¨™...")

stage_analysis_results = []

for _, basic_row in basic_info_df.iterrows():
    metrics = integrate_stage_analysis(
        basic_row['project_code'],
        basic_row['target_season'],
        sales_stage_analysis,
        stage_performance,
        cancellation_risk
    )
    stage_analysis_results.append(metrics)

stage_analysis_df = pd.DataFrame(stage_analysis_results)

print(f"âœ… å®Œæˆ {len(stage_analysis_df)} ç­†éšæ®µåˆ†ææŒ‡æ¨™æ•´åˆ")

# %%
# éšæ®µåˆ†æçµ±è¨ˆ
print(f"\nğŸ“Š éšæ®µåˆ†æçµ±è¨ˆ:")

if not stage_analysis_df.empty:
    # éŠ·å”®éšæ®µåˆ†å¸ƒ
    stage_dist = stage_analysis_df[stage_analysis_df['sales_stage'] != '']['sales_stage'].value_counts()
    if not stage_dist.empty:
        print(f"éŠ·å”®éšæ®µåˆ†å¸ƒ:")
        total_with_stage = len(stage_analysis_df[stage_analysis_df['sales_stage'] != ''])
        for stage, count in stage_dist.items():
            percentage = count / total_with_stage * 100
            print(f"   {stage}: {count:,} å€‹ ({percentage:.1f}%)")
    
    # éšæ®µè¡¨ç¾åˆ†å¸ƒ
    performance_dist = stage_analysis_df[stage_analysis_df['stage_performance'] != '']['stage_performance'].value_counts()
    if not performance_dist.empty:
        print(f"\néšæ®µè¡¨ç¾åˆ†å¸ƒ:")
        total_with_performance = len(stage_analysis_df[stage_analysis_df['stage_performance'] != ''])
        for performance, count in performance_dist.head(8).items():
            percentage = count / total_with_performance * 100
            print(f"   {performance}: {count:,} å€‹ ({percentage:.1f}%)")
    
    # è§£ç´„è­¦ç¤ºåˆ†å¸ƒ
    warning_dist = stage_analysis_df[stage_analysis_df['cancellation_warning'] != '']['cancellation_warning'].value_counts()
    if not warning_dist.empty:
        print(f"\nè§£ç´„è­¦ç¤ºåˆ†å¸ƒ:")
        total_with_warning = len(stage_analysis_df[stage_analysis_df['cancellation_warning'] != ''])
        for warning, count in warning_dist.items():
            percentage = count / total_with_warning * 100
            print(f"   {warning}: {count:,} å€‹ ({percentage:.1f}%)")
    
    # è³‡æ–™å®Œæ•´åº¦
    print(f"\nè³‡æ–™å®Œæ•´åº¦:")
    total_records = len(stage_analysis_df)
    print(f"   æœ‰éŠ·å”®éšæ®µ: {len(stage_analysis_df[stage_analysis_df['sales_stage'] != '']):,} å€‹ ({len(stage_analysis_df[stage_analysis_df['sales_stage'] != ''])/total_records*100:.1f}%)")
    print(f"   æœ‰éšæ®µè¡¨ç¾: {len(stage_analysis_df[stage_analysis_df['stage_performance'] != '']):,} å€‹ ({len(stage_analysis_df[stage_analysis_df['stage_performance'] != ''])/total_records*100:.1f}%)")
    print(f"   æœ‰è§£ç´„è­¦ç¤º: {len(stage_analysis_df[stage_analysis_df['cancellation_warning'] != '']):,} å€‹ ({len(stage_analysis_df[stage_analysis_df['cancellation_warning'] != ''])/total_records*100:.1f}%)")

# %% [markdown]
# ## 11. å“è³ªæ§åˆ¶è©•ä¼° (1æ¬„ä½)

# %%
# å“è³ªæ§åˆ¶è©•ä¼°é‚è¼¯
print("ğŸ” å“è³ªæ§åˆ¶è©•ä¼°è™•ç†")
print("=" * 50)

def assess_quality_control(project_code, target_season, basic_info, time_quantity_data, 
                         absorption_data, price_data):
    """
    è©•ä¼°å“è³ªæ§åˆ¶æŒ‡æ¨™ (1æ¬„ä½)
    
    Args:
        project_code: å»ºæ¡ˆç·¨è™Ÿ
        target_season: ç›®æ¨™å¹´å­£
        basic_info: åŸºæœ¬è³‡è¨Š
        time_quantity_data: æ™‚é–“æ•¸é‡è³‡æ–™
        absorption_data: å»åŒ–è³‡æ–™
        price_data: åƒ¹æ ¼è³‡æ–™
        
    Returns:
        dict: å“è³ªæ§åˆ¶æŒ‡æ¨™
    """
    
    metrics = {
        'project_code': project_code,
        'target_season': target_season,
        'is_complete_quarter': 'N'
    }
    
    try:
        # åˆ¤æ–·æ˜¯å¦ç‚ºå®Œæ•´å­£åº¦
        sales_start_season = basic_info.get('sales_start_season', '')
        
        # å¦‚æœæ˜¯éŠ·å”®èµ·å§‹å­£ï¼Œå¯èƒ½ä¸æ˜¯å®Œæ•´å­£åº¦
        if sales_start_season == target_season:
            # éœ€è¦é€²ä¸€æ­¥åˆ¤æ–·å¯¦éš›éŠ·å”®å¤©æ•¸
            # é€™è£¡ç°¡åŒ–è™•ç†ï¼Œå‡è¨­èµ·å§‹å­£éƒ½ä¸æ˜¯å®Œæ•´å­£
            metrics['is_complete_quarter'] = 'N'
        else:
            # å…¶ä»–å­£åº¦å‡è¨­ç‚ºå®Œæ•´å­£åº¦
            metrics['is_complete_quarter'] = 'Y'
        
        # åŸºæ–¼è³‡æ–™å®Œæ•´æ€§é€²ä¸€æ­¥èª¿æ•´
        # å¦‚æœè©²å­£åº¦æœ‰è¶³å¤ çš„äº¤æ˜“è³‡æ–™ï¼Œèªç‚ºæ˜¯å®Œæ•´å­£åº¦
        time_quantity_row = time_quantity_data[
            (time_quantity_data['project_code'] == project_code) &
            (time_quantity_data['target_season'] == target_season)
        ]
        
        if not time_quantity_row.empty:
            quarterly_transactions = time_quantity_row.iloc[0].get('quarterly_transactions', 0)
            quarterly_sales_days = time_quantity_row.iloc[0].get('quarterly_sales_days', 0)
            
            # å¦‚æœè©²å­£æœ‰æˆäº¤ä¸”éŠ·å”®å¤©æ•¸æ¥è¿‘å®Œæ•´å­£åº¦
            if quarterly_transactions > 0 and quarterly_sales_days >= 80:
                metrics['is_complete_quarter'] = 'Y'
            elif quarterly_transactions == 0 and sales_start_season != target_season:
                # å¦‚æœä¸æ˜¯èµ·å§‹å­£ä½†ç„¡æˆäº¤ï¼Œå¯èƒ½æ˜¯è³‡æ–™ä¸å®Œæ•´
                metrics['is_complete_quarter'] = 'N'
    
    except Exception as e:
        print(f"âŒ å“è³ªæ§åˆ¶è©•ä¼°éŒ¯èª¤ {project_code}: {e}")
    
    return metrics

# %%
# æ‰¹é‡è©•ä¼°å“è³ªæ§åˆ¶æŒ‡æ¨™
print("ğŸ”„ æ‰¹é‡è©•ä¼°å“è³ªæ§åˆ¶æŒ‡æ¨™...")

quality_control_results = []

for _, basic_row in basic_info_df.iterrows():
    metrics = assess_quality_control(
        basic_row['project_code'],
        basic_row['target_season'],
        basic_row.to_dict(),
        time_quantity_df,
        absorption_metrics_df,
        price_analysis_df
    )
    quality_control_results.append(metrics)

quality_control_df = pd.DataFrame(quality_control_results)

print(f"âœ… å®Œæˆ {len(quality_control_df)} ç­†å“è³ªæ§åˆ¶æŒ‡æ¨™è©•ä¼°")

# %%
# å“è³ªæ§åˆ¶çµ±è¨ˆ
print(f"\nğŸ“Š å“è³ªæ§åˆ¶çµ±è¨ˆ:")

if not quality_control_df.empty:
    complete_quarter_dist = quality_control_df['is_complete_quarter'].value_counts()
    total_records = len(quality_control_df)
    
    print(f"å®Œæ•´å­£åº¦åˆ†å¸ƒ:")
    for status, count in complete_quarter_dist.items():
        percentage = count / total_records * 100
        status_desc = "å®Œæ•´å­£" if status == 'Y' else "éå®Œæ•´å­£"
        print(f"   {status} ({status_desc}): {count:,} å€‹ ({percentage:.1f}%)")
    
    # æŒ‰å¹´å­£åˆ†æå®Œæ•´åº¦
    quarter_completeness = quality_control_df.groupby('target_season')['is_complete_quarter'].value_counts().unstack(fill_value=0)
    
    if not quarter_completeness.empty:
        print(f"\nå„å¹´å­£å®Œæ•´åº¦åˆ†æ (å‰8å€‹å¹´å­£):")
        for season in quarter_completeness.index[:8]:
            y_count = quarter_completeness.loc[season, 'Y'] if 'Y' in quarter_completeness.columns else 0
            n_count = quarter_completeness.loc[season, 'N'] if 'N' in quarter_completeness.columns else 0
            total_season = y_count + n_count
            if total_season > 0:
                y_percentage = y_count / total_season * 100
                print(f"   {season}: å®Œæ•´{y_count}å€‹({y_percentage:.1f}%), éå®Œæ•´{n_count}å€‹")

# %% [markdown]
# ## 12. ç¤¾å€ç´šå®Œæ•´å ±å‘Šç”Ÿæˆ

# %%
# ç¤¾å€ç´šå®Œæ•´å ±å‘Šç”Ÿæˆ
print("ğŸ“‹ ç¤¾å€ç´šå®Œæ•´å ±å‘Šç”Ÿæˆ")
print("=" * 50)

def generate_community_comprehensive_report():
    """
    ç”Ÿæˆå®Œæ•´çš„32æ¬„ä½ç¤¾å€ç´šå ±å‘Š
    
    Returns:
        DataFrame: å®Œæ•´å ±å‘Š
    """
    
    print("ğŸ”„ æ•´åˆæ‰€æœ‰åˆ†æçµæœ...")
    
    # ä»¥åŸºæœ¬è³‡è¨Šç‚ºä¸»è»¸é€²è¡Œè³‡æ–™åˆä½µ
    comprehensive_report = basic_info_df.copy()
    
    # åˆä½µæ™‚é–“èˆ‡æ•¸é‡æŒ‡æ¨™
    comprehensive_report = comprehensive_report.merge(
        time_quantity_df, 
        on=['project_code', 'target_season'], 
        how='left'
    )
    
    # åˆä½µè§£ç´„è³‡è¨Š
    comprehensive_report = comprehensive_report.merge(
        cancellation_metrics_df, 
        on=['project_code', 'target_season'], 
        how='left'
    )
    
    # åˆä½µå»åŒ–åˆ†æ
    comprehensive_report = comprehensive_report.merge(
        absorption_metrics_df, 
        on=['project_code', 'target_season'], 
        how='left'
    )
    
    # åˆä½µå»åŒ–å‹•æ…‹
    comprehensive_report = comprehensive_report.merge(
        absorption_dynamics_df, 
        on=['project_code', 'target_season'], 
        how='left'
    )
    
    # åˆä½µåƒ¹æ ¼åˆ†æ
    comprehensive_report = comprehensive_report.merge(
        price_analysis_df, 
        on=['project_code', 'target_season'], 
        how='left'
    )
    
    # åˆä½µéšæ®µåˆ†æ
    comprehensive_report = comprehensive_report.merge(
        stage_analysis_df, 
        on=['project_code', 'target_season'], 
        how='left'
    )
    
    # åˆä½µå“è³ªæ§åˆ¶
    comprehensive_report = comprehensive_report.merge(
        quality_control_df, 
        on=['project_code', 'target_season'], 
        how='left'
    )
    
    print(f"âœ… å®Œæˆè³‡æ–™åˆä½µï¼Œå…± {len(comprehensive_report)} ç­†è¨˜éŒ„")
    
    return comprehensive_report

# %%
# ç”Ÿæˆç¤¾å€ç´šå®Œæ•´å ±å‘Š
community_report = generate_community_comprehensive_report()

print(f"ğŸ“Š ç¤¾å€ç´šå ±å‘Šçµ±è¨ˆ:")
print(f"   ç¸½è¨˜éŒ„æ•¸: {len(community_report):,}")
print(f"   ç¸½æ¬„ä½æ•¸: {len(community_report.columns)}")

# %%
# é‡æ–°æ’åˆ—æ¬„ä½é †åºä»¥ç¬¦åˆPRDè¦æ ¼
print("ğŸ”„ é‡æ–°æ’åˆ—æ¬„ä½é †åº...")

# å®šç¾©32æ¬„ä½çš„æ¨™æº–é †åºï¼ˆä¸­æ–‡æ¬„ä½åï¼‰
STANDARD_COLUMN_ORDER = [
    # A. åŸºæœ¬è³‡è¨Š (7æ¬„)
    'å‚™æŸ¥ç·¨è™Ÿ', 'ç¤¾å€åç¨±', 'ç¸£å¸‚', 'è¡Œæ”¿å€', 'åè½è¡—é“', 'ç¸½æˆ¶æ•¸', 'éŠ·å”®èµ·å§‹å¹´å­£',
    
    # B. æ™‚é–“èˆ‡æ•¸é‡ (5æ¬„)  
    'å¹´å­£', 'éŠ·å”®å­£æ•¸', 'ç´¯ç©æˆäº¤ç­†æ•¸', 'è©²å­£æˆäº¤ç­†æ•¸', 'è©²å­£éŠ·å”®å¤©æ•¸',
    
    # C. è§£ç´„è³‡è¨Š (6æ¬„)
    'ç´¯ç©è§£ç´„ç­†æ•¸', 'è©²å­£è§£ç´„ç­†æ•¸', 'å­£åº¦è§£ç´„ç‡(%)', 'ç´¯ç©è§£ç´„ç‡(%)', 'æœ€è¿‘è§£ç´„å¹´å­£', 'é€£çºŒç„¡è§£ç´„å­£æ•¸',
    
    # D. å»åŒ–åˆ†æ (3æ¬„)
    'æ¯›å»åŒ–ç‡(%)', 'æ·¨å»åŒ–ç‡(%)', 'èª¿æ•´å»åŒ–ç‡(%)',
    
    # E. å»åŒ–å‹•æ…‹åˆ†æ (4æ¬„)
    'å­£åº¦å»åŒ–é€Ÿåº¦(æˆ¶/å­£)', 'å»åŒ–åŠ é€Ÿåº¦(%)', 'é ä¼°å®Œå”®å­£æ•¸', 'å»åŒ–æ•ˆç‡è©•ç´š',
    
    # F. åƒ¹æ ¼åˆ†æ (3æ¬„)
    'å¹³å‡äº¤æ˜“å–®åƒ¹(è¬/åª)', 'å¹³å‡ç¸½é¢ç©(åª)', 'å¹³å‡äº¤æ˜“ç¸½åƒ¹(è¬)',
    
    # G. éšæ®µåˆ†æ (3æ¬„)
    'éŠ·å”®éšæ®µ', 'éšæ®µè¡¨ç¾', 'è§£ç´„è­¦ç¤º',
    
    # H. å“è³ªæ§åˆ¶ (1æ¬„)
    'æ˜¯å¦å®Œæ•´å­£'
]

# å»ºç«‹è‹±æ–‡åˆ°ä¸­æ–‡çš„æ¬„ä½å°æ‡‰
formatted_report = pd.DataFrame()

# é€ä¸€å°æ‡‰æ¬„ä½
column_mapping_dict = {
    'å‚™æŸ¥ç·¨è™Ÿ': 'project_code',
    'ç¤¾å€åç¨±': 'project_name',
    'ç¸£å¸‚': 'county', 
    'è¡Œæ”¿å€': 'district',
    'åè½è¡—é“': 'street_address',
    'ç¸½æˆ¶æ•¸': 'total_units',
    'éŠ·å”®èµ·å§‹å¹´å­£': 'sales_start_season',
    'å¹´å­£': 'target_season',
    'éŠ·å”®å­£æ•¸': 'sales_seasons',
    'ç´¯ç©æˆäº¤ç­†æ•¸': 'cumulative_transactions',
    'è©²å­£æˆäº¤ç­†æ•¸': 'quarterly_transactions', 
    'è©²å­£éŠ·å”®å¤©æ•¸': 'quarterly_sales_days',
    'ç´¯ç©è§£ç´„ç­†æ•¸': 'cumulative_cancellations',
    'è©²å­£è§£ç´„ç­†æ•¸': 'quarterly_cancellations',
    'å­£åº¦è§£ç´„ç‡(%)': 'quarterly_cancellation_rate',
    'ç´¯ç©è§£ç´„ç‡(%)': 'cumulative_cancellation_rate',
    'æœ€è¿‘è§£ç´„å¹´å­£': 'latest_cancellation_season',
    'é€£çºŒç„¡è§£ç´„å­£æ•¸': 'consecutive_no_cancellation_seasons',
    'æ¯›å»åŒ–ç‡(%)': 'gross_absorption_rate',
    'æ·¨å»åŒ–ç‡(%)': 'net_absorption_rate',
    'èª¿æ•´å»åŒ–ç‡(%)': 'adjusted_absorption_rate',
    'å­£åº¦å»åŒ–é€Ÿåº¦(æˆ¶/å­£)': 'quarterly_absorption_speed',
    'å»åŒ–åŠ é€Ÿåº¦(%)': 'absorption_acceleration',
    'é ä¼°å®Œå”®å­£æ•¸': 'estimated_completion_seasons',
    'å»åŒ–æ•ˆç‡è©•ç´š': 'absorption_efficiency_grade',
    'å¹³å‡äº¤æ˜“å–®åƒ¹(è¬/åª)': 'avg_unit_price_per_ping',
    'å¹³å‡ç¸½é¢ç©(åª)': 'avg_total_area_ping',
    'å¹³å‡äº¤æ˜“ç¸½åƒ¹(è¬)': 'avg_total_price_wan',
    'éŠ·å”®éšæ®µ': 'sales_stage',
    'éšæ®µè¡¨ç¾': 'stage_performance',
    'è§£ç´„è­¦ç¤º': 'cancellation_warning',
    'æ˜¯å¦å®Œæ•´å­£': 'is_complete_quarter'
}

# å‰µå»ºæ ¼å¼åŒ–å ±å‘Š
for chinese_col, english_col in column_mapping_dict.items():
    if english_col in community_report.columns:
        formatted_report[chinese_col] = community_report[english_col]
    else:
        # å¦‚æœæ‰¾ä¸åˆ°å°æ‡‰æ¬„ä½ï¼Œå¡«å…¥é è¨­å€¼
        if chinese_col in ['ç´¯ç©è§£ç´„ç­†æ•¸', 'è©²å­£è§£ç´„ç­†æ•¸', 'éŠ·å”®å­£æ•¸', 'ç´¯ç©æˆäº¤ç­†æ•¸', 'è©²å­£æˆäº¤ç­†æ•¸', 'è©²å­£éŠ·å”®å¤©æ•¸', 'ç¸½æˆ¶æ•¸', 'é€£çºŒç„¡è§£ç´„å­£æ•¸', 'é ä¼°å®Œå”®å­£æ•¸']:
            formatted_report[chinese_col] = 0
        elif chinese_col in ['å­£åº¦è§£ç´„ç‡(%)', 'ç´¯ç©è§£ç´„ç‡(%)', 'æ¯›å»åŒ–ç‡(%)', 'æ·¨å»åŒ–ç‡(%)', 'èª¿æ•´å»åŒ–ç‡(%)', 'å»åŒ–åŠ é€Ÿåº¦(%)', 'å­£åº¦å»åŒ–é€Ÿåº¦(æˆ¶/å­£)', 'å¹³å‡äº¤æ˜“å–®åƒ¹(è¬/åª)', 'å¹³å‡ç¸½é¢ç©(åª)', 'å¹³å‡äº¤æ˜“ç¸½åƒ¹(è¬)']:
            formatted_report[chinese_col] = 0.0
        elif chinese_col == 'æ˜¯å¦å®Œæ•´å­£':
            formatted_report[chinese_col] = 'N'
        else:
            formatted_report[chinese_col] = ''

print(f"âœ… å®Œæˆæ¬„ä½æ ¼å¼åŒ–ï¼Œå…± {len(formatted_report.columns)} å€‹æ¬„ä½")

# ç¢ºèªæ¬„ä½é †åº
formatted_report = formatted_report[STANDARD_COLUMN_ORDER]

print(f"âœ… å·²æŒ‰ç…§PRDè¦æ ¼æ’åˆ—32å€‹æ¬„ä½")

# %% [markdown]
# ## 13. è³‡æ–™å“è³ªæª¢æŸ¥èˆ‡ç•°å¸¸è™•ç†

# %%
# è³‡æ–™å“è³ªæª¢æŸ¥èˆ‡ç•°å¸¸è™•ç†
print("ğŸ” è³‡æ–™å“è³ªæª¢æŸ¥èˆ‡ç•°å¸¸è™•ç†")
print("=" * 50)

def comprehensive_data_quality_check(report_df):
    """
    åŸ·è¡Œå…¨é¢çš„è³‡æ–™å“è³ªæª¢æŸ¥
    
    Args:
        report_df: ç¤¾å€ç´šå ±å‘ŠDataFrame
        
    Returns:
        dict: å“è³ªæª¢æŸ¥çµæœ
    """
    
    quality_report = {
        'total_records': len(report_df),
        'completeness_check': {},
        'logical_consistency_check': {},
        'value_range_check': {},
        'anomaly_detection': {},
        'data_quality_score': 0.0,
        'recommendations': []
    }
    
    try:
        # 1. å®Œæ•´æ€§æª¢æŸ¥
        print("ğŸ“Š åŸ·è¡Œå®Œæ•´æ€§æª¢æŸ¥...")
        
        completeness = {}
        required_fields = {
            'å‚™æŸ¥ç·¨è™Ÿ': 100,  # å¿…é ˆ100%å®Œæ•´
            'ç¤¾å€åç¨±': 90,   # æœŸæœ›90%ä»¥ä¸Š
            'ç¸£å¸‚': 95,       # æœŸæœ›95%ä»¥ä¸Š
            'ç¸½æˆ¶æ•¸': 90,     # æœŸæœ›90%ä»¥ä¸Š
            'æ·¨å»åŒ–ç‡(%)': 85  # æœŸæœ›85%ä»¥ä¸Š
        }
        
        for field, expected_rate in required_fields.items():
            if field in report_df.columns:
                if field in ['å‚™æŸ¥ç·¨è™Ÿ', 'ç¤¾å€åç¨±', 'ç¸£å¸‚']:
                    non_empty = len(report_df[report_df[field] != ''])
                else:
                    non_empty = len(report_df[report_df[field] > 0])
                
                actual_rate = (non_empty / len(report_df)) * 100
                completeness[field] = {
                    'actual_rate': actual_rate,
                    'expected_rate': expected_rate,
                    'meets_expectation': actual_rate >= expected_rate
                }
        
        quality_report['completeness_check'] = completeness
        
        # 2. é‚è¼¯ä¸€è‡´æ€§æª¢æŸ¥
        print("ğŸ” åŸ·è¡Œé‚è¼¯ä¸€è‡´æ€§æª¢æŸ¥...")
        
        consistency_issues = []
        
        # æª¢æŸ¥å»åŒ–ç‡é‚è¼¯
        invalid_absorption = len(report_df[report_df['æ·¨å»åŒ–ç‡(%)'] > report_df['æ¯›å»åŒ–ç‡(%)']])
        if invalid_absorption > 0:
            consistency_issues.append(f"æ·¨å»åŒ–ç‡é«˜æ–¼æ¯›å»åŒ–ç‡: {invalid_absorption}ç­†")
        
        # æª¢æŸ¥è§£ç´„ç‡é‚è¼¯
        invalid_cancellation = len(report_df[report_df['ç´¯ç©è§£ç´„ç‡(%)'] > 100])
        if invalid_cancellation > 0:
            consistency_issues.append(f"è§£ç´„ç‡è¶…é100%: {invalid_cancellation}ç­†")
        
        # æª¢æŸ¥æˆäº¤æ•¸é‚è¼¯
        invalid_transactions = len(report_df[report_df['è©²å­£æˆäº¤ç­†æ•¸'] > report_df['ç´¯ç©æˆäº¤ç­†æ•¸']])
        if invalid_transactions > 0:
            consistency_issues.append(f"å­£åº¦æˆäº¤è¶…éç´¯ç©æˆäº¤: {invalid_transactions}ç­†")
        
        # æª¢æŸ¥æˆ¶æ•¸é‚è¼¯
        invalid_units = len(report_df[report_df['ç´¯ç©æˆäº¤ç­†æ•¸'] > report_df['ç¸½æˆ¶æ•¸']])
        if invalid_units > 0:
            consistency_issues.append(f"æˆäº¤æ•¸è¶…éç¸½æˆ¶æ•¸: {invalid_units}ç­†")
        
        quality_report['logical_consistency_check'] = {
            'issues_found': len(consistency_issues),
            'issues_detail': consistency_issues
        }
        
        # 3. æ•¸å€¼ç¯„åœæª¢æŸ¥
        print("ğŸ“ åŸ·è¡Œæ•¸å€¼ç¯„åœæª¢æŸ¥...")
        
        range_checks = {
            'æ¯›å»åŒ–ç‡(%)': (0, 120),  # å…è¨±ç¨å¾®è¶…é100%
            'æ·¨å»åŒ–ç‡(%)': (0, 110),
            'ç´¯ç©è§£ç´„ç‡(%)': (0, 100),
            'å­£åº¦è§£ç´„ç‡(%)': (0, 100),
            'å¹³å‡äº¤æ˜“å–®åƒ¹(è¬/åª)': (10, 300),
            'å¹³å‡ç¸½é¢ç©(åª)': (5, 200),
            'å¹³å‡äº¤æ˜“ç¸½åƒ¹(è¬)': (500, 50000),
            'ç¸½æˆ¶æ•¸': (1, 5000),
            'éŠ·å”®å­£æ•¸': (1, 50)
        }
        
        range_violations = {}
        for field, (min_val, max_val) in range_checks.items():
            if field in report_df.columns:
                out_of_range = len(report_df[
                    (report_df[field] < min_val) | (report_df[field] > max_val)
                ])
                if out_of_range > 0:
                    range_violations[field] = out_of_range
        
        quality_report['value_range_check'] = range_violations
        
        # 4. ç•°å¸¸å€¼æª¢æ¸¬
        print("ğŸš¨ åŸ·è¡Œç•°å¸¸å€¼æª¢æ¸¬...")
        
        anomalies = {}
        numerical_fields = ['æ¯›å»åŒ–ç‡(%)', 'æ·¨å»åŒ–ç‡(%)', 'å¹³å‡äº¤æ˜“å–®åƒ¹(è¬/åª)', 'å­£åº¦å»åŒ–é€Ÿåº¦(æˆ¶/å­£)']
        
        for field in numerical_fields:
            if field in report_df.columns:
                valid_data = report_df[report_df[field] > 0][field]
                if len(valid_data) > 10:  # éœ€è¦è¶³å¤ çš„è³‡æ–™é»
                    Q1 = valid_data.quantile(0.25)
                    Q3 = valid_data.quantile(0.75)
                    IQR = Q3 - Q1
                    
                    lower_bound = Q1 - 1.5 * IQR
                    upper_bound = Q3 + 1.5 * IQR
                    
                    outliers = len(valid_data[(valid_data < lower_bound) | (valid_data > upper_bound)])
                    if outliers > 0:
                        anomalies[field] = {
                            'outlier_count': outliers,
                            'outlier_percentage': outliers / len(valid_data) * 100,
                            'bounds': (lower_bound, upper_bound)
                        }
        
        quality_report['anomaly_detection'] = anomalies
        
        # 5. è¨ˆç®—æ•´é«”å“è³ªåˆ†æ•¸
        print("ğŸ“Š è¨ˆç®—æ•´é«”å“è³ªåˆ†æ•¸...")
        
        quality_score = 100  # åŸºç¤åˆ†æ•¸
        
        # å®Œæ•´æ€§æ‰£åˆ†
        for field, info in completeness.items():
            if not info['meets_expectation']:
                shortfall = info['expected_rate'] - info['actual_rate']
                quality_score -= shortfall * 0.2  # æ¯å·®1%æ‰£0.2åˆ†
        
        # ä¸€è‡´æ€§æ‰£åˆ†
        consistency_penalty = len(consistency_issues) * 5  # æ¯å€‹å•é¡Œæ‰£5åˆ†
        quality_score -= consistency_penalty
        
        # ç¯„åœé•è¦æ‰£åˆ†
        range_penalty = sum(range_violations.values()) * 0.1  # æ¯å€‹é•è¦è¨˜éŒ„æ‰£0.1åˆ†
        quality_score -= range_penalty
        
        # ç•°å¸¸å€¼æ‰£åˆ†
        anomaly_penalty = sum([info['outlier_count'] for info in anomalies.values()]) * 0.05
        quality_score -= anomaly_penalty
        
        quality_score = max(0, min(100, quality_score))  # é™åˆ¶åœ¨0-100ç¯„åœ
        quality_report['data_quality_score'] = quality_score
        
        # 6. ç”Ÿæˆå»ºè­°
        recommendations = []
        
        if quality_score < 70:
            recommendations.append("æ•´é«”è³‡æ–™å“è³ªéœ€è¦å¤§å¹…æ”¹å–„")
        elif quality_score < 85:
            recommendations.append("è³‡æ–™å“è³ªè‰¯å¥½ï¼Œä»æœ‰æ”¹å–„ç©ºé–“")
        else:
            recommendations.append("è³‡æ–™å“è³ªå„ªè‰¯")
        
        if len(consistency_issues) > 0:
            recommendations.append("éœ€è¦ä¿®æ­£é‚è¼¯ä¸€è‡´æ€§å•é¡Œ")
        
        if len(range_violations) > 0:
            recommendations.append("éœ€è¦æª¢æŸ¥æ•¸å€¼ç¯„åœç•°å¸¸")
        
        if len(anomalies) > 0:
            recommendations.append("å»ºè­°æª¢æŸ¥ç•°å¸¸å€¼ä¸¦æ±ºå®šè™•ç†æ–¹å¼")
        
        quality_report['recommendations'] = recommendations
    
    except Exception as e:
        print(f"âŒ å“è³ªæª¢æŸ¥éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        quality_report['error'] = str(e)
    
    return quality_report

# %%
# åŸ·è¡Œè³‡æ–™å“è³ªæª¢æŸ¥
print("ğŸ”„ åŸ·è¡Œå®Œæ•´è³‡æ–™å“è³ªæª¢æŸ¥...")

quality_check_result = comprehensive_data_quality_check(formatted_report)

print(f"âœ… è³‡æ–™å“è³ªæª¢æŸ¥å®Œæˆ")

# %%
# å“è³ªæª¢æŸ¥çµæœåˆ†æ
print(f"\nğŸ“Š è³‡æ–™å“è³ªæª¢æŸ¥çµæœ:")

if quality_check_result:
    print(f"ç¸½è¨˜éŒ„æ•¸: {quality_check_result['total_records']:,}")
    print(f"æ•´é«”å“è³ªåˆ†æ•¸: {quality_check_result['data_quality_score']:.1f}/100")
    
    # å®Œæ•´æ€§æª¢æŸ¥çµæœ
    if 'completeness_check' in quality_check_result:
        print(f"\nå®Œæ•´æ€§æª¢æŸ¥:")
        for field, info in quality_check_result['completeness_check'].items():
            status = "âœ…" if info['meets_expectation'] else "âŒ"
            print(f"   {status} {field}: {info['actual_rate']:.1f}% (æœŸæœ›{info['expected_rate']:.1f}%)")
    
    # é‚è¼¯ä¸€è‡´æ€§æª¢æŸ¥çµæœ
    if 'logical_consistency_check' in quality_check_result:
        consistency = quality_check_result['logical_consistency_check']
        if consistency['issues_found'] == 0:
            print(f"\nâœ… é‚è¼¯ä¸€è‡´æ€§: æœªç™¼ç¾å•é¡Œ")
        else:
            print(f"\nâŒ é‚è¼¯ä¸€è‡´æ€§: ç™¼ç¾ {consistency['issues_found']} å€‹å•é¡Œ")
            for issue in consistency['issues_detail']:
                print(f"     - {issue}")
    
    # æ•¸å€¼ç¯„åœæª¢æŸ¥çµæœ
    if 'value_range_check' in quality_check_result:
        range_issues = quality_check_result['value_range_check']
        if not range_issues:
            print(f"\nâœ… æ•¸å€¼ç¯„åœ: æ‰€æœ‰æ¬„ä½éƒ½åœ¨åˆç†ç¯„åœå…§")
        else:
            print(f"\nâš ï¸ æ•¸å€¼ç¯„åœç•°å¸¸:")
            for field, count in range_issues.items():
                print(f"     - {field}: {count} ç­†è¶…å‡ºç¯„åœ")
    
    # ç•°å¸¸å€¼æª¢æ¸¬çµæœ
    if 'anomaly_detection' in quality_check_result:
        anomalies = quality_check_result['anomaly_detection']
        if not anomalies:
            print(f"\nâœ… ç•°å¸¸å€¼æª¢æ¸¬: æœªç™¼ç¾é¡¯è‘—ç•°å¸¸")
        else:
            print(f"\nğŸš¨ ç•°å¸¸å€¼æª¢æ¸¬:")
            for field, info in anomalies.items():
                print(f"     - {field}: {info['outlier_count']} å€‹ç•°å¸¸å€¼ ({info['outlier_percentage']:.1f}%)")
    
    # å»ºè­°
    if 'recommendations' in quality_check_result:
        print(f"\nğŸ’¡ æ”¹å–„å»ºè­°:")
        for recommendation in quality_check_result['recommendations']:
            print(f"   â€¢ {recommendation}")

# %%
# ç•°å¸¸è™•ç†èˆ‡è³‡æ–™æ¸…ç†
print("ğŸ§¹ ç•°å¸¸è™•ç†èˆ‡è³‡æ–™æ¸…ç†")
print("=" * 50)

def clean_and_standardize_report(report_df, quality_result):
    """
    æ¸…ç†å’Œæ¨™æº–åŒ–å ±å‘Šè³‡æ–™
    
    Args:
        report_df: åŸå§‹å ±å‘ŠDataFrame
        quality_result: å“è³ªæª¢æŸ¥çµæœ
        
    Returns:
        DataFrame: æ¸…ç†å¾Œçš„å ±å‘Š
    """
    
    cleaned_report = report_df.copy()
    cleaning_log = []
    
    try:
        # 1. ä¿®æ­£æ˜é¡¯çš„é‚è¼¯éŒ¯èª¤
        print("ğŸ”§ ä¿®æ­£é‚è¼¯éŒ¯èª¤...")
        
        # ä¿®æ­£æ·¨å»åŒ–ç‡é«˜æ–¼æ¯›å»åŒ–ç‡çš„å•é¡Œ
        invalid_net_absorption = cleaned_report['æ·¨å»åŒ–ç‡(%)'] > cleaned_report['æ¯›å»åŒ–ç‡(%)']
        if invalid_net_absorption.any():
            cleaned_report.loc[invalid_net_absorption, 'æ·¨å»åŒ–ç‡(%)'] = cleaned_report.loc[invalid_net_absorption, 'æ¯›å»åŒ–ç‡(%)']
            cleaning_log.append(f"ä¿®æ­£ {invalid_net_absorption.sum()} ç­†æ·¨å»åŒ–ç‡é«˜æ–¼æ¯›å»åŒ–ç‡")
        
        # ä¿®æ­£è§£ç´„ç‡è¶…é100%çš„å•é¡Œ
        over_100_cancellation = cleaned_report['ç´¯ç©è§£ç´„ç‡(%)'] > 100
        if over_100_cancellation.any():
            cleaned_report.loc[over_100_cancellation, 'ç´¯ç©è§£ç´„ç‡(%)'] = 100
            cleaning_log.append(f"ä¿®æ­£ {over_100_cancellation.sum()} ç­†è§£ç´„ç‡è¶…é100%")
        
        # ä¿®æ­£å­£åº¦æˆäº¤è¶…éç´¯ç©æˆäº¤çš„å•é¡Œ
        invalid_quarterly = cleaned_report['è©²å­£æˆäº¤ç­†æ•¸'] > cleaned_report['ç´¯ç©æˆäº¤ç­†æ•¸']
        if invalid_quarterly.any():
            cleaned_report.loc[invalid_quarterly, 'è©²å­£æˆäº¤ç­†æ•¸'] = cleaned_report.loc[invalid_quarterly, 'ç´¯ç©æˆäº¤ç­†æ•¸']
            cleaning_log.append(f"ä¿®æ­£ {invalid_quarterly.sum()} ç­†å­£åº¦æˆäº¤è¶…éç´¯ç©æˆäº¤")
        
        # 2. è™•ç†æ¥µç«¯ç•°å¸¸å€¼
        print("ğŸ¯ è™•ç†æ¥µç«¯ç•°å¸¸å€¼...")
        
        # è™•ç†å–®åƒ¹ç•°å¸¸
        extreme_price = (cleaned_report['å¹³å‡äº¤æ˜“å–®åƒ¹(è¬/åª)'] > 300) | (cleaned_report['å¹³å‡äº¤æ˜“å–®åƒ¹(è¬/åª)'] < 0)
        if extreme_price.any():
            cleaned_report.loc[extreme_price, 'å¹³å‡äº¤æ˜“å–®åƒ¹(è¬/åª)'] = 0
            cleaning_log.append(f"æ¸…é™¤ {extreme_price.sum()} ç­†æ¥µç«¯å–®åƒ¹ç•°å¸¸å€¼")
        
        # è™•ç†é¢ç©ç•°å¸¸
        extreme_area = (cleaned_report['å¹³å‡ç¸½é¢ç©(åª)'] > 200) | (cleaned_report['å¹³å‡ç¸½é¢ç©(åª)'] < 0)
        if extreme_area.any():
            cleaned_report.loc[extreme_area, 'å¹³å‡ç¸½é¢ç©(åª)'] = 0
            cleaning_log.append(f"æ¸…é™¤ {extreme_area.sum()} ç­†æ¥µç«¯é¢ç©ç•°å¸¸å€¼")
        
        # è™•ç†ç¸½åƒ¹ç•°å¸¸
        extreme_total_price = (cleaned_report['å¹³å‡äº¤æ˜“ç¸½åƒ¹(è¬)'] > 50000) | (cleaned_report['å¹³å‡äº¤æ˜“ç¸½åƒ¹(è¬)'] < 0)
        if extreme_total_price.any():
            cleaned_report.loc[extreme_total_price, 'å¹³å‡äº¤æ˜“ç¸½åƒ¹(è¬)'] = 0
            cleaning_log.append(f"æ¸…é™¤ {extreme_total_price.sum()} ç­†æ¥µç«¯ç¸½åƒ¹ç•°å¸¸å€¼")
        
        # 3. æ¨™æº–åŒ–æ–‡å­—æ¬„ä½
        print("ğŸ“ æ¨™æº–åŒ–æ–‡å­—æ¬„ä½...")
        
        # ç¢ºä¿æ–‡å­—æ¬„ä½ä¸å«NaN
        text_fields = ['ç¤¾å€åç¨±', 'ç¸£å¸‚', 'è¡Œæ”¿å€', 'åè½è¡—é“', 'éŠ·å”®èµ·å§‹å¹´å­£', 'æœ€è¿‘è§£ç´„å¹´å­£', 
                      'å»åŒ–æ•ˆç‡è©•ç´š', 'éŠ·å”®éšæ®µ', 'éšæ®µè¡¨ç¾', 'è§£ç´„è­¦ç¤º']
        
        for field in text_fields:
            if field in cleaned_report.columns:
                cleaned_report[field] = cleaned_report[field].fillna('').astype(str)
                # ç§»é™¤å¤šé¤˜ç©ºç™½
                cleaned_report[field] = cleaned_report[field].str.strip()
        
        # 4. æ¨™æº–åŒ–æ•¸å€¼æ¬„ä½
        print("ğŸ”¢ æ¨™æº–åŒ–æ•¸å€¼æ¬„ä½...")
        
        # ç¢ºä¿æ•¸å€¼æ¬„ä½çš„ç²¾åº¦
        percentage_fields = ['æ¯›å»åŒ–ç‡(%)', 'æ·¨å»åŒ–ç‡(%)', 'èª¿æ•´å»åŒ–ç‡(%)', 'å­£åº¦è§£ç´„ç‡(%)', 'ç´¯ç©è§£ç´„ç‡(%)', 'å»åŒ–åŠ é€Ÿåº¦(%)']
        for field in percentage_fields:
            if field in cleaned_report.columns:
                cleaned_report[field] = cleaned_report[field].round(2)
        
        # åƒ¹æ ¼æ¬„ä½ç²¾åº¦
        price_fields = ['å¹³å‡äº¤æ˜“å–®åƒ¹(è¬/åª)', 'å¹³å‡ç¸½é¢ç©(åª)', 'å¹³å‡äº¤æ˜“ç¸½åƒ¹(è¬)', 'å­£åº¦å»åŒ–é€Ÿåº¦(æˆ¶/å­£)']
        for field in price_fields:
            if field in cleaned_report.columns:
                cleaned_report[field] = cleaned_report[field].round(2)
        
        # æ•´æ•¸æ¬„ä½
        integer_fields = ['ç¸½æˆ¶æ•¸', 'éŠ·å”®å­£æ•¸', 'ç´¯ç©æˆäº¤ç­†æ•¸', 'è©²å­£æˆäº¤ç­†æ•¸', 'è©²å­£éŠ·å”®å¤©æ•¸',
                         'ç´¯ç©è§£ç´„ç­†æ•¸', 'è©²å­£è§£ç´„ç­†æ•¸', 'é€£çºŒç„¡è§£ç´„å­£æ•¸', 'é ä¼°å®Œå”®å­£æ•¸']
        for field in integer_fields:
            if field in cleaned_report.columns:
                cleaned_report[field] = cleaned_report[field].fillna(0).astype(int)
        
        # 5. æœ€çµ‚å“è³ªæª¢æŸ¥
        print("ğŸ” åŸ·è¡Œæœ€çµ‚å“è³ªæª¢æŸ¥...")
        
        # ç¢ºä¿é—œéµæ¬„ä½ä¸ç‚ºç©º
        critical_fields = ['å‚™æŸ¥ç·¨è™Ÿ', 'å¹´å­£']
        for field in critical_fields:
            if field in cleaned_report.columns:
                empty_critical = cleaned_report[field].isna() | (cleaned_report[field] == '')
                if empty_critical.any():
                    print(f"âš ï¸ ç™¼ç¾ {empty_critical.sum()} ç­†é—œéµæ¬„ä½ {field} ç‚ºç©ºï¼Œå°‡ç§»é™¤é€™äº›è¨˜éŒ„")
                    cleaned_report = cleaned_report[~empty_critical]
        
        print(f"âœ… è³‡æ–™æ¸…ç†å®Œæˆ")
        for log in cleaning_log:
            print(f"   â€¢ {log}")
    
    except Exception as e:
        print(f"âŒ è³‡æ–™æ¸…ç†éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    return cleaned_report, cleaning_log

# %%
# åŸ·è¡Œè³‡æ–™æ¸…ç†
print("ğŸ”„ åŸ·è¡Œè³‡æ–™æ¸…ç†...")

cleaned_report, cleaning_log = clean_and_standardize_report(formatted_report, quality_check_result)

print(f"âœ… è³‡æ–™æ¸…ç†å®Œæˆ")
print(f"   æ¸…ç†å‰è¨˜éŒ„æ•¸: {len(formatted_report):,}")
print(f"   æ¸…ç†å¾Œè¨˜éŒ„æ•¸: {len(cleaned_report):,}")
print(f"   æ¸…ç†æ“ä½œæ•¸: {len(cleaning_log)}")

# %% [markdown]
# ## 14. å ±å‘Šè¼¸å‡ºèˆ‡æ–‡æª”ç”Ÿæˆ

# %%
# æœ€çµ‚å ±å‘Šçµ±è¨ˆèˆ‡æ‘˜è¦
print("ğŸ“Š æœ€çµ‚å ±å‘Šçµ±è¨ˆèˆ‡æ‘˜è¦")
print("=" * 50)

def generate_report_summary(report_df):
    """
    ç”Ÿæˆå ±å‘Šæ‘˜è¦çµ±è¨ˆ
    
    Args:
        report_df: æœ€çµ‚å ±å‘ŠDataFrame
        
    Returns:
        dict: å ±å‘Šæ‘˜è¦
    """
    
    summary = {
        'basic_statistics': {},
        'market_overview': {},
        'risk_analysis': {},
        'performance_analysis': {},
        'data_coverage': {}
    }
    
    try:
        # åŸºæœ¬çµ±è¨ˆ
        summary['basic_statistics'] = {
            'total_projects': len(report_df),
            'total_seasons': report_df['å¹´å­£'].nunique(),
            'counties_covered': report_df['ç¸£å¸‚'].nunique(),
            'districts_covered': report_df['è¡Œæ”¿å€'].nunique(),
            'date_range': f"{report_df['å¹´å­£'].min()} ~ {report_df['å¹´å­£'].max()}"
        }
        
        # å¸‚å ´æ¦‚æ³
        valid_absorption = report_df[report_df['æ·¨å»åŒ–ç‡(%)'] > 0]
        if not valid_absorption.empty:
            summary['market_overview'] = {
                'avg_absorption_rate': valid_absorption['æ·¨å»åŒ–ç‡(%)'].mean(),
                'median_absorption_rate': valid_absorption['æ·¨å»åŒ–ç‡(%)'].median(),
                'completed_projects': len(report_df[report_df['æ·¨å»åŒ–ç‡(%)'] >= 100]),
                'completion_rate': len(report_df[report_df['æ·¨å»åŒ–ç‡(%)'] >= 100]) / len(report_df) * 100,
                'avg_sales_seasons': report_df[report_df['éŠ·å”®å­£æ•¸'] > 0]['éŠ·å”®å­£æ•¸'].mean()
            }
        
        # é¢¨éšªåˆ†æ
        with_cancellation = report_df[report_df['ç´¯ç©è§£ç´„ç­†æ•¸'] > 0]
        summary['risk_analysis'] = {
            'projects_with_cancellations': len(with_cancellation),
            'cancellation_project_rate': len(with_cancellation) / len(report_df) * 100,
            'avg_cancellation_rate': report_df['ç´¯ç©è§£ç´„ç‡(%)'].mean(),
            'high_risk_projects': len(report_df[report_df['ç´¯ç©è§£ç´„ç‡(%)'] > 5])
        }
        
        # ç¸¾æ•ˆåˆ†æ
        valid_speed = report_df[report_df['å­£åº¦å»åŒ–é€Ÿåº¦(æˆ¶/å­£)'] > 0]
        if not valid_speed.empty:
            summary['performance_analysis'] = {
                'avg_absorption_speed': valid_speed['å­£åº¦å»åŒ–é€Ÿåº¦(æˆ¶/å­£)'].mean(),
                'high_speed_projects': len(valid_speed[valid_speed['å­£åº¦å»åŒ–é€Ÿåº¦(æˆ¶/å­£)'] >= 3]),
                'slow_projects': len(valid_speed[valid_speed['å­£åº¦å»åŒ–é€Ÿåº¦(æˆ¶/å­£)'] < 1])
            }
        
        # è³‡æ–™æ¶µè“‹åº¦
        summary['data_coverage'] = {
            'projects_with_names': len(report_df[report_df['ç¤¾å€åç¨±'] != '']),
            'projects_with_location': len(report_df[report_df['ç¸£å¸‚'] != '']),
            'projects_with_units': len(report_df[report_df['ç¸½æˆ¶æ•¸'] > 0]),
            'projects_with_transactions': len(report_df[report_df['ç´¯ç©æˆäº¤ç­†æ•¸'] > 0]),
            'projects_with_prices': len(report_df[report_df['å¹³å‡äº¤æ˜“å–®åƒ¹(è¬/åª)'] > 0])
        }
    
    except Exception as e:
        print(f"âŒ æ‘˜è¦ç”ŸæˆéŒ¯èª¤: {e}")
        summary['error'] = str(e)
    
    return summary

# %%
# ç”Ÿæˆå ±å‘Šæ‘˜è¦
report_summary = generate_report_summary(cleaned_report)

print(f"ğŸ“‹ ç¤¾å€ç´šå ±å‘Šæœ€çµ‚æ‘˜è¦:")

if 'basic_statistics' in report_summary:
    basic = report_summary['basic_statistics']
    print(f"\nåŸºæœ¬çµ±è¨ˆ:")
    print(f"   ç¸½å»ºæ¡ˆæ•¸: {basic.get('total_projects', 0):,}")
    print(f"   æ¶µè“‹å¹´å­£: {basic.get('total_seasons', 0)} å€‹")
    print(f"   æ¶µè“‹ç¸£å¸‚: {basic.get('counties_covered', 0)} å€‹")
    print(f"   æ¶µè“‹è¡Œæ”¿å€: {basic.get('districts_covered', 0)} å€‹")
    print(f"   è³‡æ–™æœŸé–“: {basic.get('date_range', 'N/A')}")

if 'market_overview' in report_summary:
    market = report_summary['market_overview']
    print(f"\nå¸‚å ´æ¦‚æ³:")
    print(f"   å¹³å‡å»åŒ–ç‡: {market.get('avg_absorption_rate', 0):.1f}%")
    print(f"   ä¸­ä½æ•¸å»åŒ–ç‡: {market.get('median_absorption_rate', 0):.1f}%")
    print(f"   å®Œå”®å»ºæ¡ˆ: {market.get('completed_projects', 0):,} å€‹")
    print(f"   å®Œå”®ç‡: {market.get('completion_rate', 0):.1f}%")
    print(f"   å¹³å‡éŠ·å”®å­£æ•¸: {market.get('avg_sales_seasons', 0):.1f} å­£")

if 'risk_analysis' in report_summary:
    risk = report_summary['risk_analysis']
    print(f"\né¢¨éšªåˆ†æ:")
    print(f"   æœ‰è§£ç´„å»ºæ¡ˆ: {risk.get('projects_with_cancellations', 0):,} å€‹")
    print(f"   è§£ç´„å»ºæ¡ˆæ¯”ä¾‹: {risk.get('cancellation_project_rate', 0):.1f}%")
    print(f"   å¹³å‡è§£ç´„ç‡: {risk.get('avg_cancellation_rate', 0):.2f}%")
    print(f"   é«˜é¢¨éšªå»ºæ¡ˆ: {risk.get('high_risk_projects', 0):,} å€‹")

if 'performance_analysis' in report_summary:
    performance = report_summary['performance_analysis']
    print(f"\nç¸¾æ•ˆåˆ†æ:")
    print(f"   å¹³å‡å»åŒ–é€Ÿåº¦: {performance.get('avg_absorption_speed', 0):.2f} æˆ¶/å­£")
    print(f"   é«˜é€Ÿå»åŒ–å»ºæ¡ˆ: {performance.get('high_speed_projects', 0):,} å€‹")
    print(f"   ç·©æ…¢å»åŒ–å»ºæ¡ˆ: {performance.get('slow_projects', 0):,} å€‹")

if 'data_coverage' in report_summary:
    coverage = report_summary['data_coverage']
    total = report_summary['basic_statistics'].get('total_projects', 1)
    print(f"\nè³‡æ–™æ¶µè“‹åº¦:")
    print(f"   æœ‰å»ºæ¡ˆåç¨±: {coverage.get('projects_with_names', 0):,} å€‹ ({coverage.get('projects_with_names', 0)/total*100:.1f}%)")
    print(f"   æœ‰åœ°ç†ä½ç½®: {coverage.get('projects_with_location', 0):,} å€‹ ({coverage.get('projects_with_location', 0)/total*100:.1f}%)")
    print(f"   æœ‰æˆ¶æ•¸è³‡æ–™: {coverage.get('projects_with_units', 0):,} å€‹ ({coverage.get('projects_with_units', 0)/total*100:.1f}%)")
    print(f"   æœ‰äº¤æ˜“è¨˜éŒ„: {coverage.get('projects_with_transactions', 0):,} å€‹ ({coverage.get('projects_with_transactions', 0)/total*100:.1f}%)")
    print(f"   æœ‰åƒ¹æ ¼è³‡æ–™: {coverage.get('projects_with_prices', 0):,} å€‹ ({coverage.get('projects_with_prices', 0)/total*100:.1f}%)")

# %% [markdown]
# ## 15. è¦–è¦ºåŒ–åˆ†æ

# %%
# å‰µå»ºç¤¾å€ç´šå ±å‘Šè¦–è¦ºåŒ–åˆ†æ
print("ğŸ“Š ç¤¾å€ç´šå ±å‘Šè¦–è¦ºåŒ–åˆ†æ")
print("=" * 50)

# å‰µå»ºåœ–è¡¨
fig, axes = plt.subplots(3, 3, figsize=(20, 15))

# éæ¿¾æœ‰æ•ˆæ•¸æ“š
valid_data = cleaned_report[cleaned_report['æ·¨å»åŒ–ç‡(%)'] >= 0]

# 1. å»åŒ–ç‡åˆ†å¸ƒ
if not valid_data.empty:
    absorption_rates = valid_data[valid_data['æ·¨å»åŒ–ç‡(%)'] > 0]['æ·¨å»åŒ–ç‡(%)']
    if not absorption_rates.empty:
        axes[0, 0].hist(absorption_rates, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
        axes[0, 0].set_title('æ·¨å»åŒ–ç‡åˆ†å¸ƒ', fontsize=14, fontweight='bold')
        axes[0, 0].set_xlabel('æ·¨å»åŒ–ç‡ (%)')
        axes[0, 0].set_ylabel('å»ºæ¡ˆæ•¸é‡')
        axes[0, 0].axvline(x=absorption_rates.mean(), color='red', linestyle='--', 
                          label=f'å¹³å‡: {absorption_rates.mean():.1f}%')
        axes[0, 0].axvline(x=absorption_rates.median(), color='orange', linestyle='--', 
                          label=f'ä¸­ä½æ•¸: {absorption_rates.median():.1f}%')
        axes[0, 0].legend()

# 2. éŠ·å”®éšæ®µåˆ†å¸ƒ
stage_counts = valid_data[valid_data['éŠ·å”®éšæ®µ'] != '']['éŠ·å”®éšæ®µ'].value_counts()
if not stage_counts.empty:
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7'][:len(stage_counts)]
    wedges, texts, autotexts = axes[0, 1].pie(stage_counts.values, labels=stage_counts.index, 
                                              autopct='%1.1f%%', colors=colors, startangle=90)
    axes[0, 1].set_title('éŠ·å”®éšæ®µåˆ†å¸ƒ', fontsize=14, fontweight='bold')
    for autotext in autotexts:
        autotext.set_fontsize(9)

# 3. éŠ·å”®å­£æ•¸åˆ†å¸ƒ
sales_seasons = valid_data[valid_data['éŠ·å”®å­£æ•¸'] > 0]['éŠ·å”®å­£æ•¸']
if not sales_seasons.empty:
    season_counts = sales_seasons.value_counts().sort_index()
    bars = axes[0, 2].bar(range(len(season_counts)), season_counts.values, color='lightgreen', alpha=0.8)
    axes[0, 2].set_title('éŠ·å”®å­£æ•¸åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    axes[0, 2].set_xlabel('éŠ·å”®å­£æ•¸')
    axes[0, 2].set_ylabel('å»ºæ¡ˆæ•¸é‡')
    axes[0, 2].set_xticks(range(len(season_counts)))
    axes[0, 2].set_xticklabels(season_counts.index)
    
    # åªé¡¯ç¤ºå‰10å€‹æ¨™ç±¤ï¼Œé¿å…éæ–¼æ“æ“ 
    if len(season_counts) > 10:
        step = max(1, len(season_counts) // 10)
        axes[0, 2].set_xticks(range(0, len(season_counts), step))
        axes[0, 2].set_xticklabels(season_counts.index[::step])

# 4. ç¸£å¸‚å»åŒ–ç‡æ¯”è¼ƒ
county_absorption = valid_data[valid_data['ç¸£å¸‚'] != ''].groupby('ç¸£å¸‚')['æ·¨å»åŒ–ç‡(%)'].mean().sort_values(ascending=False)
county_counts = valid_data['ç¸£å¸‚'].value_counts()
filtered_counties = county_absorption[county_counts >= 10].head(8)  # åªé¡¯ç¤ºå»ºæ¡ˆæ•¸>=10çš„å‰8å€‹ç¸£å¸‚

if not filtered_counties.empty:
    bars = axes[1, 0].barh(range(len(filtered_counties)), filtered_counties.values, color='lightcoral', alpha=0.8)
    axes[1, 0].set_title('ç¸£å¸‚å¹³å‡å»åŒ–ç‡ (å‰8å)', fontsize=14, fontweight='bold')
    axes[1, 0].set_xlabel('å¹³å‡å»åŒ–ç‡ (%)')
    axes[1, 0].set_yticks(range(len(filtered_counties)))
    axes[1, 0].set_yticklabels(filtered_counties.index)
    
    # æ·»åŠ æ•¸å€¼æ¨™ç±¤
    for i, bar in enumerate(bars):
        width = bar.get_width()
        count = county_counts[filtered_counties.index[i]]
        axes[1, 0].text(width, bar.get_y() + bar.get_height()/2.,
                       f'{width:.1f}% ({count})', ha='left', va='center', fontsize=9)

# 5. åƒ¹æ ¼vså»åŒ–ç‡æ•£é»åœ–
price_absorption = valid_data[
    (valid_data['å¹³å‡äº¤æ˜“å–®åƒ¹(è¬/åª)'] > 0) & 
    (valid_data['æ·¨å»åŒ–ç‡(%)'] > 0) &
    (valid_data['å¹³å‡äº¤æ˜“å–®åƒ¹(è¬/åª)'] < 200)  # éæ¿¾æ¥µç«¯å€¼
]

if len(price_absorption) > 10:
    scatter = axes[1, 1].scatter(price_absorption['å¹³å‡äº¤æ˜“å–®åƒ¹(è¬/åª)'], 
                               price_absorption['æ·¨å»åŒ–ç‡(%)'],
                               alpha=0.6, color='purple')
    axes[1, 1].set_title('å–®åƒ¹ vs å»åŒ–ç‡', fontsize=14, fontweight='bold')
    axes[1, 1].set_xlabel('å¹³å‡äº¤æ˜“å–®åƒ¹ (è¬/åª)')
    axes[1, 1].set_ylabel('æ·¨å»åŒ–ç‡ (%)')
    
    # æ·»åŠ è¶¨å‹¢ç·š
    if len(price_absorption) > 20:
        z = np.polyfit(price_absorption['å¹³å‡äº¤æ˜“å–®åƒ¹(è¬/åª)'], 
                      price_absorption['æ·¨å»åŒ–ç‡(%)'], 1)
        p = np.poly1d(z)
        axes[1, 1].plot(price_absorption['å¹³å‡äº¤æ˜“å–®åƒ¹(è¬/åª)'], 
                       p(price_absorption['å¹³å‡äº¤æ˜“å–®åƒ¹(è¬/åª)']), 
                       "r--", alpha=0.8, label='è¶¨å‹¢ç·š')
        axes[1, 1].legend()

# 6. è§£ç´„ç‡åˆ†å¸ƒ
cancellation_data = valid_data[valid_data['ç´¯ç©è§£ç´„ç‡(%)'] >= 0]
if not cancellation_data.empty:
    # å‰µå»ºè§£ç´„ç‡å€é–“
    bins = [0, 1, 3, 5, 10, 100]
    labels = ['0-1%', '1-3%', '3-5%', '5-10%', '>10%']
    cancellation_data_copy = cancellation_data.copy()
    cancellation_data_copy['è§£ç´„ç‡å€é–“'] = pd.cut(cancellation_data_copy['ç´¯ç©è§£ç´„ç‡(%)'], bins=bins, labels=labels, right=False)
    
    interval_counts = cancellation_data_copy['è§£ç´„ç‡å€é–“'].value_counts().sort_index()
    if not interval_counts.empty:
        colors = ['green', 'yellow', 'orange', 'red', 'darkred'][:len(interval_counts)]
        bars = axes[1, 2].bar(range(len(interval_counts)), interval_counts.values, color=colors, alpha=0.8)
        axes[1, 2].set_title('ç´¯ç©è§£ç´„ç‡åˆ†å¸ƒ', fontsize=14, fontweight='bold')
        axes[1, 2].set_xlabel('è§£ç´„ç‡å€é–“')
        axes[1, 2].set_ylabel('å»ºæ¡ˆæ•¸é‡')
        axes[1, 2].set_xticks(range(len(interval_counts)))
        axes[1, 2].set_xticklabels(interval_counts.index, rotation=45)
        
        # æ·»åŠ æ•¸å€¼æ¨™ç±¤
        for bar in bars:
            height = bar.get_height()
            axes[1, 2].text(bar.get_x() + bar.get_width()/2., height,
                           f'{int(height)}', ha='center', va='bottom')

# 7. å»åŒ–é€Ÿåº¦åˆ†å¸ƒ
speed_data = valid_data[valid_data['å­£åº¦å»åŒ–é€Ÿåº¦(æˆ¶/å­£)'] > 0]
if not speed_data.empty:
    speeds = speed_data['å­£åº¦å»åŒ–é€Ÿåº¦(æˆ¶/å­£)']
    # éæ¿¾æ¥µç«¯å€¼
    speeds_filtered = speeds[speeds <= 20]  # å‡è¨­20æˆ¶/å­£ä»¥ä¸Šç‚ºæ¥µç«¯å€¼
    
    if not speeds_filtered.empty:
        axes[2, 0].hist(speeds_filtered, bins=15, alpha=0.7, color='lightblue', edgecolor='black')
        axes[2, 0].set_title('å­£åº¦å»åŒ–é€Ÿåº¦åˆ†å¸ƒ', fontsize=14, fontweight='bold')
        axes[2, 0].set_xlabel('å»åŒ–é€Ÿåº¦ (æˆ¶/å­£)')
        axes[2, 0].set_ylabel('å»ºæ¡ˆæ•¸é‡')
        axes[2, 0].axvline(x=speeds_filtered.mean(), color='red', linestyle='--', 
                          label=f'å¹³å‡: {speeds_filtered.mean():.2f}')
        axes[2, 0].legend()

# 8. ç¸½æˆ¶æ•¸vsæˆäº¤æ•¸æ•£é»åœ–
units_transactions = valid_data[
    (valid_data['ç¸½æˆ¶æ•¸'] > 0) & 
    (valid_data['ç´¯ç©æˆäº¤ç­†æ•¸'] > 0) &
    (valid_data['ç¸½æˆ¶æ•¸'] <= 1000)  # éæ¿¾æ¥µç«¯å€¼
]

if not units_transactions.empty:
    scatter = axes[2, 1].scatter(units_transactions['ç¸½æˆ¶æ•¸'], 
                               units_transactions['ç´¯ç©æˆäº¤ç­†æ•¸'],
                               c=units_transactions['æ·¨å»åŒ–ç‡(%)'], 
                               cmap='RdYlGn', alpha=0.6)
    axes[2, 1].set_title('ç¸½æˆ¶æ•¸ vs ç´¯ç©æˆäº¤æ•¸', fontsize=14, fontweight='bold')
    axes[2, 1].set_xlabel('ç¸½æˆ¶æ•¸')
    axes[2, 1].set_ylabel('ç´¯ç©æˆäº¤ç­†æ•¸')
    
    # æ·»åŠ é¡è‰²æ¢
    cbar = plt.colorbar(scatter, ax=axes[2, 1])
    cbar.set_label('æ·¨å»åŒ–ç‡ (%)')
    
    # æ·»åŠ å°è§’ç·šï¼ˆç†æƒ³æƒ…æ³ï¼šæˆäº¤æ•¸=ç¸½æˆ¶æ•¸ï¼‰
    max_units = units_transactions['ç¸½æˆ¶æ•¸'].max()
    axes[2, 1].plot([0, max_units], [0, max_units], 'k--', alpha=0.5, label='å®Œå”®ç·š')
    axes[2, 1].legend()

# 9. å¹´å­£è¶¨å‹¢åˆ†æ
season_trends = valid_data.groupby('å¹´å­£').agg({
    'æ·¨å»åŒ–ç‡(%)': 'mean',
    'ç´¯ç©è§£ç´„ç‡(%)': 'mean'
}).reset_index()

if not season_trends.empty and len(season_trends) > 1:
    # æ’åºå¹´å­£
    season_trends_sorted = season_trends.sort_values('å¹´å­£')
    
    ax1 = axes[2, 2]
    ax2 = ax1.twinx()
    
    line1 = ax1.plot(range(len(season_trends_sorted)), season_trends_sorted['æ·¨å»åŒ–ç‡(%)'], 
                     'b-o', label='å¹³å‡å»åŒ–ç‡', linewidth=2)
    line2 = ax2.plot(range(len(season_trends_sorted)), season_trends_sorted['ç´¯ç©è§£ç´„ç‡(%)'], 
                     'r-s', label='å¹³å‡è§£ç´„ç‡', linewidth=2)
    
    ax1.set_title('å¹´å­£è¶¨å‹¢åˆ†æ', fontsize=14, fontweight='bold')
    ax1.set_xlabel('å¹´å­£')
    ax1.set_ylabel('å¹³å‡å»åŒ–ç‡ (%)', color='b')
    ax2.set_ylabel('å¹³å‡è§£ç´„ç‡ (%)', color='r')
    
    # è¨­å®šXè»¸æ¨™ç±¤
    step = max(1, len(season_trends_sorted) // 6)  # æœ€å¤šé¡¯ç¤º6å€‹æ¨™ç±¤
    ax1.set_xticks(range(0, len(season_trends_sorted), step))
    ax1.set_xticklabels(season_trends_sorted['å¹´å­£'].iloc[::step], rotation=45)
    
    # åˆä½µåœ–ä¾‹
    lines = line1 + line2
    labels = [l.get_label() for l in lines]
    ax1.legend(lines, labels, loc='upper left')

plt.tight_layout()
plt.show()

# %% [markdown]
# ## 16. çµæœå„²å­˜èˆ‡åŒ¯å‡º

# %%
# å„²å­˜ç¤¾å€ç´šå®Œæ•´å ±å‘Š
print("ğŸ’¾ å„²å­˜ç¤¾å€ç´šå®Œæ•´å ±å‘Š...")

try:
    # 1. å„²å­˜ä¸»è¦å ±å‘Šï¼ˆ32æ¬„ä½å®Œæ•´ç‰ˆï¼‰
    output_filename = f'community_level_comprehensive_report_{datetime.now().strftime("%Y%m%d")}.csv'
    cleaned_report.to_csv(f'../data/processed/{output_filename}', 
                          index=False, encoding='utf-8-sig')
    print(f"âœ… ä¸»è¦å ±å‘Šå·²å„²å­˜: {output_filename}")
    print(f"   è¨˜éŒ„æ•¸: {len(cleaned_report):,}")
    print(f"   æ¬„ä½æ•¸: {len(cleaned_report.columns)}")
    
    # 2. å„²å­˜å ±å‘Šæ‘˜è¦
    summary_filename = f'community_report_summary_{datetime.now().strftime("%Y%m%d")}.json'
    with open(f'../data/processed/{summary_filename}', 'w', encoding='utf-8') as f:
        import json
        json.dump(report_summary, f, ensure_ascii=False, indent=2)
    print(f"âœ… å ±å‘Šæ‘˜è¦å·²å„²å­˜: {summary_filename}")
    
    # 3. å„²å­˜å“è³ªæª¢æŸ¥çµæœ
    quality_filename = f'data_quality_report_{datetime.now().strftime("%Y%m%d")}.json'
    with open(f'../data/processed/{quality_filename}', 'w', encoding='utf-8') as f:
        json.dump(quality_check_result, f, ensure_ascii=False, indent=2, default=str)
    print(f"âœ… å“è³ªæª¢æŸ¥çµæœå·²å„²å­˜: {quality_filename}")
    
    # 4. å„²å­˜æ¸…ç†æ—¥èªŒ
    if cleaning_log:
        cleaning_log_filename = f'data_cleaning_log_{datetime.now().strftime("%Y%m%d")}.txt'
        with open(f'../data/processed/{cleaning_log_filename}', 'w', encoding='utf-8') as f:
            f.write("è³‡æ–™æ¸…ç†æ“ä½œæ—¥èªŒ\n")
            f.write("=" * 50 + "\n")
            f.write(f"æ¸…ç†æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ¸…ç†å‰è¨˜éŒ„æ•¸: {len(formatted_report):,}\n")
            f.write(f"æ¸…ç†å¾Œè¨˜éŒ„æ•¸: {len(cleaned_report):,}\n\n")
            f.write("æ¸…ç†æ“ä½œè©³æƒ…:\n")
            for i, log in enumerate(cleaning_log, 1):
                f.write(f"{i}. {log}\n")
        print(f"âœ… æ¸…ç†æ—¥èªŒå·²å„²å­˜: {cleaning_log_filename}")
    
    # 5. å‰µå»ºç¯„ä¾‹å ±å‘Šï¼ˆå‰100ç­†ï¼‰
    sample_report = cleaned_report.head(100).copy()
    sample_filename = f'community_report_sample_{datetime.now().strftime("%Y%m%d")}.csv'
    sample_report.to_csv(f'../data/processed/{sample_filename}', 
                        index=False, encoding='utf-8-sig')
    print(f"âœ… ç¯„ä¾‹å ±å‘Šå·²å„²å­˜: {sample_filename} (å‰100ç­†)")
    
    # 6. å„²å­˜æ¬„ä½èªªæ˜æ–‡æª”
    column_docs = []
    for category, fields in COMMUNITY_REPORT_SCHEMA.items():
        for chinese_name, english_name in fields.items():
            column_docs.append({
                'category': category,
                'chinese_name': chinese_name,
                'english_name': english_name,
                'data_type': 'string' if chinese_name in ['å‚™æŸ¥ç·¨è™Ÿ', 'ç¤¾å€åç¨±', 'ç¸£å¸‚', 'è¡Œæ”¿å€', 'åè½è¡—é“', 'éŠ·å”®èµ·å§‹å¹´å­£', 'å¹´å­£', 'æœ€è¿‘è§£ç´„å¹´å­£', 'å»åŒ–æ•ˆç‡è©•ç´š', 'éŠ·å”®éšæ®µ', 'éšæ®µè¡¨ç¾', 'è§£ç´„è­¦ç¤º', 'æ˜¯å¦å®Œæ•´å­£'] else 'numeric',
                'description': f'{chinese_name}ç›¸é—œæŒ‡æ¨™'
            })
    
    column_docs_df = pd.DataFrame(column_docs)
    docs_filename = f'community_report_column_definitions_{datetime.now().strftime("%Y%m%d")}.csv'
    column_docs_df.to_csv(f'../data/processed/{docs_filename}', 
                         index=False, encoding='utf-8-sig')
    print(f"âœ… æ¬„ä½èªªæ˜å·²å„²å­˜: {docs_filename}")
    
    # 7. ç”Ÿæˆçµ±è¨ˆå ±å‘Š
    stats_report = {
        'generation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'report_statistics': {
            'total_records': len(cleaned_report),
            'total_columns': len(cleaned_report.columns),
            'data_quality_score': quality_check_result.get('data_quality_score', 0),
            'counties_covered': cleaned_report['ç¸£å¸‚'].nunique(),
            'districts_covered': cleaned_report['è¡Œæ”¿å€'].nunique(),
            'seasons_covered': cleaned_report['å¹´å­£'].nunique(),
            'unique_projects': cleaned_report['å‚™æŸ¥ç·¨è™Ÿ'].nunique()
        },
        'key_findings': {
            'avg_absorption_rate': cleaned_report[cleaned_report['æ·¨å»åŒ–ç‡(%)'] > 0]['æ·¨å»åŒ–ç‡(%)'].mean(),
            'completion_rate': len(cleaned_report[cleaned_report['æ·¨å»åŒ–ç‡(%)'] >= 100]) / len(cleaned_report) * 100,
            'avg_cancellation_rate': cleaned_report['ç´¯ç©è§£ç´„ç‡(%)'].mean(),
            'projects_with_cancellations': len(cleaned_report[cleaned_report['ç´¯ç©è§£ç´„ç­†æ•¸'] > 0])
        }
    }
    
    stats_filename = f'community_report_statistics_{datetime.now().strftime("%Y%m%d")}.json'
    with open(f'../data/processed/{stats_filename}', 'w', encoding='utf-8') as f:
        json.dump(stats_report, f, ensure_ascii=False, indent=2, default=str)
    print(f"âœ… çµ±è¨ˆå ±å‘Šå·²å„²å­˜: {stats_filename}")

except Exception as e:
    print(f"âŒ å„²å­˜éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")

print(f"\nâœ… æ‰€æœ‰ç¤¾å€ç´šå ±å‘Šæª”æ¡ˆå·²æˆåŠŸå„²å­˜è‡³ ../data/processed/")

# %% [markdown]
# ## 17. åˆ†æç¸½çµèˆ‡ä¸‹ä¸€æ­¥

# %%
# ç¤¾å€ç´šå ±å‘Šç”Ÿæˆåˆ†æç¸½çµ
print("ğŸ“‹ ç¤¾å€ç´šå ±å‘Šç”Ÿæˆåˆ†æç¸½çµ")
print("=" * 80)

print("1ï¸âƒ£ å ±å‘Šç”Ÿæˆå®Œæˆåº¦:")
print(f"   âœ… 32æ¬„ä½çµæ§‹: å®Œæ•´å¯¦ç¾")
print(f"   âœ… åŸºæœ¬è³‡è¨Š (7æ¬„): å®Œæˆ")
print(f"   âœ… æ™‚é–“èˆ‡æ•¸é‡ (5æ¬„): å®Œæˆ")
print(f"   âœ… è§£ç´„è³‡è¨Š (6æ¬„): å®Œæˆ")
print(f"   âœ… å»åŒ–åˆ†æ (3æ¬„): å®Œæˆ")
print(f"   âœ… å»åŒ–å‹•æ…‹ (4æ¬„): å®Œæˆ")
print(f"   âœ… åƒ¹æ ¼åˆ†æ (3æ¬„): å®Œæˆ")
print(f"   âœ… éšæ®µåˆ†æ (3æ¬„): å®Œæˆ")
print(f"   âœ… å“è³ªæ§åˆ¶ (1æ¬„): å®Œæˆ")

print(f"\n2ï¸âƒ£ è³‡æ–™æ•´åˆçµ±è¨ˆ:")
print(f"   ğŸ“Š æœ€çµ‚è¨˜éŒ„æ•¸: {len(cleaned_report):,}")
print(f"   ğŸ“Š å¯¦éš›æ¬„ä½æ•¸: {len(cleaned_report.columns)}")

if 'basic_statistics' in report_summary:
    basic = report_summary['basic_statistics']
    print(f"   ğŸ“Š æ¶µè“‹å»ºæ¡ˆæ•¸: {basic.get('total_projects', 0):,}")
    print(f"   ğŸ“Š æ¶µè“‹å¹´å­£æ•¸: {basic.get('total_seasons', 0)}")
    print(f"   ğŸ“Š æ¶µè“‹ç¸£å¸‚æ•¸: {basic.get('counties_covered', 0)}")
    print(f"   ğŸ“Š æ¶µè“‹è¡Œæ”¿å€æ•¸: {basic.get('districts_covered', 0)}")

print(f"\n3ï¸âƒ£ è³‡æ–™å“è³ªè©•ä¼°:")
quality_score = quality_check_result.get('data_quality_score', 0)
print(f"   ğŸ“ˆ æ•´é«”å“è³ªåˆ†æ•¸: {quality_score:.1f}/100")

if quality_score >= 85:
    print(f"   âœ… å“è³ªè©•ç´š: å„ªç§€")
elif quality_score >= 70:
    print(f"   âš ï¸ å“è³ªè©•ç´š: è‰¯å¥½")
else:
    print(f"   âŒ å“è³ªè©•ç´š: éœ€æ”¹å–„")

if 'data_coverage' in report_summary:
    coverage = report_summary['data_coverage']
    total = report_summary['basic_statistics'].get('total_projects', 1)
    print(f"   ğŸ“Š å»ºæ¡ˆåç¨±å®Œæ•´åº¦: {coverage.get('projects_with_names', 0)/total*100:.1f}%")
    print(f"   ğŸ“Š åœ°ç†ä½ç½®å®Œæ•´åº¦: {coverage.get('projects_with_location', 0)/total*100:.1f}%")
    print(f"   ğŸ“Š äº¤æ˜“è¨˜éŒ„å®Œæ•´åº¦: {coverage.get('projects_with_transactions', 0)/total*100:.1f}%")

print(f"\n4ï¸âƒ£ æ ¸å¿ƒå¸‚å ´æŒ‡æ¨™:")
if 'market_overview' in report_summary:
    market = report_summary['market_overview']
    print(f"   ğŸ“Š å¹³å‡å»åŒ–ç‡: {market.get('avg_absorption_rate', 0):.1f}%")
    print(f"   ğŸ“Š å®Œå”®å»ºæ¡ˆæ•¸: {market.get('completed_projects', 0):,} å€‹")
    print(f"   ğŸ“Š å®Œå”®ç‡: {market.get('completion_rate', 0):.1f}%")
    print(f"   ğŸ“Š å¹³å‡éŠ·å”®å­£æ•¸: {market.get('avg_sales_seasons', 0):.1f} å­£")

if 'risk_analysis' in report_summary:
    risk = report_summary['risk_analysis']
    print(f"   ğŸ“Š æœ‰è§£ç´„å»ºæ¡ˆ: {risk.get('projects_with_cancellations', 0):,} å€‹")
    print(f"   ğŸ“Š å¹³å‡è§£ç´„ç‡: {risk.get('avg_cancellation_rate', 0):.2f}%")

print(f"\n5ï¸âƒ£ è³‡æ–™æ¸…ç†æˆæ•ˆ:")
print(f"   ğŸ§¹ æ¸…ç†æ“ä½œæ•¸: {len(cleaning_log)}")
print(f"   ğŸ§¹ æ¸…ç†å‰è¨˜éŒ„: {len(formatted_report):,}")
print(f"   ğŸ§¹ æ¸…ç†å¾Œè¨˜éŒ„: {len(cleaned_report):,}")
retention_rate = len(cleaned_report) / len(formatted_report) * 100 if len(formatted_report) > 0 else 0
print(f"   ğŸ§¹ è³‡æ–™ä¿ç•™ç‡: {retention_rate:.1f}%")

if cleaning_log:
    print(f"   ä¸»è¦æ¸…ç†æ“ä½œ:")
    for log in cleaning_log[:3]:  # é¡¯ç¤ºå‰3å€‹ä¸»è¦æ“ä½œ
        print(f"     â€¢ {log}")

print(f"\n6ï¸âƒ£ å‰åºåˆ†ææ•´åˆåº¦:")
integration_modules = [
    ('è§£ç´„åˆ†æ', not cancellation_analysis.empty),
    ('äº¤æ˜“å»é‡', not clean_transactions.empty),
    ('å»ºæ¡ˆæ•´åˆ', not project_integration.empty),
    ('å»åŒ–ç‡è¨ˆç®—', not absorption_analysis.empty),
    ('å»åŒ–å‹•æ…‹', not quarterly_speed.empty),
    ('éšæ®µåˆ¤æ–·', not sales_stage_analysis.empty),
    ('é¢¨éšªè©•ä¼°', not comprehensive_risk.empty)
]

integrated_count = sum(1 for _, status in integration_modules if status)
print(f"   ğŸ”— æ•´åˆæ¨¡çµ„æ•¸: {integrated_count}/{len(integration_modules)}")

for module, status in integration_modules:
    status_icon = "âœ…" if status else "âŒ"
    print(f"   {status_icon} {module}")

print(f"\n7ï¸âƒ£ è¼¸å‡ºæª”æ¡ˆå®Œæ•´æ€§:")
output_files = [
    '32æ¬„ä½å®Œæ•´å ±å‘Š',
    'å ±å‘Šæ‘˜è¦çµ±è¨ˆ',
    'è³‡æ–™å“è³ªæª¢æŸ¥çµæœ',
    'è³‡æ–™æ¸…ç†æ—¥èªŒ',
    'ç¯„ä¾‹å ±å‘Š',
    'æ¬„ä½èªªæ˜æ–‡æª”',
    'çµ±è¨ˆå ±å‘Š'
]

print(f"   ğŸ“ è¼¸å‡ºæª”æ¡ˆæ•¸: {len(output_files)}")
for file_type in output_files:
    print(f"   âœ… {file_type}")

print(f"\n8ï¸âƒ£ æ ¸å¿ƒåŠŸèƒ½é©—è­‰:")
core_functions = {
    '32æ¬„ä½çµæ§‹å®Œæ•´æ€§': len(cleaned_report.columns) >= 30,
    'åŸºæœ¬è³‡è¨Šå®Œæ•´åº¦': len(cleaned_report[cleaned_report['å‚™æŸ¥ç·¨è™Ÿ'] != '']) > 0,
    'å»åŒ–ç‡è¨ˆç®—æ­£ç¢ºæ€§': len(cleaned_report[cleaned_report['æ·¨å»åŒ–ç‡(%)'] >= 0]) > 0,
    'è§£ç´„åˆ†æå®Œæ•´æ€§': 'ç´¯ç©è§£ç´„ç‡(%)' in cleaned_report.columns,
    'åƒ¹æ ¼åˆ†ææœ‰æ•ˆæ€§': len(cleaned_report[cleaned_report['å¹³å‡äº¤æ˜“å–®åƒ¹(è¬/åª)'] > 0]) > 0,
    'éšæ®µåˆ¤æ–·è¦†è“‹åº¦': len(cleaned_report[cleaned_report['éŠ·å”®éšæ®µ'] != '']) > 0,
    'å“è³ªæ§åˆ¶æ©Ÿåˆ¶': 'is_complete_quarter' in cleaned_report.columns or 'æ˜¯å¦å®Œæ•´å­£' in cleaned_report.columns
}

print(f"æ ¸å¿ƒåŠŸèƒ½æª¢æŸ¥:")
for function, status in core_functions.items():
    status_icon = "âœ…" if status else "âŒ"
    print(f"   {status_icon} {function}")

all_functions_ready = all(core_functions.values())
if all_functions_ready:
    print(f"\nğŸ‰ æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½é©—è­‰é€šéï¼Œç¤¾å€ç´šå ±å‘Šç³»çµ±å®Œæ•´å°±ç·’")
else:
    missing_functions = [k for k, v in core_functions.items() if not v]
    print(f"\nâš ï¸ ä»¥ä¸‹åŠŸèƒ½éœ€è¦æª¢æŸ¥: {', '.join(missing_functions)}")

print(f"\n9ï¸âƒ£ å ±å‘Šä½¿ç”¨å»ºè­°:")
print(f"   ğŸ“‹ ä¸»è¦å ±å‘Šæª”æ¡ˆ: community_level_comprehensive_report_YYYYMMDD.csv")
print(f"   ğŸ“‹ å¿«é€Ÿé è¦½: community_report_sample_YYYYMMDD.csv (å‰100ç­†)")
print(f"   ğŸ“‹ æ¬„ä½èªªæ˜: community_report_column_definitions_YYYYMMDD.csv")
print(f"   ğŸ“‹ å“è³ªè©•ä¼°: data_quality_report_YYYYMMDD.json")

print(f"\nğŸ”Ÿ ä¸‹ä¸€æ­¥ç™¼å±•:")
print("   ğŸ¯ è¡Œæ”¿å€ç´šèšåˆåˆ†æ")
print("   ğŸ˜ï¸ ç¸£å¸‚ç´šç¸½é«”åˆ†æ")
print("   ğŸ“Š å‹•æ…‹ç›£æ§Dashboardé–‹ç™¼")
print("   ğŸ”® é æ¸¬æ¨¡å‹å»ºç«‹")
print("   ğŸ“ˆ é¢¨éšªé è­¦ç³»çµ±éƒ¨ç½²")
print("   ğŸŒŸ å®Œæ•´åˆ†æå¹³å°æ•´åˆ")

# %%
# æ ¸å¿ƒç®—æ³•èˆ‡è¨ˆç®—é‚è¼¯é©—è­‰
print(f"\nğŸ” æ ¸å¿ƒç®—æ³•èˆ‡è¨ˆç®—é‚è¼¯é©—è­‰:")

# é©—è­‰å»åŒ–ç‡è¨ˆç®—é‚è¼¯
valid_absorption_records = cleaned_report[
    (cleaned_report['æ¯›å»åŒ–ç‡(%)'] >= 0) & 
    (cleaned_report['æ·¨å»åŒ–ç‡(%)'] >= 0) &
    (cleaned_report['ç¸½æˆ¶æ•¸'] > 0)
]

if not valid_absorption_records.empty:
    # æª¢æŸ¥æ¯›å»åŒ–ç‡ >= æ·¨å»åŒ–ç‡
    logical_correct = len(valid_absorption_records[valid_absorption_records['æ¯›å»åŒ–ç‡(%)'] >= valid_absorption_records['æ·¨å»åŒ–ç‡(%)']])
    logical_rate = logical_correct / len(valid_absorption_records) * 100
    print(f"   âœ… å»åŒ–ç‡é‚è¼¯æ­£ç¢ºæ€§: {logical_rate:.1f}% ({logical_correct}/{len(valid_absorption_records)})")
    
    # æª¢æŸ¥å»åŒ–ç‡ä¸Šé™åˆç†æ€§
    reasonable_absorption = len(valid_absorption_records[valid_absorption_records['æ¯›å»åŒ–ç‡(%)'] <= 120])
    reasonable_rate = reasonable_absorption / len(valid_absorption_records) * 100
    print(f"   âœ… å»åŒ–ç‡æ•¸å€¼åˆç†æ€§: {reasonable_rate:.1f}% (â‰¤120%)")

# é©—è­‰è§£ç´„ç‡è¨ˆç®—é‚è¼¯  
valid_cancellation_records = cleaned_report[cleaned_report['ç´¯ç©è§£ç´„ç‡(%)'] >= 0]
if not valid_cancellation_records.empty:
    reasonable_cancellation = len(valid_cancellation_records[valid_cancellation_records['ç´¯ç©è§£ç´„ç‡(%)'] <= 100])
    cancellation_reasonable_rate = reasonable_cancellation / len(valid_cancellation_records) * 100
    print(f"   âœ… è§£ç´„ç‡æ•¸å€¼åˆç†æ€§: {cancellation_reasonable_rate:.1f}% (â‰¤100%)")

# é©—è­‰åƒ¹æ ¼æ•¸æ“šåˆç†æ€§
valid_price_records = cleaned_report[cleaned_report['å¹³å‡äº¤æ˜“å–®åƒ¹(è¬/åª)'] > 0]
if not valid_price_records.empty:
    reasonable_price = len(valid_price_records[
        (valid_price_records['å¹³å‡äº¤æ˜“å–®åƒ¹(è¬/åª)'] >= 10) & 
        (valid_price_records['å¹³å‡äº¤æ˜“å–®åƒ¹(è¬/åª)'] <= 300)
    ])
    price_reasonable_rate = reasonable_price / len(valid_price_records) * 100
    print(f"   âœ… åƒ¹æ ¼æ•¸æ“šåˆç†æ€§: {price_reasonable_rate:.1f}% (10-300è¬/åª)")

# æ•´é«”æ¼”ç®—æ³•æº–ç¢ºåº¦è©•ä¼°
overall_accuracy = (logical_rate + reasonable_rate + cancellation_reasonable_rate + price_reasonable_rate) / 4
print(f"   ğŸ“Š æ•´é«”æ¼”ç®—æ³•æº–ç¢ºåº¦: {overall_accuracy:.1f}%")

if overall_accuracy >= 95:
    print(f"   ğŸ¯ æ¼”ç®—æ³•å“è³ª: å„ªç§€")
elif overall_accuracy >= 85:
    print(f"   ğŸ¯ æ¼”ç®—æ³•å“è³ª: è‰¯å¥½")
else:
    print(f"   ğŸ¯ æ¼”ç®—æ³•å“è³ª: éœ€æ”¹å–„")

print("\n" + "="*80)
print("ğŸ‰ Notebook 8 - ç¤¾å€ç´šå ±å‘Šç”Ÿæˆå®Œæˆï¼")
print("ğŸ“ å·²å®Œæˆ32æ¬„ä½å®Œæ•´å ±å‘Šï¼Œæº–å‚™é€²è¡Œè¡Œæ”¿å€ç´šèˆ‡ç¸£å¸‚ç´šèšåˆåˆ†æ")
print("="*80)
                