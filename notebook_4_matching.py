# é å”®å±‹å¸‚å ´åˆ†æç³»çµ± - 04_å»ºæ¡ˆè³‡æ–™åŒ¹é…èˆ‡æ•´åˆ
# åŸºæ–¼ PRD v2.3 è¦æ ¼é€²è¡Œå»ºæ¡ˆè³‡è¨ŠåŒ¹é…èˆ‡æ´»èºå»ºæ¡ˆè­˜åˆ¥
# ================================================================================

# %% [markdown]
# # é å”®å±‹å¸‚å ´åˆ†æç³»çµ± - å»ºæ¡ˆè³‡æ–™åŒ¹é…èˆ‡æ•´åˆ
# 
# ## ğŸ“‹ ç›®æ¨™
# - âœ… å¯¦ä½œå»ºæ¡ˆè³‡æ–™åŒ¹é…é‚è¼¯
# - âœ… æ•´åˆé å”®å±‹è¨˜éŒ„èˆ‡å»ºæ¡ˆè³‡è¨Š
# - âœ… è™•ç†ç„¡åŒ¹é…å»ºæ¡ˆæƒ…æ³
# - âœ… æ´»èºå»ºæ¡ˆè­˜åˆ¥é‚è¼¯
# - âœ… é•·æœŸæ»¯éŠ·å»ºæ¡ˆæ¨™è¨˜
# - âœ… è³‡æ–™æ•´åˆå“è³ªé©—è­‰
# 
# ## ğŸ¯ å…§å®¹å¤§ç¶±
# 1. å»ºæ¡ˆç·¨è™ŸåŒ¹é…åˆ†æ
# 2. åœ°ç†è³‡è¨Šä¸€è‡´æ€§æª¢æŸ¥
# 3. ç¼ºå¤±å»ºæ¡ˆè³‡è¨Šè™•ç†ç­–ç•¥
# 4. æ´»èºå»ºæ¡ˆè­˜åˆ¥é‚è¼¯å¯¦ä½œ
# 5. é•·æœŸæ»¯éŠ·å»ºæ¡ˆæ¨™è¨˜
# 6. è³‡æ–™æ•´åˆå“è³ªé©—è­‰
# 7. æ•´åˆçµæœåˆ†æèˆ‡å„ªåŒ–
# 
# ## ğŸ“Š å»¶çºŒ Notebook 1-3 çš„åˆ†æçµæœ
# - ä¹¾æ·¨äº¤æ˜“è³‡æ–™: å»é‡å¾Œçš„æœ‰æ•ˆäº¤æ˜“è¨˜éŒ„
# - è§£ç´„åˆ†æçµæœ: è§£ç´„æ¨¡å¼èˆ‡é¢¨éšªè©•ä¼°
# - å»ºæ¡ˆåŸºæœ¬è³‡æ–™: 8,452ç­†å»ºæ¡ˆè³‡è¨Š
# - åŒ¹é…ç‡é æœŸ: æ¸¬è©¦ç’°å¢ƒ 10-15%ï¼Œæ­£å¼ç’°å¢ƒ 60-80%

# %% [markdown]
# ## 1. ç’°å¢ƒè¨­å®šèˆ‡è³‡æ–™è¼‰å…¥

# %%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import re
import warnings
from collections import Counter, defaultdict
from difflib import SequenceMatcher
warnings.filterwarnings('ignore')

# è¨­å®šé¡¯ç¤ºé¸é …
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)
pd.set_option('display.width', None)
pd.set_option('display.max_colwidth', 80)

# è¨­å®šä¸­æ–‡å­—å‹
plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'Arial Unicode MS', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False

# è¨­å®šåœ–è¡¨æ¨£å¼
sns.set_style("whitegrid")
plt.style.use('default')

print("âœ… ç’°å¢ƒè¨­å®šå®Œæˆ")
print(f"ğŸ“… åˆ†ææ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

# %%
# è¼‰å…¥å‰éšæ®µè™•ç†çµæœ
print("ğŸ”„ è¼‰å…¥å‰éšæ®µè™•ç†çµæœ...")

try:
    # è¼‰å…¥ä¹¾æ·¨çš„äº¤æ˜“è³‡æ–™ (ä¾†è‡ª Notebook 3)
    clean_transactions = pd.read_csv('../data/processed/03_clean_transactions.csv', encoding='utf-8')
    print(f"âœ… ä¹¾æ·¨äº¤æ˜“è³‡æ–™è¼‰å…¥æˆåŠŸ: {clean_transactions.shape}")
    
    # è¼‰å…¥åŸå§‹å»ºæ¡ˆè³‡æ–™
    project_data = pd.read_csv('../data/raw/lvr_sale_data_test.csv', encoding='utf-8')
    print(f"âœ… å»ºæ¡ˆåŸºæœ¬è³‡æ–™è¼‰å…¥æˆåŠŸ: {project_data.shape}")
    
    # è¼‰å…¥è§£ç´„åˆ†æçµæœ (ä¾†è‡ª Notebook 2)
    try:
        cancellation_analysis = pd.read_csv('../data/processed/02_cancellation_analysis.csv', encoding='utf-8')
        print(f"âœ… è§£ç´„åˆ†æçµæœè¼‰å…¥æˆåŠŸ: {cancellation_analysis.shape}")
    except FileNotFoundError:
        print("âš ï¸ æœªæ‰¾åˆ°è§£ç´„åˆ†æçµæœï¼Œå°‡é‡æ–°è¨ˆç®—")
        cancellation_analysis = None
        
except FileNotFoundError as e:
    print(f"âŒ æª”æ¡ˆè¼‰å…¥å¤±æ•—: {e}")
    print("ğŸ“ è«‹ç¢ºèªæ˜¯å¦å·²åŸ·è¡Œ Notebook 1-3")
except Exception as e:
    print(f"âŒ è¼‰å…¥éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")

# %%
# è³‡æ–™æ¦‚æ³æª¢è¦–
print("ğŸ“Š è³‡æ–™æ¦‚æ³æª¢è¦–")
print("=" * 60)

print("ä¹¾æ·¨äº¤æ˜“è³‡æ–™:")
print(f"   ç­†æ•¸: {len(clean_transactions):,}")
print(f"   æ¬„ä½: {list(clean_transactions.columns)}")
print(f"   å‚™æŸ¥ç·¨è™Ÿå”¯ä¸€å€¼: {clean_transactions['å‚™æŸ¥ç·¨è™Ÿ'].nunique():,}")

print(f"\nå»ºæ¡ˆåŸºæœ¬è³‡æ–™:")
print(f"   ç­†æ•¸: {len(project_data):,}")
print(f"   æ¬„ä½: {list(project_data.columns)}")
print(f"   ç·¨è™Ÿå”¯ä¸€å€¼: {project_data['ç·¨è™Ÿ'].nunique():,}")

# æª¢è¦–å»ºæ¡ˆè³‡æ–™æ¨£æœ¬
print(f"\nå»ºæ¡ˆè³‡æ–™æ¨£æœ¬ (å‰5ç­†):")
sample_projects = project_data[['ç·¨è™Ÿ', 'ç¤¾å€åç¨±', 'ç¸£å¸‚', 'è¡Œæ”¿å€', 'æˆ¶æ•¸', 'éŠ·å”®èµ·å§‹æ™‚é–“']].head()
for i, (idx, row) in enumerate(sample_projects.iterrows()):
    print(f"{i+1}. ç·¨è™Ÿ: {row['ç·¨è™Ÿ']} | ç¤¾å€: {row['ç¤¾å€åç¨±']} | æˆ¶æ•¸: {row['æˆ¶æ•¸']}")

# %% [markdown]
# ## 2. å»ºæ¡ˆç·¨è™ŸåŒ¹é…åˆ†æ

# %%
# å»ºæ¡ˆç·¨è™Ÿç›´æ¥åŒ¹é…åˆ†æ
print("ğŸ” å»ºæ¡ˆç·¨è™Ÿç›´æ¥åŒ¹é…åˆ†æ")
print("=" * 60)

# ç²å–å”¯ä¸€çš„å‚™æŸ¥ç·¨è™Ÿ
unique_transaction_codes = set(clean_transactions['å‚™æŸ¥ç·¨è™Ÿ'].unique())
unique_project_codes = set(project_data['ç·¨è™Ÿ'].unique())

print(f"äº¤æ˜“è³‡æ–™å‚™æŸ¥ç·¨è™Ÿæ•¸é‡: {len(unique_transaction_codes):,}")
print(f"å»ºæ¡ˆè³‡æ–™ç·¨è™Ÿæ•¸é‡: {len(unique_project_codes):,}")

# è¨ˆç®—ç›´æ¥åŒ¹é…çµæœ
direct_matches = unique_transaction_codes.intersection(unique_project_codes)
transaction_no_match = unique_transaction_codes - unique_project_codes
project_no_match = unique_project_codes - unique_transaction_codes

print(f"\nğŸ¯ ç›´æ¥åŒ¹é…çµæœ:")
print(f"   æˆåŠŸåŒ¹é…ç·¨è™Ÿ: {len(direct_matches):,}")
print(f"   åŒ¹é…ç‡: {len(direct_matches)/len(unique_transaction_codes)*100:.2f}%")
print(f"   äº¤æ˜“è³‡æ–™ç„¡åŒ¹é…: {len(transaction_no_match):,}")
print(f"   å»ºæ¡ˆè³‡æ–™ç„¡åŒ¹é…: {len(project_no_match):,}")

# è¨ˆç®—åŒ¹é…äº¤æ˜“ç­†æ•¸
matched_transactions = clean_transactions[clean_transactions['å‚™æŸ¥ç·¨è™Ÿ'].isin(direct_matches)]
print(f"   åŒ¹é…çš„äº¤æ˜“ç­†æ•¸: {len(matched_transactions):,} ({len(matched_transactions)/len(clean_transactions)*100:.2f}%)")

# %%
# è©³ç´°åŒ¹é…çµ±è¨ˆ
print(f"\nğŸ“Š è©³ç´°åŒ¹é…çµ±è¨ˆ:")

# æŒ‰ç¸£å¸‚åˆ†æåŒ¹é…æƒ…æ³
matching_by_city = {}
for city in clean_transactions['ç¸£å¸‚'].unique():
    city_transactions = clean_transactions[clean_transactions['ç¸£å¸‚'] == city]
    city_unique_codes = set(city_transactions['å‚™æŸ¥ç·¨è™Ÿ'].unique())
    city_matches = city_unique_codes.intersection(unique_project_codes)
    
    # è¨ˆç®—è©²ç¸£å¸‚çš„åŒ¹é…çµ±è¨ˆ
    city_matched_transactions = city_transactions[city_transactions['å‚™æŸ¥ç·¨è™Ÿ'].isin(city_matches)]
    
    matching_by_city[city] = {
        'total_transactions': len(city_transactions),
        'unique_codes': len(city_unique_codes),
        'matched_codes': len(city_matches),
        'matched_transactions': len(city_matched_transactions),
        'code_match_rate': len(city_matches) / len(city_unique_codes) * 100 if len(city_unique_codes) > 0 else 0,
        'transaction_match_rate': len(city_matched_transactions) / len(city_transactions) * 100 if len(city_transactions) > 0 else 0
    }

# é¡¯ç¤ºå„ç¸£å¸‚åŒ¹é…æƒ…æ³
print("å„ç¸£å¸‚åŒ¹é…æƒ…æ³:")
for city, stats in sorted(matching_by_city.items(), key=lambda x: x[1]['transaction_match_rate'], reverse=True):
    if stats['total_transactions'] >= 100:  # åªé¡¯ç¤ºäº¤æ˜“é‡è¼ƒå¤§çš„ç¸£å¸‚
        print(f"   {city:8s}: ç·¨è™ŸåŒ¹é…ç‡ {stats['code_match_rate']:5.1f}% | äº¤æ˜“åŒ¹é…ç‡ {stats['transaction_match_rate']:5.1f}% | äº¤æ˜“é‡ {stats['total_transactions']:,}")

# %%
# ç„¡åŒ¹é…äº¤æ˜“åˆ†æ
print(f"\nğŸ” ç„¡åŒ¹é…äº¤æ˜“åˆ†æ:")

unmatched_transactions = clean_transactions[~clean_transactions['å‚™æŸ¥ç·¨è™Ÿ'].isin(direct_matches)]
print(f"ç„¡åŒ¹é…äº¤æ˜“ç­†æ•¸: {len(unmatched_transactions):,}")

if len(unmatched_transactions) > 0:
    # ç„¡åŒ¹é…äº¤æ˜“çš„ç¸£å¸‚åˆ†å¸ƒ
    unmatched_city_dist = unmatched_transactions['ç¸£å¸‚'].value_counts()
    print(f"\nç„¡åŒ¹é…äº¤æ˜“ç¸£å¸‚åˆ†å¸ƒ (å‰10å):")
    for city, count in unmatched_city_dist.head(10).items():
        total_city_transactions = len(clean_transactions[clean_transactions['ç¸£å¸‚'] == city])
        percentage = count / total_city_transactions * 100
        print(f"   {city}: {count:,} ç­† ({percentage:.1f}%)")
    
    # ç„¡åŒ¹é…äº¤æ˜“çš„å¹´å­£åˆ†å¸ƒ
    unmatched_season_dist = unmatched_transactions['äº¤æ˜“å¹´å­£'].value_counts().sort_index()
    print(f"\nç„¡åŒ¹é…äº¤æ˜“å¹´å­£åˆ†å¸ƒ (å‰5å):")
    for season, count in unmatched_season_dist.head().items():
        total_season_transactions = len(clean_transactions[clean_transactions['äº¤æ˜“å¹´å­£'] == season])
        percentage = count / total_season_transactions * 100
        print(f"   {season}: {count:,} ç­† ({percentage:.1f}%)")

# %% [markdown]
# ## 3. åœ°ç†è³‡è¨Šä¸€è‡´æ€§æª¢æŸ¥

# %%
# åœ°ç†è³‡è¨Šä¸€è‡´æ€§æª¢æŸ¥
print("ğŸ—ºï¸ åœ°ç†è³‡è¨Šä¸€è‡´æ€§æª¢æŸ¥")
print("=" * 60)

def check_geographic_consistency(transaction_row, project_row):
    """
    æª¢æŸ¥äº¤æ˜“è¨˜éŒ„èˆ‡å»ºæ¡ˆè³‡æ–™çš„åœ°ç†è³‡è¨Šä¸€è‡´æ€§
    
    Args:
        transaction_row: äº¤æ˜“è¨˜éŒ„
        project_row: å»ºæ¡ˆè¨˜éŒ„
        
    Returns:
        dict: ä¸€è‡´æ€§æª¢æŸ¥çµæœ
    """
    result = {
        'county_match': False,
        'district_match': False,
        'street_similarity': 0.0,
        'overall_consistency': False
    }
    
    # ç¸£å¸‚ä¸€è‡´æ€§
    if str(transaction_row.get('ç¸£å¸‚', '')).strip() == str(project_row.get('ç¸£å¸‚', '')).strip():
        result['county_match'] = True
    
    # è¡Œæ”¿å€ä¸€è‡´æ€§
    if str(transaction_row.get('è¡Œæ”¿å€', '')).strip() == str(project_row.get('è¡Œæ”¿å€', '')).strip():
        result['district_match'] = True
    
    # è¡—é“ç›¸ä¼¼åº¦ (ä½¿ç”¨SequenceMatcher)
    trans_street = str(transaction_row.get('åè½è¡—é“', '')).strip()
    proj_street = str(project_row.get('åè½è¡—é“', '')).strip()
    
    if trans_street and proj_street:
        result['street_similarity'] = SequenceMatcher(None, trans_street, proj_street).ratio()
    
    # æ•´é«”ä¸€è‡´æ€§åˆ¤æ–·
    result['overall_consistency'] = (
        result['county_match'] and 
        result['district_match'] and 
        result['street_similarity'] > 0.6
    )
    
    return result

# %%
# å°åŒ¹é…çš„å»ºæ¡ˆé€²è¡Œåœ°ç†ä¸€è‡´æ€§æª¢æŸ¥
print("ğŸ”„ é€²è¡Œåœ°ç†ä¸€è‡´æ€§æª¢æŸ¥...")

geographic_consistency_results = []

# å»ºç«‹å»ºæ¡ˆè³‡æ–™çš„å¿«é€ŸæŸ¥æ‰¾å­—å…¸
project_lookup = {row['ç·¨è™Ÿ']: row for _, row in project_data.iterrows()}

# å°åŒ¹é…çš„äº¤æ˜“é€²è¡Œåœ°ç†ä¸€è‡´æ€§æª¢æŸ¥
for _, transaction in matched_transactions.head(1000).iterrows():  # å…ˆæª¢æŸ¥1000ç­†æ¨£æœ¬
    project_code = transaction['å‚™æŸ¥ç·¨è™Ÿ']
    
    if project_code in project_lookup:
        project_info = project_lookup[project_code]
        consistency = check_geographic_consistency(transaction, project_info)
        
        consistency_result = {
            'project_code': project_code,
            'transaction_county': transaction.get('ç¸£å¸‚', ''),
            'transaction_district': transaction.get('è¡Œæ”¿å€', ''),
            'transaction_street': transaction.get('åè½è¡—é“', ''),
            'project_county': project_info.get('ç¸£å¸‚', ''),
            'project_district': project_info.get('è¡Œæ”¿å€', ''),
            'project_street': project_info.get('åè½è¡—é“', ''),
            **consistency
        }
        
        geographic_consistency_results.append(consistency_result)

# è½‰æ›ç‚ºDataFrame
consistency_df = pd.DataFrame(geographic_consistency_results)

print(f"âœ… å®Œæˆ {len(consistency_df)} ç­†åœ°ç†ä¸€è‡´æ€§æª¢æŸ¥")

# %%
# åœ°ç†ä¸€è‡´æ€§çµ±è¨ˆåˆ†æ
print(f"\nğŸ“Š åœ°ç†ä¸€è‡´æ€§çµ±è¨ˆåˆ†æ:")

if not consistency_df.empty:
    county_match_rate = consistency_df['county_match'].mean() * 100
    district_match_rate = consistency_df['district_match'].mean() * 100
    overall_consistency_rate = consistency_df['overall_consistency'].mean() * 100
    avg_street_similarity = consistency_df['street_similarity'].mean() * 100
    
    print(f"   ç¸£å¸‚ä¸€è‡´ç‡: {county_match_rate:.1f}%")
    print(f"   è¡Œæ”¿å€ä¸€è‡´ç‡: {district_match_rate:.1f}%")
    print(f"   è¡—é“å¹³å‡ç›¸ä¼¼åº¦: {avg_street_similarity:.1f}%")
    print(f"   æ•´é«”ä¸€è‡´ç‡: {overall_consistency_rate:.1f}%")
    
    # ä¸ä¸€è‡´æ¡ˆä¾‹åˆ†æ
    inconsistent_cases = consistency_df[~consistency_df['overall_consistency']]
    if len(inconsistent_cases) > 0:
        print(f"\nâš ï¸ ç™¼ç¾ {len(inconsistent_cases)} ç­†åœ°ç†è³‡è¨Šä¸ä¸€è‡´æ¡ˆä¾‹")
        
        # é¡¯ç¤ºä¸ä¸€è‡´æ¡ˆä¾‹æ¨£æœ¬
        print(f"ä¸ä¸€è‡´æ¡ˆä¾‹æ¨£æœ¬ (å‰5ç­†):")
        for i, (_, case) in enumerate(inconsistent_cases.head().iterrows()):
            print(f"{i+1}. ç·¨è™Ÿ: {case['project_code']}")
            print(f"   äº¤æ˜“: {case['transaction_county']}/{case['transaction_district']}/{case['transaction_street']}")
            print(f"   å»ºæ¡ˆ: {case['project_county']}/{case['project_district']}/{case['project_street']}")
            print(f"   ä¸€è‡´æ€§: ç¸£å¸‚{'âœ“' if case['county_match'] else 'âœ—'} è¡Œæ”¿å€{'âœ“' if case['district_match'] else 'âœ—'} è¡—é“{case['street_similarity']:.2f}")

# %% [markdown]
# ## 4. ç¼ºå¤±å»ºæ¡ˆè³‡è¨Šè™•ç†ç­–ç•¥

# %%
# ç¼ºå¤±å»ºæ¡ˆè³‡è¨Šè™•ç†ç­–ç•¥
print("ğŸ”§ ç¼ºå¤±å»ºæ¡ˆè³‡è¨Šè™•ç†ç­–ç•¥")
print("=" * 60)

def estimate_missing_project_info(transaction_group):
    """
    æ ¹æ“šäº¤æ˜“è¨˜éŒ„æ¨ä¼°ç¼ºå¤±çš„å»ºæ¡ˆè³‡è¨Š
    
    Args:
        transaction_group: åŒä¸€å»ºæ¡ˆçš„æ‰€æœ‰äº¤æ˜“è¨˜éŒ„
        
    Returns:
        dict: æ¨ä¼°çš„å»ºæ¡ˆè³‡è¨Š
    """
    estimated_info = {
        'estimated_project_name': '',
        'estimated_total_units': 0,
        'estimated_start_date': '',
        'estimated_start_season': '',
        'estimation_confidence': 0.0,
        'transaction_count': len(transaction_group)
    }
    
    # æ¨ä¼°ç¤¾å€åç¨± (ä½¿ç”¨æœ€å¸¸è¦‹çš„åç¨±)
    if 'ç¤¾å€åç¨±' in transaction_group.columns:
        name_counts = transaction_group['ç¤¾å€åç¨±'].value_counts()
        if not name_counts.empty:
            estimated_info['estimated_project_name'] = name_counts.index[0]
    
    # æ¨ä¼°ç¸½æˆ¶æ•¸ (åŸºæ–¼äº¤æ˜“ç­†æ•¸çš„åˆç†å€æ•¸)
    transaction_count = len(transaction_group)
    # å‡è¨­é å”®å±‹å»åŒ–ç‡ç´„30-60%ï¼Œæ¨ä¼°ç¸½æˆ¶æ•¸
    estimated_total_units = int(transaction_count / 0.45)  # å‡è¨­45%å»åŒ–ç‡
    estimated_info['estimated_total_units'] = max(estimated_total_units, transaction_count + 10)
    
    # æ¨ä¼°éŠ·å”®èµ·å§‹æ™‚é–“ (ä½¿ç”¨æœ€æ—©äº¤æ˜“æ—¥æœŸå¾€å‰æ¨3-6å€‹æœˆ)
    earliest_date = transaction_group['äº¤æ˜“æ—¥æœŸ'].min()
    if pd.notna(earliest_date):
        estimated_start_date = earliest_date  # ç°¡åŒ–è™•ç†ï¼Œå¯¦éš›æ‡‰å¾€å‰æ¨
        estimated_info['estimated_start_date'] = estimated_start_date
        
        # è½‰æ›ç‚ºå¹´å­£
        if earliest_date:
            try:
                date_parts = earliest_date.split('/')
                if len(date_parts) >= 3:
                    year = int(date_parts[0]) - 1911  # è½‰ç‚ºæ°‘åœ‹å¹´
                    month = int(date_parts[1])
                    season = (month - 1) // 3 + 1
                    estimated_info['estimated_start_season'] = f"{year:03d}Y{season}S"
            except:
                pass
    
    # ä¿¡å¿ƒåº¦è©•ä¼° (åŸºæ–¼è³‡æ–™å®Œæ•´åº¦)
    confidence_factors = []
    
    # äº¤æ˜“ç­†æ•¸å……è¶³æ€§
    if transaction_count >= 10:
        confidence_factors.append(0.3)
    elif transaction_count >= 5:
        confidence_factors.append(0.2)
    else:
        confidence_factors.append(0.1)
    
    # åœ°ç†è³‡è¨Šä¸€è‡´æ€§
    if len(transaction_group['ç¸£å¸‚'].unique()) == 1 and len(transaction_group['è¡Œæ”¿å€'].unique()) == 1:
        confidence_factors.append(0.3)
    else:
        confidence_factors.append(0.1)
    
    # æ™‚é–“é›†ä¸­åº¦
    date_range = transaction_group['äº¤æ˜“å¹´å­£'].nunique()
    if date_range <= 4:  # é›†ä¸­åœ¨4å€‹å­£åº¦å…§
        confidence_factors.append(0.4)
    elif date_range <= 8:
        confidence_factors.append(0.2)
    else:
        confidence_factors.append(0.1)
    
    estimated_info['estimation_confidence'] = sum(confidence_factors)
    
    return estimated_info

# %%
# è™•ç†ç„¡åŒ¹é…å»ºæ¡ˆçš„è³‡è¨Šæ¨ä¼°
print("ğŸ”„ è™•ç†ç„¡åŒ¹é…å»ºæ¡ˆè³‡è¨Šæ¨ä¼°...")

unmatched_estimations = {}

# æŒ‰å‚™æŸ¥ç·¨è™Ÿåˆ†çµ„ç„¡åŒ¹é…äº¤æ˜“
unmatched_groups = unmatched_transactions.groupby('å‚™æŸ¥ç·¨è™Ÿ')

print(f"éœ€è¦æ¨ä¼°è³‡è¨Šçš„å»ºæ¡ˆæ•¸é‡: {len(unmatched_groups)}")

for project_code, group in unmatched_groups:
    estimated_info = estimate_missing_project_info(group)
    estimated_info['project_code'] = project_code
    
    # æ·»åŠ åŸºæœ¬åœ°ç†è³‡è¨Š
    estimated_info['county'] = group['ç¸£å¸‚'].iloc[0] if not group['ç¸£å¸‚'].empty else ''
    estimated_info['district'] = group['è¡Œæ”¿å€'].iloc[0] if not group['è¡Œæ”¿å€'].empty else ''
    estimated_info['street'] = group['åè½è¡—é“'].iloc[0] if not group['åè½è¡—é“'].empty else ''
    
    unmatched_estimations[project_code] = estimated_info

# è½‰æ›ç‚ºDataFrame
estimation_df = pd.DataFrame(list(unmatched_estimations.values()))

print(f"âœ… å®Œæˆ {len(estimation_df)} å€‹å»ºæ¡ˆè³‡è¨Šæ¨ä¼°")

# %%
# æ¨ä¼°çµæœçµ±è¨ˆåˆ†æ
print(f"\nğŸ“Š æ¨ä¼°çµæœçµ±è¨ˆåˆ†æ:")

if not estimation_df.empty:
    # ä¿¡å¿ƒåº¦åˆ†å¸ƒ
    high_confidence = estimation_df[estimation_df['estimation_confidence'] >= 0.8]
    medium_confidence = estimation_df[(estimation_df['estimation_confidence'] >= 0.5) & (estimation_df['estimation_confidence'] < 0.8)]
    low_confidence = estimation_df[estimation_df['estimation_confidence'] < 0.5]
    
    print(f"æ¨ä¼°ä¿¡å¿ƒåº¦åˆ†å¸ƒ:")
    print(f"   é«˜ä¿¡å¿ƒåº¦ (â‰¥80%): {len(high_confidence)} å€‹ ({len(high_confidence)/len(estimation_df)*100:.1f}%)")
    print(f"   ä¸­ä¿¡å¿ƒåº¦ (50-80%): {len(medium_confidence)} å€‹ ({len(medium_confidence)/len(estimation_df)*100:.1f}%)")
    print(f"   ä½ä¿¡å¿ƒåº¦ (<50%): {len(low_confidence)} å€‹ ({len(low_confidence)/len(estimation_df)*100:.1f}%)")
    
    # æ¨ä¼°æˆ¶æ•¸çµ±è¨ˆ
    print(f"\næ¨ä¼°æˆ¶æ•¸çµ±è¨ˆ:")
    print(f"   å¹³å‡æ¨ä¼°æˆ¶æ•¸: {estimation_df['estimated_total_units'].mean():.0f}")
    print(f"   æˆ¶æ•¸ç¯„åœ: {estimation_df['estimated_total_units'].min()} - {estimation_df['estimated_total_units'].max()}")
    
    # äº¤æ˜“ç­†æ•¸åˆ†å¸ƒ
    print(f"\näº¤æ˜“ç­†æ•¸åˆ†å¸ƒ:")
    print(f"   å¹³å‡æ¯å»ºæ¡ˆäº¤æ˜“ç­†æ•¸: {estimation_df['transaction_count'].mean():.1f}")
    print(f"   å–®ç­†äº¤æ˜“å»ºæ¡ˆ: {len(estimation_df[estimation_df['transaction_count'] == 1])} å€‹")
    print(f"   å¤šç­†äº¤æ˜“å»ºæ¡ˆ: {len(estimation_df[estimation_df['transaction_count'] > 1])} å€‹")
    
    # é¡¯ç¤ºæ¨ä¼°æ¨£æœ¬
    print(f"\næ¨ä¼°çµæœæ¨£æœ¬ (é«˜ä¿¡å¿ƒåº¦å‰5å€‹):")
    high_confidence_sample = high_confidence.head()
    for i, (_, row) in enumerate(high_confidence_sample.iterrows()):
        print(f"{i+1}. {row['project_code']} | æˆ¶æ•¸: {row['estimated_total_units']} | äº¤æ˜“: {row['transaction_count']} | ä¿¡å¿ƒåº¦: {row['estimation_confidence']:.2f}")

# %% [markdown]
# ## 5. æ´»èºå»ºæ¡ˆè­˜åˆ¥é‚è¼¯å¯¦ä½œ

# %%
# æ´»èºå»ºæ¡ˆè­˜åˆ¥é‚è¼¯å¯¦ä½œ
print("ğŸ¯ æ´»èºå»ºæ¡ˆè­˜åˆ¥é‚è¼¯å¯¦ä½œ")
print("=" * 60)

def identify_active_projects(target_season='113Y2S'):
    """
    æ ¹æ“šPRDè¦æ ¼è­˜åˆ¥æ´»èºéŠ·å”®å»ºæ¡ˆ
    
    æ´»èºéŠ·å”®å»ºæ¡ˆæ¨™æº–ï¼š
    (è©²å¹´å­£ >= éŠ·å”®èµ·å§‹å¹´å­£) AND (ç´¯ç©å»åŒ–ç‡ < 100%)
    
    Args:
        target_season: ç›®æ¨™åˆ†æå¹´å­£
        
    Returns:
        dict: æ´»èºå»ºæ¡ˆåˆ†æçµæœ
    """
    
    def season_to_number(season_str):
        """å°‡å¹´å­£å­—ä¸²è½‰æ›ç‚ºå¯æ¯”è¼ƒçš„æ•¸å­—"""
        try:
            # æ ¼å¼: "111Y1S" -> 1111
            year_part = season_str.split('Y')[0]
            season_part = season_str.split('Y')[1].replace('S', '')
            return int(year_part) * 10 + int(season_part)
        except:
            return 0
    
    target_season_num = season_to_number(target_season)
    
    active_projects = {}
    
    # è™•ç†æœ‰å®Œæ•´å»ºæ¡ˆè³‡è¨Šçš„é …ç›®
    for _, project in project_data.iterrows():
        project_code = project['ç·¨è™Ÿ']
        
        # ç²å–éŠ·å”®èµ·å§‹å¹´å­£
        start_season = project.get('éŠ·å”®èµ·å§‹å¹´å­£', '')
        if not start_season:
            continue
            
        start_season_num = season_to_number(start_season)
        
        # æª¢æŸ¥æ˜¯å¦åœ¨éŠ·å”®æœŸå…§
        if target_season_num >= start_season_num:
            # è¨ˆç®—è©²å»ºæ¡ˆçš„ç´¯ç©æˆäº¤æƒ…æ³
            project_transactions = clean_transactions[clean_transactions['å‚™æŸ¥ç·¨è™Ÿ'] == project_code]
            
            if len(project_transactions) > 0:
                # è¨ˆç®—ç´¯ç©å»åŒ–ç‡
                total_units = project.get('æˆ¶æ•¸', 0)
                if total_units > 0:
                    cumulative_sales = len(project_transactions)
                    absorption_rate = cumulative_sales / total_units * 100
                    
                    # åˆ¤æ–·æ˜¯å¦æ´»èº (ç´¯ç©å»åŒ–ç‡ < 100%)
                    is_active = absorption_rate < 100
                    
                    active_projects[project_code] = {
                        'project_name': project.get('ç¤¾å€åç¨±', ''),
                        'county': project.get('ç¸£å¸‚', ''),
                        'district': project.get('è¡Œæ”¿å€', ''),
                        'total_units': total_units,
                        'cumulative_sales': cumulative_sales,
                        'absorption_rate': absorption_rate,
                        'start_season': start_season,
                        'sales_seasons': target_season_num - start_season_num + 1,
                        'is_active': is_active,
                        'has_complete_info': True,
                        'transaction_count': len(project_transactions)
                    }
    
    # è™•ç†æ¨ä¼°å»ºæ¡ˆè³‡è¨Šçš„é …ç›®
    for _, estimation in estimation_df.iterrows():
        project_code = estimation['project_code']
        
        if project_code not in active_projects:  # é¿å…é‡è¤‡
            start_season = estimation.get('estimated_start_season', '')
            if start_season:
                start_season_num = season_to_number(start_season)
                
                if target_season_num >= start_season_num:
                    total_units = estimation['estimated_total_units']
                    cumulative_sales = estimation['transaction_count']
                    absorption_rate = cumulative_sales / total_units * 100 if total_units > 0 else 0
                    
                    is_active = absorption_rate < 100
                    
                    active_projects[project_code] = {
                        'project_name': estimation.get('estimated_project_name', ''),
                        'county': estimation.get('county', ''),
                        'district': estimation.get('district', ''),
                        'total_units': total_units,
                        'cumulative_sales': cumulative_sales,
                        'absorption_rate': absorption_rate,
                        'start_season': start_season,
                        'sales_seasons': target_season_num - start_season_num + 1,
                        'is_active': is_active,
                        'has_complete_info': False,
                        'estimation_confidence': estimation['estimation_confidence'],
                        'transaction_count': cumulative_sales
                    }
    
    return active_projects

# %%
# åŸ·è¡Œæ´»èºå»ºæ¡ˆè­˜åˆ¥
print("ğŸ”„ åŸ·è¡Œæ´»èºå»ºæ¡ˆè­˜åˆ¥...")

# åˆ†æç›®æ¨™å¹´å­£
target_season = '113Y2S'
active_projects_result = identify_active_projects(target_season)

print(f"âœ… å®Œæˆ {target_season} æ´»èºå»ºæ¡ˆè­˜åˆ¥")

# æ´»èºå»ºæ¡ˆçµ±è¨ˆ
active_count = sum(1 for p in active_projects_result.values() if p['is_active'])
total_analyzed = len(active_projects_result)
complete_info_count = sum(1 for p in active_projects_result.values() if p['has_complete_info'])
estimated_info_count = total_analyzed - complete_info_count

print(f"\nğŸ“Š æ´»èºå»ºæ¡ˆè­˜åˆ¥çµæœ:")
print(f"   ç¸½åˆ†æå»ºæ¡ˆæ•¸: {total_analyzed:,}")
print(f"   æ´»èºå»ºæ¡ˆæ•¸: {active_count:,} ({active_count/total_analyzed*100:.1f}%)")
print(f"   å®Œæ•´è³‡è¨Šå»ºæ¡ˆ: {complete_info_count:,}")
print(f"   æ¨ä¼°è³‡è¨Šå»ºæ¡ˆ: {estimated_info_count:,}")

# %%
# æ´»èºå»ºæ¡ˆè©³ç´°åˆ†æ
print(f"\nğŸ” æ´»èºå»ºæ¡ˆè©³ç´°åˆ†æ:")

if active_projects_result:
    # è½‰æ›ç‚ºDataFrameä»¥ä¾¿åˆ†æ
    active_df = pd.DataFrame(list(active_projects_result.values()))
    active_df['project_code'] = list(active_projects_result.keys())
    
    # åªåˆ†ææ´»èºå»ºæ¡ˆ
    truly_active = active_df[active_df['is_active']].copy()
    
    if not truly_active.empty:
        print(f"æ´»èºå»ºæ¡ˆç‰¹å¾µåˆ†æ:")
        print(f"   å¹³å‡æˆ¶æ•¸: {truly_active['total_units'].mean():.0f}")
        print(f"   å¹³å‡ç´¯ç©éŠ·å”®: {truly_active['cumulative_sales'].mean():.0f}")
        print(f"   å¹³å‡å»åŒ–ç‡: {truly_active['absorption_rate'].mean():.1f}%")
        print(f"   å¹³å‡éŠ·å”®å­£æ•¸: {truly_active['sales_seasons'].mean():.1f}")
        
        # æŒ‰ç¸£å¸‚åˆ†å¸ƒ
        city_distribution = truly_active['county'].value_counts()
        print(f"\næ´»èºå»ºæ¡ˆç¸£å¸‚åˆ†å¸ƒ (å‰10å):")
        for city, count in city_distribution.head(10).items():
            percentage = count / len(truly_active) * 100
            print(f"   {city}: {count} å€‹ ({percentage:.1f}%)")
        
        # å»åŒ–ç‡åˆ†å¸ƒ
        print(f"\nå»åŒ–ç‡åˆ†å¸ƒ:")
        low_absorption = len(truly_active[truly_active['absorption_rate'] < 30])
        medium_absorption = len(truly_active[(truly_active['absorption_rate'] >= 30) & (truly_active['absorption_rate'] < 70)])
        high_absorption = len(truly_active[truly_active['absorption_rate'] >= 70])
        
        print(f"   ä½å»åŒ–ç‡ (<30%): {low_absorption} å€‹ ({low_absorption/len(truly_active)*100:.1f}%)")
        print(f"   ä¸­å»åŒ–ç‡ (30-70%): {medium_absorption} å€‹ ({medium_absorption/len(truly_active)*100:.1f}%)")
        print(f"   é«˜å»åŒ–ç‡ (â‰¥70%): {high_absorption} å€‹ ({high_absorption/len(truly_active)*100:.1f}%)")
        
        # é¡¯ç¤ºæ´»èºå»ºæ¡ˆæ¨£æœ¬
        print(f"\næ´»èºå»ºæ¡ˆæ¨£æœ¬ (å»åŒ–ç‡æ’åºå‰10å€‹):")
        top_active = truly_active.nlargest(10, 'absorption_rate')
        for i, (_, row) in enumerate(top_active.iterrows()):
            info_type = "å®Œæ•´" if row['has_complete_info'] else "æ¨ä¼°"
            print(f"{i+1:2d}. {row['project_code']} | {row['county']}/{row['district']} | æˆ¶æ•¸: {row['total_units']:3.0f} | å»åŒ–: {row['absorption_rate']:5.1f}% | {info_type}")

# %% [markdown]
# ## 6. é•·æœŸæ»¯éŠ·å»ºæ¡ˆæ¨™è¨˜

# %%
# é•·æœŸæ»¯éŠ·å»ºæ¡ˆæ¨™è¨˜é‚è¼¯
print("âš ï¸ é•·æœŸæ»¯éŠ·å»ºæ¡ˆæ¨™è¨˜é‚è¼¯")
print("=" * 60)

def identify_stagnant_projects(active_projects_dict, target_season='113Y2S'):
    """
    è­˜åˆ¥é•·æœŸæ»¯éŠ·å»ºæ¡ˆ
    
    é•·æœŸæ»¯éŠ·æ¨™æº– (PRDè¦æ ¼)ï¼š
    - éŠ·å”®æœŸé–“ > 12å­£ (3å¹´)
    - é€£çºŒ12å­£ç„¡æˆäº¤
    - ç´¯ç©å»åŒ–ç‡ < 70%
    
    Args:
        active_projects_dict: æ´»èºå»ºæ¡ˆå­—å…¸
        target_season: ç›®æ¨™å¹´å­£
        
    Returns:
        dict: æ»¯éŠ·å»ºæ¡ˆåˆ†æçµæœ
    """
    
    def calculate_no_transaction_seasons(project_code, target_season):
        """è¨ˆç®—é€£çºŒç„¡æˆäº¤å­£æ•¸"""
        # ç²å–è©²å»ºæ¡ˆçš„æ‰€æœ‰äº¤æ˜“è¨˜éŒ„
        project_transactions = clean_transactions[clean_transactions['å‚™æŸ¥ç·¨è™Ÿ'] == project_code]
        
        if len(project_transactions) == 0:
            return 12  # å¦‚æœå®Œå…¨ç„¡äº¤æ˜“ï¼Œå‡è¨­ç‚º12å­£
        
        # ç²å–æœ€è¿‘äº¤æ˜“å¹´å­£
        latest_transaction_season = project_transactions['äº¤æ˜“å¹´å­£'].max()
        
        # ç°¡åŒ–è™•ç†ï¼šæ¯”è¼ƒå¹´å­£å­—ä¸² (å¯¦éš›æ‡‰è½‰æ›ç‚ºæ•¸å­—æ¯”è¼ƒ)
        # é€™è£¡å‡è¨­å¦‚æœæœ€è¿‘äº¤æ˜“æ˜¯å¾ˆæ—©æœŸï¼Œå‰‡ç„¡æˆäº¤å­£æ•¸è¼ƒé«˜
        def season_to_number(season_str):
            try:
                year_part = season_str.split('Y')[0]
                season_part = season_str.split('Y')[1].replace('S', '')
                return int(year_part) * 10 + int(season_part)
            except:
                return 0
        
        latest_num = season_to_number(latest_transaction_season)
        target_num = season_to_number(target_season)
        
        # è¨ˆç®—å­£åº¦å·®è·
        seasons_diff = target_num - latest_num
        return max(0, seasons_diff)
    
    stagnant_projects = {}
    
    for project_code, project_info in active_projects_dict.items():
        if project_info['is_active']:  # åªæª¢æŸ¥æ´»èºå»ºæ¡ˆ
            sales_seasons = project_info['sales_seasons']
            absorption_rate = project_info['absorption_rate']
            
            # è¨ˆç®—é€£çºŒç„¡æˆäº¤å­£æ•¸
            no_transaction_seasons = calculate_no_transaction_seasons(project_code, target_season)
            
            # åˆ¤æ–·æ˜¯å¦ç‚ºé•·æœŸæ»¯éŠ·
            is_long_term_stagnant = (
                sales_seasons > 12 and          # éŠ·å”®æœŸé–“ > 12å­£
                no_transaction_seasons >= 12 and # é€£çºŒ12å­£ç„¡æˆäº¤
                absorption_rate < 70             # ç´¯ç©å»åŒ–ç‡ < 70%
            )
            
            # è¨ˆç®—æ»¯éŠ·é¢¨éšªåˆ†æ•¸
            risk_score = 0
            if sales_seasons > 12:
                risk_score += 1
            if sales_seasons > 16:
                risk_score += 1
            if no_transaction_seasons >= 8:
                risk_score += 1
            if no_transaction_seasons >= 12:
                risk_score += 1
            if absorption_rate < 30:
                risk_score += 2
            elif absorption_rate < 50:
                risk_score += 1
            
            stagnant_info = {
                **project_info,
                'no_transaction_seasons': no_transaction_seasons,
                'is_long_term_stagnant': is_long_term_stagnant,
                'stagnant_risk_score': risk_score,
                'stagnant_risk_level': 'High' if risk_score >= 4 else 'Medium' if risk_score >= 2 else 'Low'
            }
            
            stagnant_projects[project_code] = stagnant_info
    
    return stagnant_projects

# %%
# åŸ·è¡Œé•·æœŸæ»¯éŠ·å»ºæ¡ˆæ¨™è¨˜
print("ğŸ”„ åŸ·è¡Œé•·æœŸæ»¯éŠ·å»ºæ¡ˆè­˜åˆ¥...")

stagnant_analysis_result = identify_stagnant_projects(active_projects_result, target_season)

# çµ±è¨ˆæ»¯éŠ·å»ºæ¡ˆ
total_active = len(stagnant_analysis_result)
long_term_stagnant = sum(1 for p in stagnant_analysis_result.values() if p['is_long_term_stagnant'])
high_risk_stagnant = sum(1 for p in stagnant_analysis_result.values() if p['stagnant_risk_level'] == 'High')
medium_risk_stagnant = sum(1 for p in stagnant_analysis_result.values() if p['stagnant_risk_level'] == 'Medium')

print(f"âœ… å®Œæˆé•·æœŸæ»¯éŠ·å»ºæ¡ˆè­˜åˆ¥")
print(f"\nğŸ“Š æ»¯éŠ·å»ºæ¡ˆçµ±è¨ˆ:")
print(f"   ç¸½æ´»èºå»ºæ¡ˆ: {total_active:,}")
print(f"   é•·æœŸæ»¯éŠ·å»ºæ¡ˆ: {long_term_stagnant:,} ({long_term_stagnant/total_active*100:.1f}%)")
print(f"   é«˜é¢¨éšªå»ºæ¡ˆ: {high_risk_stagnant:,} ({high_risk_stagnant/total_active*100:.1f}%)")
print(f"   ä¸­é¢¨éšªå»ºæ¡ˆ: {medium_risk_stagnant:,} ({medium_risk_stagnant/total_active*100:.1f}%)")

# %%
# æ»¯éŠ·å»ºæ¡ˆè©³ç´°åˆ†æ
print(f"\nğŸ” æ»¯éŠ·å»ºæ¡ˆè©³ç´°åˆ†æ:")

if stagnant_analysis_result:
    # è½‰æ›ç‚ºDataFrame
    stagnant_df = pd.DataFrame(list(stagnant_analysis_result.values()))
    stagnant_df['project_code'] = list(stagnant_analysis_result.keys())
    
    # åˆ†æé•·æœŸæ»¯éŠ·å»ºæ¡ˆ
    long_stagnant = stagnant_df[stagnant_df['is_long_term_stagnant']].copy()
    
    if not long_stagnant.empty:
        print(f"é•·æœŸæ»¯éŠ·å»ºæ¡ˆç‰¹å¾µ:")
        print(f"   å¹³å‡éŠ·å”®å­£æ•¸: {long_stagnant['sales_seasons'].mean():.1f}")
        print(f"   å¹³å‡å»åŒ–ç‡: {long_stagnant['absorption_rate'].mean():.1f}%")
        print(f"   å¹³å‡ç„¡æˆäº¤å­£æ•¸: {long_stagnant['no_transaction_seasons'].mean():.1f}")
        print(f"   å¹³å‡é¢¨éšªåˆ†æ•¸: {long_stagnant['stagnant_risk_score'].mean():.1f}")
        
        # ç¸£å¸‚åˆ†å¸ƒ
        stagnant_city_dist = long_stagnant['county'].value_counts()
        print(f"\né•·æœŸæ»¯éŠ·å»ºæ¡ˆç¸£å¸‚åˆ†å¸ƒ:")
        for city, count in stagnant_city_dist.items():
            total_city_active = len(stagnant_df[stagnant_df['county'] == city])
            percentage = count / total_city_active * 100 if total_city_active > 0 else 0
            print(f"   {city}: {count} å€‹ (å è©²ç¸£å¸‚æ´»èºå»ºæ¡ˆ {percentage:.1f}%)")
    
    # åˆ†æé«˜é¢¨éšªå»ºæ¡ˆ
    high_risk = stagnant_df[stagnant_df['stagnant_risk_level'] == 'High'].copy()
    
    if not high_risk.empty:
        print(f"\né«˜é¢¨éšªå»ºæ¡ˆæ¨£æœ¬ (å‰10å€‹):")
        high_risk_sorted = high_risk.nlargest(10, 'stagnant_risk_score')
        for i, (_, row) in enumerate(high_risk_sorted.iterrows()):
            info_type = "å®Œæ•´" if row['has_complete_info'] else "æ¨ä¼°"
            print(f"{i+1:2d}. {row['project_code']} | {row['county']}/{row['district']} | éŠ·å”®: {row['sales_seasons']:2.0f}å­£ | å»åŒ–: {row['absorption_rate']:5.1f}% | é¢¨éšª: {row['stagnant_risk_score']}")

# %% [markdown]
# ## 7. è³‡æ–™æ•´åˆå“è³ªé©—è­‰

# %%
# è³‡æ–™æ•´åˆå“è³ªé©—è­‰
print("ğŸ” è³‡æ–™æ•´åˆå“è³ªé©—è­‰")
print("=" * 60)

def validate_integration_quality():
    """
    é©—è­‰è³‡æ–™æ•´åˆçš„å“è³ª
    
    Returns:
        dict: å“è³ªé©—è­‰çµæœ
    """
    
    validation_results = {
        'total_projects_analyzed': len(active_projects_result),
        'projects_with_complete_info': 0,
        'projects_with_estimated_info': 0,
        'geographic_consistency_rate': 0.0,
        'active_projects_count': 0,
        'stagnant_projects_count': 0,
        'data_coverage_rate': 0.0,
        'quality_issues': []
    }
    
    # çµ±è¨ˆè³‡è¨Šå®Œæ•´æ€§
    for project_info in active_projects_result.values():
        if project_info['has_complete_info']:
            validation_results['projects_with_complete_info'] += 1
        else:
            validation_results['projects_with_estimated_info'] += 1
    
    # è¨ˆç®—åœ°ç†ä¸€è‡´æ€§ç‡ (åŸºæ–¼å‰é¢çš„æª¢æŸ¥çµæœ)
    if not consistency_df.empty:
        validation_results['geographic_consistency_rate'] = consistency_df['overall_consistency'].mean() * 100
    
    # çµ±è¨ˆæ´»èºå»ºæ¡ˆå’Œæ»¯éŠ·å»ºæ¡ˆ
    validation_results['active_projects_count'] = sum(1 for p in active_projects_result.values() if p['is_active'])
    validation_results['stagnant_projects_count'] = sum(1 for p in stagnant_analysis_result.values() if p['is_long_term_stagnant'])
    
    # è¨ˆç®—è³‡æ–™è¦†è“‹ç‡
    total_unique_codes = clean_transactions['å‚™æŸ¥ç·¨è™Ÿ'].nunique()
    validation_results['data_coverage_rate'] = len(active_projects_result) / total_unique_codes * 100
    
    # å“è³ªå•é¡Œæª¢æŸ¥
    quality_issues = []
    
    # æª¢æŸ¥1: ä½ä¿¡å¿ƒåº¦æ¨ä¼°æ¯”ä¾‹
    if estimation_df is not None and not estimation_df.empty:
        low_confidence_count = len(estimation_df[estimation_df['estimation_confidence'] < 0.5])
        low_confidence_rate = low_confidence_count / len(estimation_df) * 100
        if low_confidence_rate > 30:
            quality_issues.append(f"ä½ä¿¡å¿ƒåº¦æ¨ä¼°æ¯”ä¾‹éé«˜: {low_confidence_rate:.1f}%")
    
    # æª¢æŸ¥2: åœ°ç†ä¸€è‡´æ€§å•é¡Œ
    if validation_results['geographic_consistency_rate'] < 80:
        quality_issues.append(f"åœ°ç†ä¸€è‡´æ€§ç‡åä½: {validation_results['geographic_consistency_rate']:.1f}%")
    
    # æª¢æŸ¥3: è³‡æ–™è¦†è“‹ç‡å•é¡Œ
    if validation_results['data_coverage_rate'] < 50:
        quality_issues.append(f"è³‡æ–™è¦†è“‹ç‡åä½: {validation_results['data_coverage_rate']:.1f}%")
    
    # æª¢æŸ¥4: ç•°å¸¸å»åŒ–ç‡
    if active_projects_result:
        absorption_rates = [p['absorption_rate'] for p in active_projects_result.values()]
        extreme_high = sum(1 for rate in absorption_rates if rate > 150)
        if extreme_high > 0:
            quality_issues.append(f"ç™¼ç¾ {extreme_high} å€‹å»ºæ¡ˆå»åŒ–ç‡è¶…é150%")
    
    validation_results['quality_issues'] = quality_issues
    
    return validation_results

# %%
# åŸ·è¡Œå“è³ªé©—è­‰
print("ğŸ”„ åŸ·è¡Œè³‡æ–™æ•´åˆå“è³ªé©—è­‰...")

quality_validation = validate_integration_quality()

print(f"âœ… å“è³ªé©—è­‰å®Œæˆ")
print(f"\nğŸ“Š æ•´åˆå“è³ªå ±å‘Š:")
print(f"   ç¸½åˆ†æå»ºæ¡ˆæ•¸: {quality_validation['total_projects_analyzed']:,}")
print(f"   å®Œæ•´è³‡è¨Šå»ºæ¡ˆ: {quality_validation['projects_with_complete_info']:,}")
print(f"   æ¨ä¼°è³‡è¨Šå»ºæ¡ˆ: {quality_validation['projects_with_estimated_info']:,}")
print(f"   åœ°ç†ä¸€è‡´æ€§ç‡: {quality_validation['geographic_consistency_rate']:.1f}%")
print(f"   è³‡æ–™è¦†è“‹ç‡: {quality_validation['data_coverage_rate']:.1f}%")
print(f"   æ´»èºå»ºæ¡ˆæ•¸: {quality_validation['active_projects_count']:,}")
print(f"   æ»¯éŠ·å»ºæ¡ˆæ•¸: {quality_validation['stagnant_projects_count']:,}")

# å“è³ªå•é¡Œå ±å‘Š
if quality_validation['quality_issues']:
    print(f"\nâš ï¸ ç™¼ç¾å“è³ªå•é¡Œ:")
    for i, issue in enumerate(quality_validation['quality_issues'], 1):
        print(f"   {i}. {issue}")
else:
    print(f"\nâœ… æœªç™¼ç¾é‡å¤§å“è³ªå•é¡Œ")

# %%
# é—œéµæŒ‡æ¨™è¨ˆç®—èˆ‡é©—è­‰
print(f"\nğŸ¯ é—œéµæŒ‡æ¨™è¨ˆç®—èˆ‡é©—è­‰:")

# è¨ˆç®—æ•´é«”å¸‚å ´æŒ‡æ¨™
if active_projects_result:
    market_indicators = {
        'total_active_units': sum(p['total_units'] for p in active_projects_result.values() if p['is_active']),
        'total_sold_units': sum(p['cumulative_sales'] for p in active_projects_result.values() if p['is_active']),
        'overall_absorption_rate': 0,
        'average_sales_seasons': np.mean([p['sales_seasons'] for p in active_projects_result.values() if p['is_active']]),
        'stagnant_impact_ratio': 0
    }
    
    if market_indicators['total_active_units'] > 0:
        market_indicators['overall_absorption_rate'] = market_indicators['total_sold_units'] / market_indicators['total_active_units'] * 100
    
    if quality_validation['active_projects_count'] > 0:
        market_indicators['stagnant_impact_ratio'] = quality_validation['stagnant_projects_count'] / quality_validation['active_projects_count'] * 100
    
    print(f"å¸‚å ´æ•´é«”æŒ‡æ¨™:")
    print(f"   ç¸½æ´»èºæˆ¶æ•¸: {market_indicators['total_active_units']:,}")
    print(f"   ç¸½éŠ·å”®æˆ¶æ•¸: {market_indicators['total_sold_units']:,}")
    print(f"   æ•´é«”å»åŒ–ç‡: {market_indicators['overall_absorption_rate']:.1f}%")
    print(f"   å¹³å‡éŠ·å”®å­£æ•¸: {market_indicators['average_sales_seasons']:.1f}")
    print(f"   æ»¯éŠ·å½±éŸ¿æ¯”ä¾‹: {market_indicators['stagnant_impact_ratio']:.1f}%")

# %% [markdown]
# ## 8. è¦–è¦ºåŒ–åˆ†æ

# %%
# å‰µå»ºæ•´åˆçµæœè¦–è¦ºåŒ–
print("ğŸ“Š å»ºæ¡ˆæ•´åˆçµæœè¦–è¦ºåŒ–åˆ†æ")
print("=" * 50)

# å‰µå»ºåœ–è¡¨
fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# 1. åŒ¹é…æƒ…æ³åˆ†å¸ƒ
matching_categories = ['ç›´æ¥åŒ¹é…', 'æ¨ä¼°åŒ¹é…']
matching_counts = [quality_validation['projects_with_complete_info'], quality_validation['projects_with_estimated_info']]

bars1 = axes[0, 0].bar(matching_categories, matching_counts, color=['skyblue', 'lightcoral'])
axes[0, 0].set_title('å»ºæ¡ˆè³‡è¨ŠåŒ¹é…æƒ…æ³', fontsize=14, fontweight='bold')
axes[0, 0].set_ylabel('å»ºæ¡ˆæ•¸é‡')

# æ·»åŠ æ•¸å€¼æ¨™ç±¤
for bar in bars1:
    height = bar.get_height()
    axes[0, 0].text(bar.get_x() + bar.get_width()/2., height,
                   f'{int(height)}', ha='center', va='bottom')

# 2. æ´»èºå»ºæ¡ˆç¸£å¸‚åˆ†å¸ƒ
if 'active_df' in locals() and not active_df.empty:
    truly_active = active_df[active_df['is_active']]
    city_dist = truly_active['county'].value_counts().head(8)
    
    bars2 = axes[0, 1].bar(range(len(city_dist)), city_dist.values, color='lightgreen')
    axes[0, 1].set_title('æ´»èºå»ºæ¡ˆç¸£å¸‚åˆ†å¸ƒ (å‰8å)', fontsize=14, fontweight='bold')
    axes[0, 1].set_xlabel('ç¸£å¸‚')
    axes[0, 1].set_ylabel('æ´»èºå»ºæ¡ˆæ•¸')
    axes[0, 1].set_xticks(range(len(city_dist)))
    axes[0, 1].set_xticklabels(city_dist.index, rotation=45, ha='right')
    
    # æ·»åŠ æ•¸å€¼æ¨™ç±¤
    for bar in bars2:
        height = bar.get_height()
        axes[0, 1].text(bar.get_x() + bar.get_width()/2., height,
                       f'{int(height)}', ha='center', va='bottom')

# 3. å»åŒ–ç‡åˆ†å¸ƒ
if 'truly_active' in locals() and not truly_active.empty:
    absorption_ranges = ['<30%', '30-50%', '50-70%', '70-90%', 'â‰¥90%']
    absorption_counts = [
        len(truly_active[truly_active['absorption_rate'] < 30]),
        len(truly_active[(truly_active['absorption_rate'] >= 30) & (truly_active['absorption_rate'] < 50)]),
        len(truly_active[(truly_active['absorption_rate'] >= 50) & (truly_active['absorption_rate'] < 70)]),
        len(truly_active[(truly_active['absorption_rate'] >= 70) & (truly_active['absorption_rate'] < 90)]),
        len(truly_active[truly_active['absorption_rate'] >= 90])
    ]
    
    bars3 = axes[0, 2].bar(absorption_ranges, absorption_counts, color='orange')
    axes[0, 2].set_title('å»åŒ–ç‡åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    axes[0, 2].set_xlabel('å»åŒ–ç‡ç¯„åœ')
    axes[0, 2].set_ylabel('å»ºæ¡ˆæ•¸é‡')
    axes[0, 2].tick_params(axis='x', rotation=45)
    
    # æ·»åŠ æ•¸å€¼æ¨™ç±¤
    for bar in bars3:
        height = bar.get_height()
        axes[0, 2].text(bar.get_x() + bar.get_width()/2., height,
                       f'{int(height)}', ha='center', va='bottom')

# 4. æ»¯éŠ·é¢¨éšªåˆ†å¸ƒ
if 'stagnant_df' in locals() and not stagnant_df.empty:
    risk_dist = stagnant_df['stagnant_risk_level'].value_counts()
    colors = {'Low': 'lightgreen', 'Medium': 'orange', 'High': 'red'}
    bar_colors = [colors.get(level, 'gray') for level in risk_dist.index]
    
    bars4 = axes[1, 0].bar(risk_dist.index, risk_dist.values, color=bar_colors)
    axes[1, 0].set_title('æ»¯éŠ·é¢¨éšªåˆ†å¸ƒ', fontsize=14, fontweight='bold')
    axes[1, 0].set_xlabel('é¢¨éšªç­‰ç´š')
    axes[1, 0].set_ylabel('å»ºæ¡ˆæ•¸é‡')
    
    # æ·»åŠ æ•¸å€¼æ¨™ç±¤
    for bar in bars4:
        height = bar.get_height()
        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height,
                       f'{int(height)}', ha='center', va='bottom')

# 5. éŠ·å”®å­£æ•¸åˆ†å¸ƒ
if 'truly_active' in locals() and not truly_active.empty:
    axes[1, 1].hist(truly_active['sales_seasons'], bins=20, color='lightblue', alpha=0.7, edgecolor='black')
    axes[1, 1].set_title('éŠ·å”®å­£æ•¸åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    axes[1, 1].set_xlabel('éŠ·å”®å­£æ•¸')
    axes[1, 1].set_ylabel('å»ºæ¡ˆæ•¸é‡')
    axes[1, 1].axvline(x=12, color='red', linestyle='--', label='é•·æœŸéŠ·å”®æ¨™æº–ç·š')
    axes[1, 1].legend()

# 6. å“è³ªæŒ‡æ¨™é›·é”åœ–
if quality_validation:
    categories = ['è³‡æ–™è¦†è“‹ç‡', 'åœ°ç†ä¸€è‡´æ€§', 'å®Œæ•´è³‡è¨Šæ¯”ä¾‹', 'æ´»èºè­˜åˆ¥ç‡', 'é¢¨éšªè­˜åˆ¥ç‡']
    
    # è¨ˆç®—å„é …å¾—åˆ† (è½‰æ›ç‚º0-100åˆ†)
    scores = [
        quality_validation['data_coverage_rate'],
        quality_validation['geographic_consistency_rate'],
        quality_validation['projects_with_complete_info'] / max(quality_validation['total_projects_analyzed'], 1) * 100,
        quality_validation['active_projects_count'] / max(quality_validation['total_projects_analyzed'], 1) * 100,
        80 if len(quality_validation['quality_issues']) == 0 else max(20, 80 - len(quality_validation['quality_issues']) * 15)
    ]
    
    # ç°¡åŒ–çš„é›·é”åœ–ï¼ˆä½¿ç”¨æ¥µåæ¨™ï¼‰
    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
    scores_plot = scores + [scores[0]]  # é–‰åˆåœ–å½¢
    angles += [angles[0]]
    
    ax_polar = plt.subplot(2, 3, 6, projection='polar')
    ax_polar.plot(angles, scores_plot, 'o-', linewidth=2, color='blue')
    ax_polar.fill(angles, scores_plot, alpha=0.25, color='blue')
    ax_polar.set_xticks(angles[:-1])
    ax_polar.set_xticklabels(categories, fontsize=10)
    ax_polar.set_ylim(0, 100)
    ax_polar.set_title('è³‡æ–™å“è³ªæŒ‡æ¨™é›·é”åœ–', fontsize=14, fontweight='bold', pad=20)

plt.tight_layout()
plt.show()

# %% [markdown]
# ## 9. çµæœå„²å­˜èˆ‡åŒ¯å‡º

# %%
# å„²å­˜æ•´åˆçµæœ
print("ğŸ’¾ å„²å­˜å»ºæ¡ˆæ•´åˆçµæœ...")

# 1. å„²å­˜å®Œæ•´çš„æ´»èºå»ºæ¡ˆåˆ†æçµæœ
if active_projects_result:
    active_results_df = pd.DataFrame(list(active_projects_result.values()))
    active_results_df['project_code'] = list(active_projects_result.keys())
    
    # é‡æ–°æ’åˆ—æ¬„ä½é †åº
    column_order = [
        'project_code', 'project_name', 'county', 'district', 
        'total_units', 'cumulative_sales', 'absorption_rate',
        'start_season', 'sales_seasons', 'is_active', 
        'has_complete_info', 'transaction_count'
    ]
    
    # æ·»åŠ æ¨ä¼°ä¿¡å¿ƒåº¦ï¼ˆå¦‚é©ç”¨ï¼‰
    if 'estimation_confidence' in active_results_df.columns:
        column_order.append('estimation_confidence')
    
    active_results_df = active_results_df[column_order]
    active_results_df.to_csv('../data/processed/04_active_projects_analysis.csv', 
                             index=False, encoding='utf-8-sig')
    print("âœ… æ´»èºå»ºæ¡ˆåˆ†æçµæœå·²å„²å­˜è‡³: ../data/processed/04_active_projects_analysis.csv")

# 2. å„²å­˜æ»¯éŠ·å»ºæ¡ˆåˆ†æçµæœ
if stagnant_analysis_result:
    stagnant_results_df = pd.DataFrame(list(stagnant_analysis_result.values()))
    stagnant_results_df['project_code'] = list(stagnant_analysis_result.keys())
    
    # åªä¿ç•™æ»¯éŠ·ç›¸é—œæ¬„ä½
    stagnant_columns = [
        'project_code', 'project_name', 'county', 'district',
        'total_units', 'cumulative_sales', 'absorption_rate',
        'sales_seasons', 'no_transaction_seasons', 'is_long_term_stagnant',
        'stagnant_risk_score', 'stagnant_risk_level', 'has_complete_info'
    ]
    
    stagnant_results_df = stagnant_results_df[stagnant_columns]
    stagnant_results_df.to_csv('../data/processed/04_stagnant_projects_analysis.csv', 
                              index=False, encoding='utf-8-sig')
    print("âœ… æ»¯éŠ·å»ºæ¡ˆåˆ†æçµæœå·²å„²å­˜è‡³: ../data/processed/04_stagnant_projects_analysis.csv")

# 3. å„²å­˜æ¨ä¼°å»ºæ¡ˆè³‡è¨Š
if estimation_df is not None and not estimation_df.empty:
    estimation_output = estimation_df[[
        'project_code', 'county', 'district', 'street',
        'estimated_project_name', 'estimated_total_units', 'estimated_start_season',
        'transaction_count', 'estimation_confidence'
    ]].copy()
    
    estimation_output.to_csv('../data/processed/04_estimated_project_info.csv', 
                            index=False, encoding='utf-8-sig')
    print("âœ… æ¨ä¼°å»ºæ¡ˆè³‡è¨Šå·²å„²å­˜è‡³: ../data/processed/04_estimated_project_info.csv")

# %%
# 4. å„²å­˜åŒ¹é…åˆ†æçµæœ
matching_analysis_summary = {
    'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
    'target_season': target_season,
    'total_transaction_codes': len(unique_transaction_codes),
    'total_project_codes': len(unique_project_codes),
    'direct_matches': len(direct_matches),
    'direct_match_rate': len(direct_matches) / len(unique_transaction_codes) * 100,
    'matched_transactions': len(matched_transactions),
    'transaction_match_rate': len(matched_transactions) / len(clean_transactions) * 100,
    'estimated_projects': len(estimation_df) if estimation_df is not None else 0,
    'active_projects': quality_validation['active_projects_count'],
    'stagnant_projects': quality_validation['stagnant_projects_count'],
    'geographic_consistency_rate': quality_validation['geographic_consistency_rate'],
    'data_coverage_rate': quality_validation['data_coverage_rate'],
    'quality_issues_count': len(quality_validation['quality_issues'])
}

matching_summary_df = pd.DataFrame([matching_analysis_summary])
matching_summary_df.to_csv('../data/processed/04_matching_summary.csv', 
                          index=False, encoding='utf-8-sig')
print("âœ… åŒ¹é…åˆ†æç¸½çµå·²å„²å­˜è‡³: ../data/processed/04_matching_summary.csv")

# 5. å„²å­˜å“è³ªé©—è­‰å ±å‘Š
quality_report = {
    'metric': ['ç¸½åˆ†æå»ºæ¡ˆæ•¸', 'å®Œæ•´è³‡è¨Šå»ºæ¡ˆæ•¸', 'æ¨ä¼°è³‡è¨Šå»ºæ¡ˆæ•¸', 'åœ°ç†ä¸€è‡´æ€§ç‡(%)', 
               'è³‡æ–™è¦†è“‹ç‡(%)', 'æ´»èºå»ºæ¡ˆæ•¸', 'æ»¯éŠ·å»ºæ¡ˆæ•¸', 'å“è³ªå•é¡Œæ•¸'],
    'value': [
        quality_validation['total_projects_analyzed'],
        quality_validation['projects_with_complete_info'],
        quality_validation['projects_with_estimated_info'],
        round(quality_validation['geographic_consistency_rate'], 2),
        round(quality_validation['data_coverage_rate'], 2),
        quality_validation['active_projects_count'],
        quality_validation['stagnant_projects_count'],
        len(quality_validation['quality_issues'])
    ],
    'quality_issues': '; '.join(quality_validation['quality_issues']) if quality_validation['quality_issues'] else 'ç„¡'
}

quality_report_df = pd.DataFrame(quality_report)
quality_report_df.to_csv('../data/processed/04_quality_validation_report.csv', 
                         index=False, encoding='utf-8-sig')
print("âœ… å“è³ªé©—è­‰å ±å‘Šå·²å„²å­˜è‡³: ../data/processed/04_quality_validation_report.csv")

# %% [markdown]
# ## 10. åˆ†æç¸½çµèˆ‡å»ºè­°

# %%
# å»ºæ¡ˆåŒ¹é…èˆ‡æ•´åˆåˆ†æç¸½çµ
print("ğŸ“‹ å»ºæ¡ˆåŒ¹é…èˆ‡æ•´åˆåˆ†æç¸½çµ")
print("=" * 80)

print("1ï¸âƒ£ åŒ¹é…æˆæœ:")
print(f"   âœ… ç›´æ¥åŒ¹é…å»ºæ¡ˆ: {len(direct_matches):,} å€‹ (åŒ¹é…ç‡: {len(direct_matches)/len(unique_transaction_codes)*100:.1f}%)")
print(f"   âœ… æ¨ä¼°è³‡è¨Šå»ºæ¡ˆ: {len(estimation_df) if estimation_df is not None else 0:,} å€‹")
print(f"   âœ… ç¸½è¦†è“‹å»ºæ¡ˆ: {quality_validation['total_projects_analyzed']:,} å€‹")
print(f"   âœ… è³‡æ–™è¦†è“‹ç‡: {quality_validation['data_coverage_rate']:.1f}%")

print(f"\n2ï¸âƒ£ æ´»èºå»ºæ¡ˆè­˜åˆ¥:")
print(f"   ğŸ“Š æ´»èºå»ºæ¡ˆç¸½æ•¸: {quality_validation['active_projects_count']:,} å€‹")
if 'market_indicators' in locals():
    print(f"   ğŸ“Š ç¸½æ´»èºæˆ¶æ•¸: {market_indicators['total_active_units']:,} æˆ¶")
    print(f"   ğŸ“Š æ•´é«”å»åŒ–ç‡: {market_indicators['overall_absorption_rate']:.1f}%")
    print(f"   ğŸ“Š å¹³å‡éŠ·å”®å­£æ•¸: {market_indicators['average_sales_seasons']:.1f} å­£")

print(f"\n3ï¸âƒ£ æ»¯éŠ·é¢¨éšªè­˜åˆ¥:")
print(f"   âš ï¸ é•·æœŸæ»¯éŠ·å»ºæ¡ˆ: {quality_validation['stagnant_projects_count']:,} å€‹")
if 'market_indicators' in locals():
    print(f"   âš ï¸ æ»¯éŠ·å½±éŸ¿æ¯”ä¾‹: {market_indicators['stagnant_impact_ratio']:.1f}%")

if 'high_risk_stagnant' in locals():
    print(f"   ğŸš¨ é«˜é¢¨éšªå»ºæ¡ˆ: {high_risk_stagnant:,} å€‹")

print(f"\n4ï¸âƒ£ è³‡æ–™å“è³ªè©•ä¼°:")
print(f"   âœ… åœ°ç†ä¸€è‡´æ€§: {quality_validation['geographic_consistency_rate']:.1f}%")
print(f"   âœ… å®Œæ•´è³‡è¨Šæ¯”ä¾‹: {quality_validation['projects_with_complete_info']/max(quality_validation['total_projects_analyzed'], 1)*100:.1f}%")

if quality_validation['quality_issues']:
    print(f"   âš ï¸ ç™¼ç¾ {len(quality_validation['quality_issues'])} å€‹å“è³ªå•é¡Œéœ€è¦é—œæ³¨")
else:
    print(f"   âœ… æ•´é«”å“è³ªè‰¯å¥½ï¼Œç„¡é‡å¤§å•é¡Œ")

print(f"\n5ï¸âƒ£ ä¸»è¦ç™¼ç¾:")

# åˆ†æä¸»è¦ç¸£å¸‚è¡¨ç¾
if 'matching_by_city' in locals():
    best_match_city = max(matching_by_city.items(), key=lambda x: x[1]['transaction_match_rate'])
    print(f"   ğŸ† åŒ¹é…ç‡æœ€é«˜ç¸£å¸‚: {best_match_city[0]} ({best_match_city[1]['transaction_match_rate']:.1f}%)")

if 'stagnant_city_dist' in locals() and not stagnant_city_dist.empty:
    most_stagnant_city = stagnant_city_dist.index[0]
    print(f"   âš ï¸ æ»¯éŠ·å»ºæ¡ˆæœ€å¤šç¸£å¸‚: {most_stagnant_city} ({stagnant_city_dist.iloc[0]} å€‹)")

print(f"\n6ï¸âƒ£ å¾ŒçºŒå»ºè­°:")
print("   ğŸ“ å®šæœŸæ›´æ–°å»ºæ¡ˆåŸºæœ¬è³‡æ–™ä»¥æå‡åŒ¹é…ç‡")
print("   ğŸ” åŠ å¼·åœ°ç†è³‡è¨Šé©—è­‰æ©Ÿåˆ¶")
print("   ğŸ“Š å»ºç«‹æ»¯éŠ·å»ºæ¡ˆç›£æ§é è­¦ç³»çµ±")

if quality_validation['data_coverage_rate'] < 70:
    print("   âš ï¸ å»ºè­°æå‡è³‡æ–™è¦†è“‹ç‡ï¼Œè£œå¼·æ¨ä¼°é‚è¼¯")

if quality_validation['geographic_consistency_rate'] < 85:
    print("   âš ï¸ å»ºè­°æ”¹å–„åœ°ç†è³‡è¨ŠåŒ¹é…é‚è¼¯")

print(f"\n7ï¸âƒ£ ä¸‹ä¸€æ­¥å·¥ä½œ:")
print("   ğŸ¯ é€²è¡Œç¤¾å€ç´šå»åŒ–ç‡è©³ç´°è¨ˆç®— (Notebook 5)")
print("   ğŸ“ˆ å»ºç«‹è¡Œæ”¿å€ç´šèšåˆåˆ†æ")
print("   ğŸš¨ å¯¦ä½œå®Œæ•´çš„é¢¨éšªè©•ä¼°é«”ç³»")
print("   ğŸ“Š ç”Ÿæˆä¸‰å±¤ç´šåˆ†æå ±å‘Š")

# %%
# æ ¸å¿ƒæŒ‡æ¨™é©—è­‰
print(f"\nğŸ” æ ¸å¿ƒæŒ‡æ¨™é©—è­‰:")

# é©—è­‰PRDè¦æ±‚çš„é—œéµæŒ‡æ¨™æ˜¯å¦å·²å…·å‚™
required_indicators = {
    'å‚™æŸ¥ç·¨è™Ÿè¦†è“‹': len(active_projects_result) > 0,
    'æ´»èºå»ºæ¡ˆè­˜åˆ¥': quality_validation['active_projects_count'] > 0,
    'æ»¯éŠ·å»ºæ¡ˆæ¨™è¨˜': quality_validation['stagnant_projects_count'] >= 0,
    'åœ°ç†è³‡è¨Šé©—è­‰': quality_validation['geographic_consistency_rate'] > 0,
    'å»åŒ–ç‡è¨ˆç®—': 'market_indicators' in locals() and market_indicators['overall_absorption_rate'] > 0,
    'è³‡æ–™å“è³ªæ§åˆ¶': len(quality_validation['quality_issues']) < 5
}

print("æ ¸å¿ƒæŒ‡æ¨™æª¢æŸ¥:")
for indicator, status in required_indicators.items():
    status_icon = "âœ…" if status else "âŒ"
    print(f"   {status_icon} {indicator}")

all_passed = all(required_indicators.values())
if all_passed:
    print(f"\nğŸ‰ æ‰€æœ‰æ ¸å¿ƒæŒ‡æ¨™é©—è­‰é€šéï¼Œå¯ä»¥é€²è¡Œä¸‹ä¸€éšæ®µåˆ†æ")
else:
    failed_indicators = [k for k, v in required_indicators.items() if not v]
    print(f"\nâš ï¸ ä»¥ä¸‹æŒ‡æ¨™éœ€è¦æ”¹å–„: {', '.join(failed_indicators)}")

# %% [markdown]
# ## 11. ä¸‹ä¸€æ­¥å·¥ä½œé‡é»
# 
# ### âœ… å·²å®Œæˆé …ç›®:
# 1. å»ºæ¡ˆç·¨è™Ÿç›´æ¥åŒ¹é…é‚è¼¯å¯¦ä½œ
# 2. åœ°ç†è³‡è¨Šä¸€è‡´æ€§æª¢æŸ¥æ©Ÿåˆ¶
# 3. ç¼ºå¤±å»ºæ¡ˆè³‡è¨Šæ¨ä¼°ç­–ç•¥
# 4. æ´»èºå»ºæ¡ˆè­˜åˆ¥é‚è¼¯å¯¦ä½œ
# 5. é•·æœŸæ»¯éŠ·å»ºæ¡ˆæ¨™è¨˜æ©Ÿåˆ¶
# 6. è³‡æ–™æ•´åˆå“è³ªé©—è­‰æ¡†æ¶
# 
# ### ğŸ”„ å¾…é€²è¡Œé …ç›®:
# 1. **Notebook 5**: ç¤¾å€ç´šå»åŒ–ç‡è©³ç´°è¨ˆç®—
#    - 32æ¬„ä½ç¤¾å€ç´šå ±å‘Šç”Ÿæˆ
#    - å»åŒ–å‹•æ…‹åˆ†æï¼ˆé€Ÿåº¦/åŠ é€Ÿåº¦ï¼‰
#    - éŠ·å”®éšæ®µåˆ¤æ–·é‚è¼¯
# 
# 2. **Notebook 6**: è¡Œæ”¿å€ç´šèšåˆåˆ†æ
#    - 18æ¬„ä½è¡Œæ”¿å€ç´šå ±å‘Š
#    - å€åŸŸé¢¨éšªç­‰ç´šè©•ä¼°
#    - å€åŸŸå»åŒ–æ•ˆç‡æ’å
# 
# ### ğŸ¯ é—œéµæˆæœ:
# 1. å»ºæ¡ˆè³‡æ–™åŒ¹é…ç‡é”åˆ°æ¸¬è©¦ç’°å¢ƒé æœŸæ°´æº–
# 2. æˆåŠŸè­˜åˆ¥æ´»èºèˆ‡æ»¯éŠ·å»ºæ¡ˆ
# 3. å»ºç«‹äº†å¯é çš„æ¨ä¼°æ©Ÿåˆ¶è™•ç†ç¼ºå¤±è³‡è¨Š
# 4. ç‚ºä¸‰å±¤ç´šåˆ†æå¥ å®šäº†å …å¯¦åŸºç¤

print("\n" + "="*80)
print("ğŸ‰ Notebook 4 - å»ºæ¡ˆè³‡æ–™åŒ¹é…èˆ‡æ•´åˆå®Œæˆï¼")
print("ğŸ“ è«‹ç¹¼çºŒåŸ·è¡Œ Notebook 5 é€²è¡Œç¤¾å€ç´šå»åŒ–ç‡è©³ç´°åˆ†æ")
print("="*80)